{
  "title": "Multilingual Representations and Models for Improved Low-Resource Language Processing",
  "outline": [
    {
      "level": "H2",
      "text": "Multilingual Representations and Models for\nImproved Low-Resource Language Processing\nDissertation\nan der Fakultät für Mathematik, Informatik und Statistik\nder Ludwig–Maximilians–Universität München\nvorgelegt von\nMasoud Jalili Sabet\naus Tehran\nMünchen, den 15. December 2021",
      "page": 1,
      "type": "text",
      "language": "de"
    },
    {
      "level": "H2",
      "text": "Erstgutachter: Prof. Dr. Hinrich Schütze\nZweitgutachter: Prof. Dr. Andy Way\nDrittgutachter: Prof. Dr. Ehsan Shareghi\nTag der Einreichung: 15. December 2021\nTag der mündlichen Prüfung: 18. July 2022",
      "page": 2,
      "type": "text",
      "language": "de"
    },
    {
      "level": "H2",
      "text": "Eidesstattliche Versicherung\n(siehe Promotionsordnung vom 12.07.11, § 8, Abs. 2 Pkt. .5.)\nHiermit erkläre ich an Eides statt, dass die Dissertation von mir selbstständig ohne\nunerlaubte Beihilfe angefertigt ist.\nMünchen, den 15. December 2021\nMasoud Jalili Sabet\n3",
      "page": 3,
      "type": "text",
      "language": "de"
    },
    {
      "level": "H2",
      "text": "Abstract\nWord representations are the cornerstone of modern NLP. Representing words or\ncharacters using real-valued vectors as static representations that can capture the\nSemantics and encode the meaning has been popular among researchers. In more\nrecent years, Pretrained Language Models using large amounts of data and creating\ncontextualized representations achieved great performance in various tasks such as\nSemantic Role Labeling. These large pretrained language models are capable of\nstoring and generalizing information and can be used as knowledge bases.\nLanguage models can produce multilingual representations while only using\nmonolingual data during training. These multilingual representations can be bene-\nﬁcial in many tasks such as Machine Translation. Further, knowledge extraction\nmodels that only relied on information extracted from English resources, can now\nbeneﬁt from extra resources in other languages.\nAlthough these results were achieved for high-resource languages, there are\nthousands of languages that do not have large corpora. Moreover, for other tasks\nsuch as machine translation, if large monolingual data is not available, the models\nneed parallel data, which is scarce for most languages. Further, many languages\nlack tokenization models, and splitting the text into meaningful segments such\nas words is not trivial. Although using subwords helps the models to have better\ncoverage over unseen data and new words in the vocabulary, generalizing over\nlow-resource languages with different alphabets and grammars is still a challenge.\nThis thesis investigates methods to overcome these issues for low-resource\nlanguages. In the ﬁrst publication, we explore the degree of multilinguality in\nmultilingual pretrained language models. We demonstrate that these language\nmodels can produce high-quality word alignments without using parallel training\ndata, which is not available for many languages. In the second paper, we extract\nword alignments for all available language pairs in the public bible corpus (PBC).\nFurther, we created a tool for exploring these alignments which are especially\nhelpful in studying low-resource languages. The third paper investigates word\nalignment in multiparallel corpora and exploits graph algorithms for extracting new\nalignment edges. In the fourth publication, we propose a new model to iteratively\ngenerate cross-lingual word embeddings and extract word alignments when only\nsmall parallel corpora are available. Lastly, the ﬁfth paper ﬁnds that aggregation\nof different granularities of text can improve word alignment quality. We propose\nusing subword sampling to produce such granularities.\n5",
      "page": 5,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Zusammenfassung\nWortdarstellungen sind der Eckpfeiler der modernen Maschinellen Sprachver-\narbeitung. Die Darstellung von Wörtern oder Zeichen mit Hilfe von Vektoren\nals statistische Repräsentationen, die die Semantik erfassen und die Bedeutung\nkodieren können, ist in der Forschung weitverbreitet. In den letzten Jahren haben\nvortrainierte Sprachmodelle, die große Datenmengen verwenden und kontextua-\nlisierte Repräsentationen erstellen, bei verschiedenen Aufgaben, wie z.B. Semantic\nRole Labeling, große Erfolge erzielt. Diese großen vortrainierten Sprachmodelle\nsind in der Lage, Informationen zu speichern und zu verallgemeinern und können\nals Wissensbasis verwendet werden.\nSprachmodelle können mehrsprachige Repräsentationen erzeugen, obwohl\nsie beim Training nur einsprachige Daten verwenden. Diese mehrsprachigen\nRepräsentationen können bei vielen Aufgaben wie der maschinellen Übersetzung\nvon Vorteil sein. Außerdem können Modelle zur Wissensextraktion, die nur\nauf Informationen aus englischen Ressourcen basieren, nun von zusätzlichen\nRessourcen in anderen Sprachen proﬁtieren.\nObwohl diese Ergebnisse für Sprachen mit vielen Ressourcen erzielt wurden,\ngibt es Tausende von Sprachen, die nicht über große Korpora verfügen. Außerdem\nbenötigen die Modelle für andere Aufgaben wie die maschinelle Übersetzung,\nwenn keine großen einsprachigen Daten verfügbar sind, parallele Daten, die für\ndie meisten Sprachen nur in geringem Umfang vorhanden sind. Außerdem gibt es\nin vielen Sprachen keine Tokenisierungsmodelle, und die Aufteilung des Textes\nin sinnvolle Segmente wie Wörter ist nicht trivial. Obwohl die Verwendung von\nSubwords den Modellen hilft, eine bessere Abdeckung von ungesehenen Daten\nund neuen Wörtern im Vokabular zu erreichen, ist die Verallgemeinerung auf\nressourcenarme Sprachen mit unterschiedlichen Alphabeten und Grammatiken\nimmer noch eine Herausforderung.\nIn dieser Arbeit werden Methoden zur Überwindung dieser Probleme für\nSprachen mit geringen Ressourcen untersucht. In der ersten Veröffentlichung unter-\nsuchen wir den Grad der Mehrsprachigkeit in mehrsprachigen vortrainierten Sprach-\nmodellen. Wir zeigen, dass diese Sprachmodelle hochwertige Wortalignierungen\nerzeugen können, ohne parallele Trainingsdaten zu verwenden, welche für viele\nSprachen nicht verfügbar sind.\nIn der zweiten Arbeit extrahieren wir Wort-\nalignierungen für alle verfügbaren Sprachpaare im Public Bible Corpus\n(PBC). Darüber hinaus haben wir ein Tool zur Untersuchung dieser Alignierungen\nentwickelt, das insbesondere bei der Untersuchung von Sprachen mit geringen\nRessourcen hilfreich ist. Die dritte Arbeit untersucht die Wortalignierungen in\nmultiparallelen Korpora und nutzt Graphalgorithmen zur Extraktion neuer Aus-\nrichtungskanten. In der vierten Veröffentlichung schlagen wir ein neues Modell\n7",
      "page": 7,
      "type": "text",
      "language": "de"
    },
    {
      "level": "H2",
      "text": "zur iterativen Erzeugung von crosslingual word embeddings und zur Extraktion\nvon Wortalignierungen vor, wenn nur kleine parallele Korpora verfügbar sind. Im\nfünften Beitrag schließlich wird festgestellt, dass die Aggregation verschiedener\nTextgranularitäten die Qualität der Wortalignierungen verbessern kann. Wir schla-\ngen die Verwendung von sogenanntem subword sampling vor, um solche Granula-\nritäten zu erzeugen.\n8",
      "page": 8,
      "type": "text",
      "language": "de"
    },
    {
      "level": "H2",
      "text": "Publications and\nDeclaration of Co-Authorship\nChapter 2 corresponds to the following publication:\nMasoud Jalili Sabet*, Philipp Dufter*, François Yvon, and Hinrich\nSchütze. SimAlign: High Quality Word Alignments Without Paral-\nlel Training Data Using Static and Contextualized Embeddings. In\nProceedings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP): Findings, pp. 1627–1643. 2020.\n*equal contribution.\nThe basis of this publication was my earlier (unpublished) work on using embed-\ndings for word alignments. Except for the initial proof of concept, I performed the\nimplementation and most evaluation for this work. Along with my coauthors, I\nco-wrote the published paper.\nChapter 3 corresponds to the following publication:\nAyyoob Imani, Masoud Jalili Sabet, Philipp Dufter, Michael Cysouw,\nand Hinrich Schütze. ParCourE: A Parallel Corpus Explorer for a\nMassively Multilingual Corpus. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics (ACL):\nSystem Demonstrations, pp. 63-72. 2021.\nPhilipp Dufter and I conceived of the original research contributions. I built a\nproof-of-concept of ParCourE that tested and developed a subset of the ParCourE\nfunctionality. Ayyoob Imani then built the ﬁnal ParCourE system based on the\nproof-of-concept. I implemented the alignment reader in the backend for the ﬁnal\nsystem. All authors wrote the initial draft of the article and did the subsequent\ncorrections. All authors regularly discussed this work with each other and improved\nthe draft.\n9",
      "page": 9,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Chapter 4 corresponds to the following publication:\nAyyoob Imani*, Masoud Jalili Sabet*, LutﬁKerem Senel, Philipp\nDufter, François Yvon, and Hinrich Schütze. Graph Algorithms for\nMultiparallel Word Alignment. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing (EMNLP).\n2021. *equal contribution.\nAyyoob Imani and I conceived of the original research contributions. My origi-\nnal contribution consisted of classical graph algorithms, Ayyoob Imani’s original\ncontribution consisted of recommendation algorithms. I also contributed the idea\nof using translated datasets. I performed implementation and evaluation for the\nclassical graph algorithms (Section 3.3). I performed the experiments for the\nanalysis of the model (Section 5.3). All authors wrote the initial draft of the article\nand did the subsequent corrections. All authors regularly discussed this work with\neach other and improved the draft.\nChapter 5 corresponds to the following publication:\nNina Poerner, Masoud Jalili Sabet, Benjamin Roth, and Hinrich\nSchütze. Aligning Very Small Parallel Corpora Using Cross-Lingual\nWord Embeddings and a Monogamy Objective. In Computing Research\nRepository 1811.00066.\nI conceived of the original idea of using and ﬁne-tuning bilingual word embed-\ndings for low-resource word alignment. I implemented the initial proof-of-concept\nusing word2vec representations trained with S-IDs. Based on the proof-of-concept,\nNina Poerner implemented our ﬁnal experimental system. She also contributed a\nnew loss function (“monogamy”). I performed the experiments with the baseline\nmethods and evaluated the model (Figure 2). Nina Poerner and I wrote related\nwork, evaluation and discussion sections together. She wrote the initial draft of the\nrest of the paper. I regularly discussed this work with my coauthors and assisted in\nimproving the draft.\nChapter 6 corresponds to the following publication:\nEhsaneddin Asgari*, Masoud Jalili Sabet*, Philipp Dufter, Christo-\npher Ringlstetter, and Hinrich Schütze. Subword Sampling for Low Re-\nsource Word Alignment. In Computing Research Repository 2012.11657.\n*equal contribution.\nI conceived of the original idea of using subword tokenization for word alignment\nand implemented a proof-of-concept using a new subword tokenization model\n10",
      "page": 10,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "trained with an entropy-based loss function. Ehsaneddin Asgari implemented the\niterative subword sampling algorithm. I implemented subword alignment and\nevaluation. I performed most of the experiments and evaluations (Tables 2 and 3).\nAll authors wrote the initial draft of the article. I regularly discussed this work with\nmy coauthors and assisted in improving the draft.\n11",
      "page": 11,
      "type": "text",
      "language": "af"
    },
    {
      "level": "H2",
      "text": "Contents\n1\nIntroduction\n15\n1.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n1.1.1\nApproach . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n1.1.2\nResearch Questions . . . . . . . . . . . . . . . . . . . . .\n18\n1.1.3\nOutline . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.2\nFoundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.2.1\nNotation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.2.2\nLocal and Distributed Representations . . . . . . . . . . .\n19\n1.3\nStatic Representations . . . . . . . . . . . . . . . . . . . . . . . .\n21\n1.3.1\nMonolingual Representations\n. . . . . . . . . . . . . . .\n21\n1.3.2\nMultilingual Representations . . . . . . . . . . . . . . . .\n23\n1.4\nContextualized Representations . . . . . . . . . . . . . . . . . . .\n27\n1.4.1\nMonolingual Representations\n. . . . . . . . . . . . . . .\n28\n1.4.2\nMultilingual Representations . . . . . . . . . . . . . . . .\n33\n1.5\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n1.5.1\nWord Alignment . . . . . . . . . . . . . . . . . . . . . .\n35\n1.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n1.6.1\nContributions . . . . . . . . . . . . . . . . . . . . . . . .\n36\n1.6.2\nFuture Work\n. . . . . . . . . . . . . . . . . . . . . . . .\n37\n2\nSimAlign: High Quality Word Alignments Without Parallel Train-\ning Data Using Static and Contextualized Embeddings\n39\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n2.2\nMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n2.2.1\nAlignments from Similarity Matrices\n. . . . . . . . . . .\n41\n2.2.2\nDistortion and Null Extensions . . . . . . . . . . . . . . .\n42\n2.3\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n2.3.1\nEmbedding Learning . . . . . . . . . . . . . . . . . . . .\n42\n2.3.2\nWord and Subword Alignments\n. . . . . . . . . . . . . .\n42\n2.3.3\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n2.3.4\nEvaluation Measures . . . . . . . . . . . . . . . . . . . .\n43\n12",
      "page": 12,
      "type": "text",
      "language": "sw"
    },
    {
      "level": "H2",
      "text": "CONTENTS\n2.3.5\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n2.4\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n2.4.1\nEmbedding Layer . . . . . . . . . . . . . . . . . . . . . .\n43\n2.4.2\nComparison with Prior Work . . . . . . . . . . . . . . . .\n44\n2.4.3\nAdditional Methods and Extensions . . . . . . . . . . . .\n45\n2.4.4\nWords and Subwords . . . . . . . . . . . . . . . . . . . .\n46\n2.4.5\nPart-of-Speech Analysis . . . . . . . . . . . . . . . . . .\n47\n2.5\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n2.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n2.7\nAppendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n3\nParCourE: A Parallel Corpus Explorer for a Massively Multilin-\ngual Corpus\n57\n3.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n3.2\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n3.3\nFeatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n3.3.1\nMultiparallel Alignment Browser: MULTALIGN . . . . .\n59\n3.3.2\nLexicon View: LEXICON . . . . . . . . . . . . . . . . .\n60\n3.3.3\nInterconnections\n. . . . . . . . . . . . . . . . . . . . . .\n60\n3.3.4\nAlignment Generation View: INTERACTIVE . . . . . . .\n61\n3.4\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.5\nBackend Design . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.6\nParCourE Use Cases\n. . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.7\nExtension to Other Corpora . . . . . . . . . . . . . . . . . . . . .\n63\n3.7.1\nComputing Infrastructure and Runtime\n. . . . . . . . . .\n63\n3.8\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n3.9\nEthical Considerations\n. . . . . . . . . . . . . . . . . . . . . . .\n64\n4\nGraph Algorithms for Multiparallel Word Alignment\n69\n4.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n4.2\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n4.3\nMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n4.3.1\nThe MPWA framework . . . . . . . . . . . . . . . . . . .\n72\n4.3.2\nNon-negative matrix factorization . . . . . . . . . . . . .\n73\n4.4\nLink Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.5\nExperimental setup . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n4.5.1\nPBC . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n4.5.2\nWord alignment datasets . . . . . . . . . . . . . . . . . .\n74\n4.6\nInitial word alignments . . . . . . . . . . . . . . . . . . . . . . .\n74\n4.7\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n13",
      "page": 13,
      "type": "text",
      "language": "sw"
    },
    {
      "level": "H2",
      "text": "Introduction\n4.7.1\nMultiparallel corpus results . . . . . . . . . . . . . . . . .\n75\n4.7.2\nMT dataset results\n. . . . . . . . . . . . . . . . . . . . .\n76\n4.7.3\nAnalysis\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n4.8\nConclusion and Future Work . . . . . . . . . . . . . . . . . . . .\n78\n4.9\nPipeline Details . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n5\nAligning Very Small Parallel Corpora Using Cross-Lingual Word\nEmbeddings and a Monogamy Objective\n83\n5.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n5.2\nMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n5.2.1\nCLWE-only baseline . . . . . . . . . . . . . . . . . . . .\n84\n5.2.2\nFine-tuning method . . . . . . . . . . . . . . . . . . . . .\n85\n5.3\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.4.1\nCorpus size . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.4.2\nBeneﬁts of ﬁne-tuning . . . . . . . . . . . . . . . . . . .\n86\n5.5\nUse case: Aligning the UDHR . . . . . . . . . . . . . . . . . . .\n86\n5.5.1\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.5.2\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n6\nSubword Sampling for Low Resource Word Alignment\n91\n6.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n6.2\nMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n6.2.1\nDataset . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n6.2.2\nEvaluation\n. . . . . . . . . . . . . . . . . . . . . . . . .\n93\n6.2.3\nSentence subword space . . . . . . . . . . . . . . . . . .\n93\n6.2.4\nIterative subword sampling algorithm . . . . . . . . . . .\n94\n6.2.5\nIntuition behind the use of logarithmic priors for the vo-\ncabulary size . . . . . . . . . . . . . . . . . . . . . . . .\n94\n6.2.6\nSubword sampling in other languages . . . . . . . . . . .\n95\n6.2.7\nExperimental Setup . . . . . . . . . . . . . . . . . . . . .\n95\n6.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n6.3.1\nIterative Subword Sampling . . . . . . . . . . . . . . . .\n95\n6.3.2\nLow-Resource Alignment Results . . . . . . . . . . . . .\n95\n6.3.3\nMid-Resource Alignment Results\n. . . . . . . . . . . . .\n96\n6.3.4\nQualitative Analysis\n. . . . . . . . . . . . . . . . . . . .\n97\n6.4\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n6.5\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\nBibliography\n102\n14",
      "page": 14,
      "type": "text",
      "language": "sw"
    },
    {
      "level": "H2",
      "text": "Chapter 1\nIntroduction\n1.1\nMotivation\nNatural language is one of the most important features of human civilization. As\na result, the understanding, processing, and generation of natural language have\nbeen a major research topic in computer science from the start. The beginning of\nnatural language processing dates back to the 1950s when Alan Turing proposed\nthe task of automated natural language understanding and generation as a criterion\nof intelligence for machines, which is now called the Turing test (Turing, 1950).\nDuring the early years, most works on language processing were rule-based.\nAfter the introduction of machine learning methods for language processing, statis-\ntical models such as statistical machine translation (Brown et al., 1988a,b) became\npopular. Since in natural languages an inﬁnite number of different sentences are\npossible, which all contain a different number of words, models have to divide the\ntext into smaller meaningful units, such as sentences and words, in order to process\nit. Statistical models then calculate the frequency of words and their co-occurrences\nin order to estimate conditional probability distributions – for example, based on\nn-grams – and use them to represent words and the relationships between them.\nHowever, these simple techniques were at their limits in many tasks, and scaling\nup the size of the datasets did not result in any signiﬁcant progress (Mikolov et al.,\n2013a).\nWord or token representation forms the basis of most modern NLP approaches.\nFollowing this approach, the text is segmented into smaller units, which can be\nwords, tokens, or characters. Each segment is represented by a real-valued vector\nthat can encode the semantics of the segment and its context in the text (Schütze,\n1992). It should be mentioned that these distributed representations can be used\nas a similarity measure between different segments. Word representations were\nused as the ﬁrst layer of neural networks for different tasks such as part-of-speech\n15",
      "page": 15,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\ntagging, named entity recognition, and semantic role labeling (Collobert et al.,\n2011). Although these models achieve good performance, there are several open\nquestions about the best way to create the underlying representations. There are\nmultiple challenges regarding this process:\n• Handling new tokens and out-of-vocabulary (OOV) words: creating em-\nbeddings for new tokens that ﬁt the representations of known tokens is a\nchallenge. Schick and Schütze (2019) propose a method to leverage both\nthe surface form and the context of the word to create embeddings, but only\ninvestigate static representations.\n• Choosing the smallest meaningful unit: while using subwords and characters\nis a promising potential solution to represent the semantics of OOV words,\nit is hard to capture their meaning through representation learning (Park\net al., 2021). Pretrained language models mostly use subword tokenizers\nthat are generated by compression algorithms such as byte pair encoding\n(Sennrich et al., 2016) and WordPiece (Schuster and Nakajima, 2012), which\nare not morphologically informed (Hofmann et al., 2021). As an example, in\nthe BERT language model (Devlin et al., 2019), the word “Superbizarre” is\nrepresented by the subwords { “superb”, “iza”, “rre” }, while “Superb” can\nhave a contrastive sentiment with “Superbizarre”; this means that segmenting\nwords into meaningful subunits is even more challenging in multilingual\nsettings.\n• Hierarchical representations: producing representations for a group of units\n(e.g., an embedding for a sentence or phrase, which is produced by individual\nword embeddings) or for subunits (e.g., generating character or subword\nembeddings that are comparable with word embeddings) is desirable, but non-\ntrivial. As an example, the word “war” can appear as a subunit in other words,\nsuch as “warzone”, “warehouse”, “warranty”, and “wardrobe”. Since the\nlanguage models should be able to produce the word representations from\nthe corresponding subunits, assigning a single representation to “war”, which\nis compatible with the meanings of all the mentioned words, can be difﬁcult.\nSurprisingly, averaging word embeddings of all words in a sentence has\nproven to be a strong baseline (Faruqui et al., 2015), but for long sentences,\nparagraphs, and documents, averaging the representations may lose important\ninformation.\nIn more recent years, pretrained language models (LMs) that create contex-\ntualized word representations have achieved good performance in a wide range\nof tasks (Peters et al., 2018; Devlin et al., 2019). Various factors contributed to\ntheir exceptional performance. First, the leveraging of contextualized, rather than\n16",
      "page": 16,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.1 Motivation\nstatic representations, provides more information for downstream tasks. Second,\nusing the transformer architecture allows for more parallelization and therefore\nfor the training of larger models on larger amounts of data. As a consequence,\nthese large pretrained models are capable of storing and generalizing information\nand can be used as knowledge bases (Petroni et al., 2019). Third, the usage of\nsubwords (Sennrich et al., 2016) has increased the coverage of the models and\nprovided a solution for OOV words with high performance. Previous works use\nmorphological information (in the form of a table of possible afﬁxes) to increase\nthe coverage over OOV words in morphologically rich languages (Passban et al.,\n2018), but building a large list of all possible afﬁxes for many languages is a costly\nand time-consuming task, and might not be a good solution for all phenomena for\nall languages (e.g., compound words in German).\nAlthough these contributions have paved the way for more innovations in\nnatural language processing, they have not had the same impact on many low-\nresource languages (Blasi et al., 2021). There are more than 7000 languages in\nthe world (Eberhard et al., 2020), only a few of which have adequate resources\nfor training such models. Due to memory constraints and to overcome sparsity,\npretrained multilingual language models use subword tokenization models with\nlimited vocabulary sizes, which means that most languages (e.g., Khmer, Lao,\nKurdish, and Oromo) are not included in the vocabulary, and the tokens are mostly\nchosen from dominant languages (the languages with the most training data, such\nas English, German, and French). Additionally, for some low-resource languages\n(e.g., Arabic, Finnish, Korean, Russian, and Turkish) that are included in the\nvocabulary, the tokens are over-segmented, which results in poor quality token\nrepresentations (Rust et al., 2021).\nLanguage models can produce multilingual representations even though they\nonly use monolingual data during training (Devlin et al., 2019; Conneau et al.,\n2020a). Training multilingual models can be beneﬁcial for many reasons. First,\nthe model requires less data to train and generalize on multiple languages. In\naddition, the multilingual representations can be beneﬁcial for many tasks that\ndirectly make use of them, such as machine translation. Furthermore, knowledge\nextraction models, which formerly only had access to information extracted from\nresources available in one language (e.g., English), can now beneﬁt from resources\nin all languages (Roy et al., 2020).\nThis extends further to annotated data from high-resource languages, which can\nbe used to train a model that can be applied to other languages. The last advantage\nof multilingual language models is that they are easier to maintain compared to\nindividual models for each language.\n17",
      "page": 17,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\n1.1.1\nApproach\nIn this work, we approach the mentioned issues of multilingual NLP models for\nlow-resource languages using different methods:\n• Tokenization: we examine different tokenization models for languages, to\nimprove the quality of their representations and enable models to reach better\nperformance when large training datasets are not available.\n• Multilingual embeddings: we experiment with different multilingual models\nin order to study the strengths and weaknesses of such models for various\nlanguages. This will provide an insight into the effectiveness of different\nfeatures of these models and may help develop stronger models.\n• Multi-parallel datasets: we investigate multi-parallel datasets and different\nmethods of employing them to improve the quality of representations for all\nlanguages.\nIt should be noted that the above approaches are language agnostic. Despite\nthe fact that languages have different characteristics, which means that applying\nthe same procedures to all of them may lead to poor results for some, we pursue\npipelines and methods that can be applied to models effortlessly. We argue that\nusing language-speciﬁc rules hinders the growth and scalability of multilingual\nmodels.\n1.1.2\nResearch Questions\nConsidering the necessity of multilingual representations and models, and their\naforementioned beneﬁts, we can pursue a range of interesting questions:\n(i) Data: What kinds of datasets can help improve multilingual models? How\nmuch data is needed for the model during training time to show desirable\nperformance on a language (or a language pair)?\n(ii) Models: Do models require supervision in order to obtain better generaliza-\ntion? Are there any additional signals from the training data that the models\ncan use to perform better for more languages?\n(iii) Analysis: It is not clear how similar concepts are connected in different\nlanguages. How can we study this similarity? Is multilinguality based on the\nsimilarity of the languages? Does the model performance improve when the\ninformation from the other languages is added?\nIn this work, we aim to address all of these questions.\n18",
      "page": 18,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.2 Foundations\n1.1.3\nOutline\nIn this chapter, we start by deﬁning the notation used throughout this thesis and\nclarifying the terminology. We then introduce the basic methods of monolingual\nand multilingual representation learning, both in static and contextualized forms.\nWe ﬁnally describe applications that can beneﬁt from these representations and\nshow common evaluation methods for them.\nIn Chapter 2, we analyze existing multilingual representations and their per-\nformance in different languages. We show that we can obtain high-quality word\nalignments from them. In the second publication, presented in Chapter 3, we\ngenerate word alignments for all language pairs in the Parallel Bible Corpus (PBC)\nand present a tool to study the connections between languages. In Chapter 4, we\npropose a graph model that uses multi-parallel datasets to improve the quality of\nword alignments. This method especially aims for improvements in low-resource\nlanguages. Chapter 5 presents a model to iteratively improve the multilingual rep-\nresentations and word alignments. The last paper, presented in Chapter 6, measures\nthe word alignment performance based on the quality of the subword tokenization.\nWe propose a new model which samples from several subword tokenizations and\nimproves the word alignment quality for multiple language pairs.\n1.2\nFoundations\n1.2.1\nNotation\nWith positive integers d, t ∈N+, we denote vectors as boldface lowercase x ∈Rd,\nand matrices as boldface uppercase letters X ∈Rt×d. The i-th element of x\ncan be referred to as xi and matrices can be indexed (Xij)i=1,2...,t,j=1,2,...d = X.\nThe i-th row and j-th column of X are denoted as Xi ∈Rd, X⋆,j ∈Rt. When\na textual unit is used to index a matrix or vector, instead of an index, it refers\nto the vector or scalar corresponding to that unit; i.e., Xbook refers to the vector\nXi where index(book) = i and index() is a bijective function that assigns each\ntextual unit a unique integer. The cardinality of a set S is denoted by |S|, and\nthe Euclidean norm of a vector is ∥x∥. The transposed vector and matrix are\ndenoted as x⊺and X⊺. We denote the cosine similarity of two vectors x, y as\ncos-sim(x, y) := x⊺y/(∥x∥∥y∥).\n1.2.2\nLocal and Distributed Representations\nNatural language text has an order and can be interpreted as sequential data. We\ncan therefore denote it as (u1, u2, . . . , ut), where ui is some unit of text. The set\n19",
      "page": 19,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nEveryone enjoyed the food. I cooked pasta.\nCorpus\nEveryone enjoyed the food. I cooked pasta.\nSentences\nEveryone enjoyed the food . I\n...\nWords\nEvery one enjoy ed\n...\nSubwords\nE v e r y\n...\nCharacters\n69 118 101 114 121\n...\nBytes\nFigure 1.1 – Common ways to split text data into units. The byte representation\ndepends on the encoding for unicode points (e.g., utf-8).\nof all distinct text units is called the vocabulary V = {v1, v2, . . . , vn}. Common\nchoices for units are shown in Figure 1.1. In order to process units of text, these\nunits need to be assigned numerical representations, called embeddings. We denote\nan embedding function as a map e : V →E, which assigns each unit in the\nvocabulary an embedding. A simple choice of embedding function is the one-hot\nencoding, which chooses E = {0, 1}|V | and assigns each element vi ∈V the i-th\nunit in E. This requires one computing element for each unit vi and is called a\nlocal representation (Hinton et al., 1990). In contrast, distributed representations\nuse multiple computing elements to represent each vi. This implies that E is a\nd-dimensional space with d ≪|V |.\nFor example, assume a vocabulary of n elements V = {“cook”, “cookies”,\n“begin”, “began”, . .. }. We can use one-hot encodings as local representations by\nassigning e(“cook”) = (1, 0, 0 . . . ), e(“cookies”) = (0, 1, 0 . . . ), etc. A possible\ndistributed representation could consist of d dimensional vectors, where d is the\nnumber of all characters across all elements in V . Afterward, we assign e(vi)j =\n1{cj ∈vi} where {c1, c2, . . . , cd} is the set of all characters. This means e(vi)\nassings 1 to all the characters in vi.\nAt ﬁrst glance, it can be noticed that unlike distributed representations, in the\nlocal representations, all vectors are orthogonal to each other, and developing a\nuseful similarity metric is therefore challenging. On the other hand, on distributed\nrepresentations, a metric such as cosine similarity can be used: as “cook” and\n“cookies” share more common characters, their respective vectors are more similar\nthan “cook” and “begin”. Another advantage of this is that adding new elements to\nthe vocabulary does not require the embedding function to change the number of\nits dimensions.\nFinding a meaningful mapping for a distributed representation model is not triv-\nial. Furthermore, it is not clear how to assess a representation model. One common\nway of evaluating the resulting representations is to measure their performance on\n20",
      "page": 20,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.3 Static Representations\ndownstream tasks. Another is to check whether the similarity measures correlate\nwith the semantic similarity of the text units. When looking at multilingual repre-\nsentations speciﬁcally, more challenges regarding deﬁnition and evaluation arise.\nIn the literature, the term multilingual is sometimes used for models that can pro-\ncess multiple languages with no cross-language connections, whereas crosslingual\nmodels can leverage the connections between languages and provide a meaningful\nsimilarity measure between text units in different languages. These two groups\nnaturally require different methods of evaluation.\n1.3\nStatic Representations\nA static embedding function over a vocabulary V is deﬁned as\ne : V →Rd,\n(1.1)\nwith dimensionality d. Most static representation methods use the context of\nthe text unit to create the embedding for that unit (Mikolov et al., 2013a). The\nsupporting argument is that two words that occur in similar contexts are likely to\nhave similar meanings. For example, the words “bank” and “money” are more\nlikely to occur in the same sentences, therefore their embeddings are expected to\nhave high similarity.\nGiven a sequence of text units S = (s1, s2, . . . , st) ∈V t, the co-occurrence\nmatrix of the vocabulary V with size n is denoted as C ∈Rn×n, where Cij is the\nco-occurrence of si and sj in S. Formally, a text unit si is in the context of sj if\n|i −j| ≤c, where c is the size of the context window. The co-occurrence can\nthen be calculated as the number of times that two units occur in the same context\nwindow. A more relaxed version of co-occurrence is calculated by the number\nof times two units co-occur in the same batch of units in the corpus S, such as\nsentences or paragraphs (Levy et al., 2017). Note that these co-occurrence counts\ncan be weighted based on the distance between words or normalized over units,\ni.e., rows of C.\n1.3.1\nMonolingual Representations\nA simple method for creating monolingual representations is to use the co-occurrence\nmatrix C as embeddings; i.e., the embedding function is deﬁned as e(vi) := Ci.\nConsequently, if vi and vj occur in similar contexts, the embeddings e(vi) and\ne(vj) will have higher cosine similarity. Although this method is preferable to local\nrepresentations, there are certain disadvantages. The main issue is that since the\nsize of the vectors depends on the vocabulary size n, a large vocabulary may result\nin large input size and therefore higher computational costs for the model that uses\n21",
      "page": 21,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nthem. Furthermore, the addition of new units to the vocabulary will increase the\ndimensionality of overall representations, resulting in a need to update the entire\npipeline. Lastly, C is sparse since most units never co-occur which may result in\nproblems in downstream models.\nOne way of overcoming this issue is the use of matrix factorization as proposed\nby Schütze (1992). More speciﬁcally, using the singular value decomposition\n(SVD) of C for the embeddings enables the representations to be of any chosen\ndimensionality. During recent years, as neural networks became more popular in\nnatural language processing, the need for low-dimensional representations became\nmore prominent. For instance, Levy and Goldberg (2014) and Pennington et al.\n(2014) introduced the use of matrix factorization for word representations.\nAs neural networks became more popular in natural language processing, new\nmethods were introduced that exploited them for the generation of representations.\nMikolov et al. (2013a) employed shallow neural networks to encode the text and\nused the hidden layer representation as word embeddings, which is a widely popular\nmethod nowadays. We move forward by discussing two such methods in more\ndetails.\nContinuous Bag-of-Words and Skip-gram\nMikolov et al. (2013a) proposed two new methods for generating word representa-\ntions using neural networks. In the ﬁrst method, continuous bag of words (CBOW),\nthe model predicts a word ui given the representations of words uj in the context\nwindow. The context window includes words both from the left and the right of\nthe target word.\nThe second proposed architecture is skip-gram with negative sampling. Similar\nto CBOW, the main idea of the model is to predict, given a word ui, whether other\nunits uj are likely to appear in the context window of ui. To summarize, CBOW\npredicts a word given all the words in the context while skip-gram predicts all the\ncontext words given a word.\nMore formally, assume two matrices E, W ∈R|V |×d where Eui is the word\nembedding of ui and Wui is the context embedding of the word. Further, consider\ncp(ui) ⊂V as the set of words in the context window of ui and let cn(ui) ⊂V\nbe a set of random negative samples for the training. The skip-gram model with\nnegative sampling tries to minimize the following objective:\nL(E, W) = −\n|V |\nX\ni=1\nX\nw∈cp(ui)\nlog\n\u0000σ(E⊺\nuiWw)\n\u0001\n−\nX\nw∈cn(ui)\nlog\n\u0000σ(−E⊺\nuiWw)\n\u0001\n,\n(1.2)\n22",
      "page": 22,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.3 Static Representations\nwhere σ : R →R is the sigmoid function. σ(x) = 1/(1 + e−x). Mikolov et al.\n(2013a) used a context window size of C = 10.\nIncorporating Subword Information\nBoth skip-gram and CBOW use only the context of the words for prediction, while\nignoring the internal structure of the words. For example, bank and banks while\nspelled similarly, are assined independent vector representations.\nBojanowski et al. (2017) proposed a method that makes use of the additional\ninformation source of subword structure. They published the implementation of\nthis model under the name fastText. We denote Gk : V →Ck as a function that\nmaps a word to the set of k-grams contained in the word. To allow the model to\ndistinguish preﬁxes and sufﬁxes from other sequences, special boundary symbols\n< and > are added at the beginning and end of words. Considering the word “book”\nand k = 3 as an example, G3(book) = {<bo, boo, ook, ok>}. Let Ck be the set of\nall possible n-grams in V . Each n-gram g in Ck will be represented with a vector\nzg and ﬁnally, for each word ui, Eui in Eq. 1.2 will be replaced with\nEui +\nX\ng∈Gk(ui)\nzg.\n(1.3)\nThis allows the model to integrate the subword information into the word\nrepresentations.\n1.3.2\nMultilingual Representations\nSo far, we have described two static monolingual representation learning models.\nIn this section, we revisit the methods for creating multilingual embedding spaces\nin more detail. For languages e and f with vocabularies Ve and Vf, let us consider\nE(e) and E(f) as the static embeddings with dimensions de and df. For simplicity,\nwe assume de = df =: d.\nIt is not clear what properties should be expected from a multilingual em-\nbedding space. It is desirable that the multilingual representations preserve the\nmonolingual features and can be used in monolingual tasks. However, high quality\nmultilingual representations should also exhibit transferability between different\nlanguage spaces, i.e., if a model is trained on a task with embeddings E(e), it\nshould be able to process E(f) for the same task without a signiﬁcant decrease in\nperformance. One way to achieve this is to require semantically similar units to\nhave similar representations. For example, if monolingual embedding spaces for\nEnglish and German are learned separately, there is no correlation between the\nsimilar words in the two languages. For instance, E(e)\ndrive and E(f)\nfahren are in different\n23",
      "page": 23,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nlaw\nsuit\ndress\nwalk\njudge\nGesetz\nKlage\nAnzug\nKleid\ngehen\nging\nRichter\nGesetz\nKlage\nAnzug\nKleid\ngehen\nging\nRichter\nlaw\nsuit\ndress\nwalk\njudge\nW\nFigure 1.2 – Mapping a German monolingual embedding space into English\nembeddings using a rotation matrix W. Figure taken from Dufter (2021).\nspaces and therefore have random cosine similarity. However, in a multilingual\nspace with embeddings Ee+f, cos-sim(Ee+f\ndrive, Ee+f\nfahren) should be close to 1.\nMapping Approach\nOne popular approach to the creation of multilingual embedding spaces is to\nindividually learn monolingual representations E(e), E(f) for both languages, and\nthen learn a mapping w : Rd →Rd, which will be applied to one or both embedding\nspaces such that ¯E(f)\ni\n:= w(E(f)\ni ), and E(e) and ¯E(f) are in a multilingual space.\nThis is illustrated in Figure 1.2. This mapping approach relies on the assumption\nthat the monolingual spaces have similar structure and a mapping can therefore\nproperly transfer one embedding space to the other (Vuli´c et al., 2020). As the early\nresearch on this approach was done on European languages, which have similar\nstructure and training data (Mikolov et al., 2013b), the facts that many languages\nhave different structures and that their training data comes from various domains\nwere neglected.\nAssume that a bilingual dictionary is available for the target pair of languages\nD := {(u(e)\n1 , u(f)\n1 ), (u(e)\n2 , u(f)\n2 ), . . . , (u(e)\nm , u(f)\nm )},\n(1.4)\nwhere each tuple has a unit from both languages and m is the number of entries in\nthe dictionary. Further, consider creating modiﬁed embedding matrices ˜E(e), ˜E(f)\nthat only contain embeddings from the units in D. A function w(x) = W⊺x with\nW ∈Rd×d, can be used for the mapping by minimizing the loss function\n24",
      "page": 24,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.3 Static Representations\nL(W) = 1\n2\nm\nX\ni=1\n\r\r\r˜E(e)\ni W −˜E(f)\ni\n\r\r\r\n2\n.\n(1.5)\nThis optimization problem is an instance of the Procrustes Problem. Mikolov\net al. (2013b) used a gradient descent method for this optimization and showed\nthat the multilingual spaces created by this method can perform well for word\ntranslation.\nAn extension to this approach is to constrain the matrix W to be orthonormal,\ni.e., W⊺W = I (Xing et al., 2015). This transformation does not modify the\nstructure of the embedding space which is desirable. This constraint will change\nEq. 1.5 to the Orthogonal Procrustes Problem (Schönemann, 1966), which can\nbe solved with singular value decomposition (SVD). By computing the SVD of\nmatrix (˜E(f))⊺˜E(e) that is (˜E(f))⊺˜E(e) = UΣV⊺, with E, V as complex unitary\nmatrices, and Σ as a rectangular diagonal matrix with non-negative real numbers\non the diagonal, the transformation is given by W∗= VU⊺.\nThe aforementioned approaches are effective when a bilingual dictionary is\navailable. In the unsupervised setting, the underlying challenge is that E(e) and\nE(f) are unaligned across both axes: neither the ith vocabulary item E(e)\ni∗and E(f)\ni∗\nnor the jth dimension of the embeddings E(e)\n∗j and E(f)\n∗j are aligned.\nSince creating these dictionaries is challenging for many language pairs, un-\nsupervised embedding alignment approaches gained popularity, in particular the\napproach introduced by Lample et al. (2018). They proposed a generative adver-\nsarial learning method following the training method of Goodfellow et al. (2014).\nConsider a discriminator model with parameters θD, where fθD(z) shows the prob-\nability of vector z being an embedding from language e. Values output by the\ndiscriminator that are closer to zero show that z is an embedding from language f.\nThe discriminator and the mapping losses are written as\nLD(θD|W) = −1\nn\nn\nX\ni=1\nlog\n\u0010\nfθD(WE(e)\ni )\n\u0011\n−1\nm\nm\nX\ni=1\nlog\n\u0010\n1 −fθD(E(f)\ni )\n\u0011\n(1.6)\nLG(W|θD) = −1\nn\nn\nX\ni=1\nlog\n\u0010\n1 −fθD(WE(e)\ni )\n\u0011\n−1\nm\nm\nX\ni=1\nlog\n\u0010\nfθD(E(f)\ni )\n\u0011\n(1.7)\nThe transformation W is trained with the mapping loss so that the discriminator\nis unable to predict the original language of an embedding. During the training,\nboth the discriminator and the transformation matrix are trained alternatingly with\ngradient descent. The authors also propose a reﬁnement procedure that uses well-\naligned units as a noisy dictionary which is used in a Procrustes solution to ﬁnd a\nﬁnal transformation.\n25",
      "page": 25,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nArtetxe et al. (2018) shows that the generator-discriminator model is rather\nunstable and frequently fails to ﬁnd a transformation. Thus they introduced a model\ncalled Vecmap that exploits the structural similarity of the embeddings and a robust\nself-learning algorithm that iteratively improves this solution.\nJoint Learning\nThe main assumption of the mapping approaches is that the embedding spaces\nE(e), E(f) are monolingual representations that have been already learned individ-\nually for both languages. Joint learning approaches introduce other embedding\nlearning algorithms that aim to learn E(e), E(f) simultaneously in the same multi-\nlingual space.\nMany joint learning methods employ a parallel corpus with two or more lan-\nguages. Assume that this corpus consists of n parallel sentences in two languages\nS(e) = (s(e)\n1 , s(e)\n2 , . . . , s(e)\nn ), S(f) = (s(f)\n1 , s(f)\n2 , . . . , s(f)\nn ) where each sentence pair\ns(e)\ni\n= (u(e)\n1 , . . . , u(e)\nl(e)\ni ), s(f)\ni\n= (u(f)\n1 , . . . , u(f)\nl(f)\ni ) consists of two sentences that are\ntranslations of each other, and l(e)\ni , l(f)\ni\nare the number of units in each sentence.\nThe Parallel Bible Corpus (PBC) (Mayer and Cysouw, 2014) and the Proceedings\nof the European Parliament (Koehn, 2005) are two examples of a parallel corpus.\nBoth of these corpora are also multi-parallel, meaning that the translations of each\nsentence is provided in multiple languages, more than a thousand in the case of\nPBC.\nAs a simple approach of joint learning, Vuli´c and Moens (2015) proposed to cre-\nate pseudo-bilingual sentences, where the sentences s(e)\ni\nand s(f)\ni\nare concatenated\nand\nrandomly\nshufﬂed.\nFor\ninstance,\nwith\ntwo\nsentences\n“Ich habe eine Katze” and “I have a cat”, the pseudo-bilingual sentence could be\n“eine I Katze habe Ich cat a have”. Afterwards, a monolingual learning algorithm,\nsuch as skip-gram, is trained on the resulting corpus. The intuition behind this\napproach is that the monolingual learning algorithms treat the sentences as context\nfor words. Creating pseudo-bilingual sentences will allow the model to treat the\nwords from all languages as the context. In the example sentence, the tokens\nhave and habe will more likely occur in similar contexts and therefore will have\nsimilar vector representations. While there are certainly better ways of constructing\npseudo-bilingual sentences, this work only considered random shufﬂing. Other\nworks learn distinct embedding models for the source and the target languages\nthat keep the sentence orders, while jointly learning a cross-lingual regularizer,\nenforcing word pairs aligned in the parallel text to have similar representations\n(Klementiev et al., 2012; Gouws et al., 2015). The word pairs are obtained from a\ndictionary with 1:1 mappings between source and target words.\nLevy et al. (2017) proposed an algorithm based on sentence IDs, where the\n26",
      "page": 26,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.4 Contextualized Representations\nintuition is similar to pseudo-bilingual sentences: units across languages that occur\nin similar sentences in a parallel corpus are likely to have a similar meaning. In this\napproach, instead of using the individual words as context, the parallel sentences\nwill be represented with a special token and used as context. A new corpus will be\nconstructed, where each sentence consists of a pair of tokens. The ﬁrst token in\neach pair is the special token of the parallel sentence and the second token is one\nof the words in that sentence. After training the skip-gram learning method on the\nﬁnal corpus, the words that occur in similar sentences will have more matching\nsentence IDs as their context and thus similar vector representations.\nAnother approach for joint training uses matrix factorization to create the\nrepresentations. In this approach, an inverted index of words and sentences is\ncreated, where the vector for each word has the same dimensionality as the number\nof sentences in the corpus. Subsequently, the matrix factorization is used to reduce\nthe dimensionality of word vectors to smaller numbers (Søgaard et al., 2015;\nJalili Sabet et al., 2016).\n1.4\nContextualized Representations\nStatic representations are created as mappings e : V →Rd, which assign a\nparticular vector to each unit in the vocabulary (Mikolov et al., 2013a). To give\nan example, in static models, the unit bank is always represented by the same\nembedding vector, whether it occurs in the phrase river bank (where its meaning\nis “the land alongside or sloping to a river or lake”) or the bank account (where\nits meaning is “a ﬁnancial establishment”). It is up to debate whether aggregating\nmultiple meanings of a unit and representing them with a single embedding vector\nis problematic. Neelakantan et al. (2014) approaches this issue with learning\nseparate vectors for each word sense.\nContextualized representation methods attempt to take into account the context\nin which a unit appears (McCann et al., 2017). We can deﬁne a contextualized\nembedding function as\ne : V tmax →Rtmax×d,\n(1.8)\nwhere tmax is the maximum number of units that the function can process at once.\nThis number can vary between the size of a phrase, a sentence, a paragraph or a\ndocument. Therefore, the contextualized embedding of the unit ui in a sentence\n(u1, . . . , ui, . . . ut) depends on all other units in the sentence. In the case of the\nabove example, the two different contexts will result in different contextualized\nrepresentations for the unit bank.\nThe model proposed by Peters et al. (2017) uses bidirectional LSTMs (Hochre-\niter and Schmidhuber, 1997) to process the text in a forward and reverse fashion,\n27",
      "page": 27,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nwhich means the model can potentially use all the previous and all the following\nwords as context. Recent transformer-based language models, such as Devlin et al.\n(2019), use self-attention operations, and their computation costs scale quadrati-\ncally with the sequence length. This means that these models are not optimized to\nprocess long sequences. A solution for this issue is to reduce their cost closer to\nlinear complexity (Beltagy et al., 2020).\n1.4.1\nMonolingual Representations\nThis section presents various methods of learning contextualized monolingual\nrepresentations.\nPretrained Language Models\nPeters et al. (2017) is one of the ﬁrst works that attempted to learn contextualized\nvector representations for words. The key idea is to train a neural language model\nwithout the need for a labeled training dataset. The hidden states of the model can\nthen be used as contextualized embeddings.\nA language model is a probability distribution over strings of text. It takes\nunstructured text as input for training and estimates the probability of a sequence of\nunits occurring in that text. Formally, it models the probability P(u1, u2, . . . , ut).\nAs text is sequential in nature and in order to simplify the estimation of the\nprobability distribution, the chain rule of probabilities is commonly applied in\nvarious approaches. The resulting forward language model is given by\nP(u0, u1, u2, . . . , ut, ut+1) = P(u0)\ntY\ni=0\nP(ui+1|u≤i),\n(1.9)\nwhere u≤i are all tokens with index j ≤i, and u0, ut+1 are extra tokens that indicate\nthe start and end of a sequence. Analogously, the backward language model can be\nformulated as\nP(u0, u1, u2, . . . , ut, ut+1) = P(ut+1)\ntY\ni=0\nP(ui|u≥i+1).\n(1.10)\nThe probability model Pθ(ui|u<i) can be parameterized with different model\narchitectures such as recurrent neural networks (RNNs). A RNN is a model that is\nrecursively computed as\nh(i) = σh(W(u)e(i) + W(h)h(i−1) + b(h))\n(1.11)\ny(i) = σy(W(y)h(i) + b(y))\n(1.12)\n28",
      "page": 28,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.4 Contextualized Representations\nfor i = 1, . . . , n, where Pθ(ui|u<i) = ˆy(i)\nui is the probability of the unit ui occurring\nat position i,\nθ = (W(u), W(h) ∈Rd×d; h(0), b(h) ∈Rd; E, W(y) ∈Rn×d; b(y) ∈Rn) (1.13)\nare the parameters of the model. The W’s are weight matrices and the b’s are\nbiases. e(i) is an embedding vector for the unit ui, i.e., e(i) = Eui, h(i) is the\nhidden layer representation of the unit ui, and σ : Rm →Rm are activation\nfunctions. A common choice is to apply tangens hyperbolicus to the hidden\nlayer vectors σh(x)k = tanh(xk) component-wise, and the softmax function\nσy(x)k = exk/ Pn\ni=1 exi to generate the probability over tokens in the output vector.\nNegative cross entropy between the ˆy(i) and the observed units is commonly used\nas the objective function. The latter is formulated as:\nL(θ, U) = −1\nm\nm\nX\nk=1\ntk\nX\ni=1\nCE(y(i), ˆy(i)) = −1\nm\nm\nX\nk=1\ntk\nX\ni=1\nlog(ˆy(i)\nui ),\n(1.14)\nwhere U = (s1, . . . , sm) is a corpus with m sentences with sk = (u1, . . . , utk),\nand y(i) is a one-hot vector. This objective function can then be minimized using\nstochastic gradient descent. One of the popular variants of RNN models is Long\nShort-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) which has\nshown good performance for language modeling.\nUsing either a forward or a backward language model neglects one side of the\nsequence while creating the contextualized representations. Peters et al. (2017)\nproposed to train a language model that consists of both forward and backward\nLSTMs. They considered the concatenation of the hidden states of the LSTMs as\nembeddings and used them together with static embeddings as input to models for\nsolving downstream tasks, such as named entity recognition. This approach yielded\nstate of the art performance and was further developed in their next paper: Peters\net al. (2018) introduced deep contextualized embeddings called Embeddings from\nLanguage Models (ELMo), which outperformed the state of the art performance\nacross many tasks. In ELMo, bidirectional LSTMs are trained with the task of\nlanguage modeling.\nThe advantage of these models is that language models do not need any man-\nually labeled data and can be (pre-)trained on large amounts of text data, which\ncould for example be sourced from the Internet. These models can then be used for\ndownstream tasks. This motivates the name Pretrained Language Models (PLMs).\nTransformer Models\nRecurrent neural networks take advantage of the inherent sequential nature of text.\nHowever, studies show that recurrent neural networks cannot properly propagate\n29",
      "page": 29,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\na) Scaled Dot-Product\nAttention\nb) Multi-Head Attention\nc) Transformer Encoder\nBlock\nFigure 1.3 – Transformer model. a) Scaled Dot-Product Attention. b) Multi-\nHead Attention consists of several attention layers running in parallel. c)\nSchema of a transformer encoder block that can be repeated for l layers.\nFigure taken from Vaswani et al. (2017)\ninformation across long spans of text, as analyzed for example by Cho et al. (2014)\nfor machine translation. Bahdanau et al. (2015) proposed using other extensions\nsuch as attention to overcome this issue.\nA key milestone in this area is Vaswani et al. (2017), where the authors intro-\nduced a machine translation system that only uses the attention mechanism, and\nthis model exhibits superior performance compared to recurrent neural networks.\nThey proposed to stack Transformer Encoder Blocks as shown in Figure 1.3. These\nblocks are functions tθ : Rn×d →Rn×d with tθ(X) =: Z, which is computed as\nfollows:\nQ, K, V = XW(q), XW(k), XW(v)\nA = Softmax(QK⊺\n√dh\n)\nM = AV\nO = LayerNorm1(M + X)\nF = ReLU(OW(f1) + b(f1))W(f2) + b(f2)\nZ = LayerNorm2(F + O),\n(1.15)\nwhere Q, K, V are respectively projections of the embeddings for queries, keys,\nand values, A is the self attention matrix, and the softmax function is applied\n30",
      "page": 30,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.4 Contextualized Representations\nrow-wise. LayerNorm(X)i = g ⊙(Xi −µ(Xi))/σ(Xi)+b is layer normalization\nwith µ(x), σ(x) returning the mean and standard deviation of a vector and g, b as\nparameters (Ba et al., 2016), and ReLU(X) = max(0, X). The parameters of such\na block are\nθ = (W(q), W(k), W(v) ∈Rd×d; g(1), g(2), b(1), b(2) ∈Rd;\nW(f1) ∈Rd×df; W(f2) ∈Rdf×d; b(f1) ∈Rdf; b(f2) ∈Rd),\n(1.16)\nwhere d is called the hidden dimension, df the intermediate dimension, and n is\nthe sequence length. Multi-head attention runs several attention layers in parallel\nand concatenates their results. In this setting, each of the h heads learns its own\nprojection matrices for queries, keys, and values, that is W(q), W(k), W(v) ∈Rd×dh\nwhere d = hdh. The output matrices M(h) ∈Rt×dh are then concatenated along\nthe second dimension to obtain the ﬁnal M.\nAs an example, consider how a Transformer is applied to text data, for the\nsequence U = (u1, u2, . . . , ut). The token embeddings T ∈Rt×d are created by\na projection of the token IDs to the embedding matrix E ∈Rv×d, where v is the\nvocabulary size. The Transformer block, speciﬁcally the multihead attention, does\nnot take into consideration the position information in a sequence and is therefore\ninvariant to reorderings of the input. The way the attention mechanism is designed\nmakes it suitable to operate on sets (Lee et al., 2019). Therefore, the sequential\ninformation has to be injected into the model with a different way. The simplest\nmethod is to add positional encodings to the input embeddings by creating a matrix\nof absolute position embeddings P ∈Rt×d, and changing the ﬁnal input to Tθ to\nU + P. In some models additional embeddings such as language embeddings or\nlayer embeddings are also added to the input.\nTransformer-Based Language Models\nDevlin et al. (2019) proposed a new language model Bidirectional Encoder Rep-\nresentations from Transformers (BERT). BERT is pretrained on a new variant of\nlanguage modeling. Most of the previously proposed language modeling meth-\nods use the sequential nature of text and learn a unidirectional model. The main\ninnovation of BERT is that it is not a unidirectional language model, but rather\nbidirectional. Instead of only utilizing the right or left context of a word, BERT\nhas access to context words on both sides simultaneously. This is realized by the\nuse of masked language modeling (MLM).\nConsider a corpus U = (u1, . . . , ut). The MLM objective is to receive a cor-\nrupted version of U, here U ′, as input, and attempt to reconstruct U. The corrupted\ninput U ′ is created by randomly altering some tokens in U. For instance, in Devlin\net al. (2019), each token ui has the probability 0.15 to be corrupted, and the model\n31",
      "page": 31,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nlearns to reconstruct such tokens. The possible changes applied to corrupted tokens\nare: (i) replacing it with the special token [MASK], (ii) replacing it with another\nrandom token from the vocabulary, and (iii) keeping the token as is. This process\ncan be deﬁned by two sequences of independent and identically distributed (iid)\nrandom variables (R(1)\ni )i=1,...,t, (R(2)\ni )i=1,...,t with uniform distributions on [0, 1].\nThen U ′ is created by\nu′\ni =\n\n\n\n\n\n\n\n\n\n[MASK]\nif R(1)\ni\n≤ρ1 ∧R(2)\ni\n≤ρ2\nur\nif R(1)\ni\n≤ρ1 ∧ρ2 < R(2)\ni\n≤ρ3\nui\nif R(1)\ni\n≤ρ1 ∧ρ3 < R(2)\ni\nui\notherwise,\n(1.17)\nwhere ur is a randomly sampled unit from the vocabulary and [MASK] is a special\ntoken. The values used in Devlin et al. (2019) are ρ1 = 0.15, ρ2 = 0.8, ρ3 = 0.9.\nThe MLM uses this modiﬁed corpus as input to a Transformer. The original\ninput X can then be used as the targets for prediction. The ﬁnal hidden vectors\ncorresponding to the corrupted tokens are fed into an output softmax over the\nvocabulary. In contrast to previous language modeling methods, the model only\npredicts the masked tokens, rather than reconstructing the entire input. Although\nMLM is useful for learning a bidirectional pre-trained model, one downside is\nthat this creates a mismatch between pre-training and ﬁne-tuning. This is caused\nby the fact that the [MASK] tokens only appear during pre-training and do not\nappear during ﬁne-tuning. To mitigate this issue, the corrupted tokens are not\nalways replaced with [MASK] tokens, but instead are sometimes left unchanged or\nreplaced by a random token. All of the corruped tokens are then used to predict the\noriginal token with cross entropy loss.\nOther technical aspects were taken into consideration during training. Using\nwords as the basic units requires large vocabularies, which leads to high memory\nconsumption. To mitigate this, BERT uses the wordpiece tokenizer (Schuster and\nNakajima, 2012) to split the text into subword units. With this, the vocabulary size\ncan be adjusted before training the model, which reduces the memory requirements\nof the embedding matrix E, while at the same time enabling the model to cover\nvarious text sequences. For pre-training, the Adam optimizer (Kingma and Ba,\n2015) is used and regularization methods such as dropout (Srivastava et al., 2014)\nare applied. The model also includes a second loss term, next sentence prediction,\nwhich aims at generating better sentence representations. However, experiments\nlater showed that it has no impact on the performance of the model (Liu et al.,\n2019). Hence, in the follow-up works, this term is mostly excluded from the overall\nloss.\n32",
      "page": 32,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.5 Evaluation\n1.4.2\nMultilingual Representations\nBERT presented a method for learning high-quality contextualized representa-\ntions. This section discusses methods for training multilingual contextualized\nrepresentations.\nJoint Training\nThe authors of Devlin et al. (2019) have also published a multilingual version of\nBERT (mBERT). This model is trained on Wikipedia across multiple languages,\nselected for largest amount of data. In case of mBERT, the 104 languages with the\nlargest amount of available text on Wikipedia were used for training. The articles\nfrom these languages are concatenated and shufﬂed and a shared vocabulary across\nall these corpora is learned. Afterwards, a standard BERT model is trained on this\ndata.\nIn this method, the model does not use any explicit crosslingual supervision.\nThis means no parallel data or dictionary is used, and the loss function does not\ninclude any term that contains special signals for multilinguality. The underlying\nidea behind the multilinguality in this approach is that the tokens in the shared\nvocabulary can appear in multiple languages. As an example, the word ﬁnd appears\nboth in the English word ﬁnding and the German word ﬁnden. Experiments show\nthat this model yields promising multilingual representations when evaluated with\nthe methods described in Section 1.5.\nOther works attempted to further improve mBERT’s multilinguality. Conneau\nand Lample (2019) proposed a new loss term, Translation Language Modeling\n(TLM) that uses parallel sentences. In this model, a pair of parallel sentences are\nused as the input of the Transformer with a similar task of predicting the masked\ntokens, so that the model can also use the sentence in the other language as context.\nThe intuition is that this can help increase the multilinguality of the model. In\nfollow-up work, Conneau et al. (2020a) propose (XLM-R), which does crosslingual\nlearning at scale by using more data, and drops the next sequence prediction loss\nterm. Similar to static representation mapping approaches, Conneau et al. (2020b)\nshow that monolingual contextualized embeddings can also be mapped into a\ncommon embedding space using linear transformations.\n1.5\nEvaluation\nPrevious sections have discussed how to learn static and contextualized represen-\ntation models for both monolingual and multilingual settings. In this section we\nstudy different methods to evaluate the quality of such representations.\n33",
      "page": 33,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nThere are intrinsic and extrinsic evaluations for assessing the quality of mono-\nlingual static embeddings. The intrinsic evaluations include tasks for analyzing\ndifferent properties of the embeddings, and they range from word similarity, e.g.,\n(Hill et al., 2015; Gerz et al., 2016), and word analogy (Mikolov et al., 2013c; Glad-\nkova et al., 2016), to correlation with linguistic features (Tsvetkov et al., 2015). In\nextrinsic evaluations, the main objective is a downstream task and the embeddings\nare used as input to a model that solves the task. Examples of downstream tasks\nare part-of-speech tagging and named entity recognition.\nMonolingual contextualized embeddings can also be evaluated using perplexity\nin language modeling. However, to better understand the quality of the contextual-\nization, there is often a range of extrinsic tasks for which the model is ﬁnetuned\nand then evaluated, e.g., the GLUE or SuperGLUE benchmarks (Wang et al., 2018,\n2019) for natural language understanding, SQUAD and SQUAD 2.0 benchmarks\n(Rajpurkar et al., 2016, 2018) with questions for machine comprehension of text,\nthe natural language inference (NLI) task (Bowman et al., 2015), and common\ntasks like named entity recognition.\nMultilingual representations can be applied to many use cases, which in turn\ncan be used for evaluating the representation models.\n1. Translation. The representations of text units such as words, phrases, or\nsentences, that are semantically similar, should be close to each other across\nlanguages. This means that these representations can be used for translation\nor other applications including word alignment and crosslingual sentence\nretrieval.\n2. Zero-shot transfer. Consider a model that is only trained on a speciﬁc\ndataset or language, e.g., English, for a downstream task such as part-of-\nspeech tagging. If this model, without additional training, can be applied to\nother languages and is able to correctly tag words in non-English sentences,\nthe model is performing zero-shot transfer across languages. Using multilin-\ngual representations can enable the models to achieve this. This is a desirable\nquality of these representations since annotating training data, especially for\nmore complex tasks, requires labor, which is costly and time-consuming. If\nwe aim to annotate data for tens or hundreds of languages, the costs quickly\nbecome infeasible. For this reason, zero-shot transfer is a common trend in\nmodern NLP.\n3. Low-resource coverage. Training datasets of various tasks are mostly an-\nnotated for the English language. On the other hand, a large number of\nlanguages are low-resource: there are only a few or no datasets available for\nthem. One of the main objectives of multilingual NLP is to use the represen-\ntations to make a multilingual model that exhibits sensible improvements in\n34",
      "page": 34,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.5 Evaluation\nSki excursions are excellent .\nSkiausﬂüge sind hervorragend .\nFigure 1.4 – Example of a word alignment. Figure taken from Jalili Sabet et al.\n(2020)\nperformance for low-resource languages. Another advantage of such models\nis that it is much easier to maintain a single model than to have multiple\nlanguage-speciﬁc models.\nIn this thesis we intend to cover all these use cases with different experiments:\nthe word alignment task covers the ﬁrst use case as an intrinsic evaluation, and\npart-of-speech tagging with annotation projection as an extrinsic evaluation covers\nthe other two use cases.\n1.5.1\nWord Alignment\nWord alignment is a task where translations of words in two parallel sentences\nshould be identiﬁed, as in the example shown in Figure 1.4. Consider a corpus\nwith parallel sentences U = {(s(e)\n1 , s(f)\n1 ), (s(e)\n2 , s(f)\n2 ), . . . , (s(e)\nm , s(f)\nm )}. For a parallel\nsentence pair s(e)\ni , s(f)\ni , the word alignment of the whole sentence can be considered\nas a bipartite graph, where the units in s(e)\ni\nand s(f)\ni\nare the nodes in the graph\ndenoted by V (e)\ni\n, V (f)\ni\n. In some datasets, instead of one type of alignment, there are\ntwo sets of sure and possible edges, Si, Pi ⊂V (e)\ni\n× V (f)\ni\nwhere Si ⊂Pi. The task\nis to automatically predict the edges that were marked as correct in the manually\ncreated gold standard. The model generates a set of prediction edges Ai, while\nthe edge sets without index denote the union of word alignments across sentences,\ni.e., S = Sm\ni=1 Si. Standard evaluation metrics are then precision, recall, F1 and\nalignment error rate (AER) (Och and Ney, 2000) computed by\nprecision = |A ∩P|\n|A|\n, recall = |A ∩S|\n|S|\n,\nAER = 1 −|A ∩S| + |A ∩P|\n|A| + |S|\n,\nF1 = 2 precision × recall\nprecision + recall .\n(1.18)\nSince word alignment is also an important step in the statistical machine\ntranslation (SMT) pipeline, there are several works that proposed different methods\n35",
      "page": 35,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nfor generating high quality word alignments. Most statistical methods are either an\nimplementation of the IBM alignment models or inspired by them (Brown et al.,\n1993). Namely, Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013), as well\nas follow-up models such as eﬂomal (Östling and Tiedemann, 2016), are widely\nused for word alignment.\nMore recently, several works attempted to use multilingual representations for\nthis task. In this set of approaches, a simple method to induce the alignment edges\ncould be to match each unit with another unit in the parallel sentence, such that the\ncorresponding embeddings have the highest similarity. Most such models require\ntraining or ﬁnetuning on the same task, or similar tasks such as translation. Garg\net al. (2019) pursued a multitask approach for alignment and translation, while\nin other works only the word alignment task is used for end-to-end training or\nﬁnetuning, and only new loss terms help the model to learn better representations\n(Zenkel et al., 2020; Dou and Neubig, 2021).\nBoth statistical and neural approaches have their advantages for different lan-\nguage pairs. However, the output alignments of these models can be aggregated\nto form an ensemble method for word alignment with better performance (Stein-\ngrímsson et al., 2021).\n1.6\nConclusion\nThis introductory chapter has described the main concepts of multilingual rep-\nresentations that are relevant to this thesis. We presented mathematical notation\nand linguistic foundations, which were used to introduce the existing static and\ncontextualized representation learning methods. We discussed the approaches for\nachieving multilinguality through joint training or mapping. Finally, we described\nthe evaluation methods with possible use cases for multilingual models. The\nnext chapters use the presented methods in a series of research papers and aim to\nimprove the performance of different tasks for low-resource languages.\n1.6.1\nContributions\nIn light of the three research questions posed at the beginning, we can categorize\nand summarize our contributions in this thesis as follows.\ni) Data: In Chapter 2, we introduce SimAlign, a high-quality word alignment\ntool that uses static and contextualized embeddings and does not require\nparallel training data. We examine multiple datasets and show that, with only\nusing monolingual training data, our unsupervised model performs better\nthan popular supervised models. We train statistical word aligners, such as\n36",
      "page": 36,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1.6 Conclusion\nfast-align, eﬂomal, and Giza++, with several training data sizes and show the\neffect of training data size on word alignment performance. Our experiments\nreveal that when we use SimAlgin with Vecmap embeddings, the alignment\nquality is on par with fast-align trained on ten thousand sentence pairs, while\nusing SimAlign with mBERT performs better than all statistical aligners\ntrained with more than 1 million sentence pairs. In Chapter 4, we show that\ntraining on multi-parallel corpora can enable the word aligner to improve its\nperformance over bilingual corpora. Interestingly, the experiments reveal\nthat multi-parallel corpora created by translations can still contribute to better\nalignment performance for the target language pair.\nii) Models: We incorporate various signals into existing word alignment models\nand introduce a new word aligner, SimAlign, that does not require supervi-\nsion or parallel training data. We use contextualized representations trained\non monolingual corpora as features for a word aligner (Chapter 2). We study\nthe graph structures and use parallel sentences in a multi-parallel corpus\nfor extra information in order to generate better bilingual word alignments\n(Chapter 4). In another work, subword structures and different granularities\nof text are studied (Chapter 6). We show that the aggregation of subword\nsamplings of a language (subword models with different vocabulary sizes)\ncan improve the quality of word-level alignments. We improve the state-\nof-the-art for the word alignment task in several evaluation datasets, while\naiming to use little or no training data (Chapter 2, Chapter 5).\niii) Analysis: We build a tool named ParCourE to study languages and their word\nconnections to better understand the quality of representations (Chapter 3).\nParCourE can also help linguists as an interactive explorer for studying\nlow-resource languages in PBC. In Chapter 4, we investigate the effect\nof anchor languages on bilingual word alignments of a target language\npair and show that using similar languages as anchors is more effective\nfor improving performance. Furthermore, we explore the effect of using\nsimilar granularities of text learned for a language pair and applying it to\nother languages for word alignment (Chapter 6). The experiments show that\nsuch features can be transferred to similar languages and improve the model\nperformance.\n1.6.2\nFuture Work\nWe now describe future work and related literature that address our research\nquestions.\nTraining a pretrained multilingual language model that covers more than 1000\nlanguages can be beneﬁcial for the performance of NLP tasks in low-resource\n37",
      "page": 37,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1. Introduction\nlanguages. The popular pretrained language models mostly use some form of\nsubword tokenization. Since large amounts of text are not available for low-\nresource languages, this puts such languages at a disadvantage when subword\ntokenization methods are learning a shared vocabulary (Maronikolakis et al., 2021).\nOne way to solve this issue is to use character-level encoders. Other works have\ntried to replace subwords by using visual representations (Salesky et al., 2021),\ndownsampling characters (Clark et al., 2021), and using deep character encoders\n(Xue et al., 2021). These models need large datasets for training. The next step\ntowards character-level encoders can be to use multi-parallel corpora, and the\nbilingual signals in such corpora, to enable the models to converge faster and with\nless training data.\nOur work in Chapter 4 shows that using multi-parallel corpora can boost the\nperformance of word alignment for low-resource and distant languages by using\nother languages as anchors. We used graph algorithms that only rely on the\ninformation from the graph of words. This means that important considerations of\nword aligners, such as the distortion (introduced in IBM Model 2), and the context\nof the sentence, are neglected in the current model. Having a better encoding of\nthe graph of languages might be beneﬁcial for creating better word representations\n(nodes in the graph), and alignment prediction (edges in the graph). Using graph\nneural networks (GNN) (Scarselli et al., 2009) could be the key to solving this\nchallenge. The GNN encoders can take the representations of words and their\npositions as input. It is also possible to include additional information as input\nto GNNs, such as part-of-speech tags of the words and the dependency trees of\nthe sentences. Furthermore, by adding language identiﬁers as features for word\nrepresentations, the GNN encoder can learn which anchor languages to use for\neach word. All of this suggests that by using proper features and models, the\nperformance of word alignment can be further improved.\nUsing the graph of multiple languages can also be used for tasks other than\nword alignment. The word alignment edges can be used to project tags, such as\npart-of-speech tags and semantic role labels, from several languages to a target\nlanguage (Agi´c et al., 2016). This method can create a training dataset for a\ntarget low-resource language while only parallel data and proper taggers for other\nlanguages (e.g., English and German) are available. It is worth investigating the\neffect of using GNN encoders for annotation projection and creating training\ndatasets for all languages in a multi-parallel corpus such as PBC.\n38",
      "page": 38,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Chapter 2\nSimAlign: High Quality Word\nAlignments Without Parallel\nTraining Data Using Static and\nContextualized Embeddings\n39",
      "page": 39,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1627–1643\nNovember 16 - 20, 2020. c⃝2020 Association for Computational Linguistics\n1627\nSimAlign: High Quality Word Alignments Without Parallel Training Data\nUsing Static and Contextualized Embeddings\nMasoud Jalili Sabet∗1, Philipp Dufter∗1, Franc¸ois Yvon2, Hinrich Sch¨utze1\n1 Center for Information and Language Processing (CIS), LMU Munich, Germany\n2 Universit´e Paris-Saclay, CNRS, LIMSI, France\n{masoud,philipp}@cis.lmu.de,francois.yvon@limsi.fr\nAbstract\nWord alignments are useful for tasks like sta-\ntistical and neural machine translation (NMT)\nand cross-lingual annotation projection. Statis-\ntical word aligners perform well, as do meth-\nods that extract alignments jointly with trans-\nlations in NMT. However, most approaches\nrequire parallel training data, and quality de-\ncreases as less training data is available. We\npropose word alignment methods that require\nno parallel data.\nThe key idea is to lever-\nage multilingual word embeddings – both\nstatic and contextualized – for word alignment.\nOur multilingual embeddings are created from\nmonolingual data only without relying on any\nparallel data or dictionaries. We ﬁnd that align-\nments created from embeddings are superior\nfor four and comparable for two language pairs\ncompared to those produced by traditional sta-\ntistical aligners – even with abundant parallel\ndata; e.g., contextualized embeddings achieve\na word alignment F1 for English-German that\nis 5 percentage points higher than eﬂomal, a\nhigh-quality statistical aligner, trained on 100k\nparallel sentences.\n1\nIntroduction\nWord alignments are essential for statistical ma-\nchine translation and useful in NMT, e.g., for im-\nposing priors on attention matrices (Liu et al.,\n2016; Chen et al., 2016; Alkhouli and Ney, 2017;\nAlkhouli et al., 2018) or for decoding (Alkhouli\net al., 2016; Press and Smith, 2018). Further, word\nalignments have been successfully used in a range\nof tasks such as typological analysis (Lewis and\nXia, 2008; ¨Ostling, 2015b), annotation projection\n(Yarowsky et al., 2001; Pad´o and Lapata, 2009;\nAsgari and Sch¨utze, 2017; Huck et al., 2019) and\ncreating multilingual embeddings (Guo et al., 2016;\nAmmar et al., 2016; Dufter et al., 2018).\n∗Equal contribution - random order.\nDer Pinguin Nils Olav wurde vom norwegischen König zum Ritter geschlagen\nPingvin Nils Olav Norvegiya qiroli tomonidan ritsar edi\nSir Nils Olav III. ですペンギンknighted by el rey noruego\nNils Olav der Dritte is a penguin nominato cavaliere par un roi norvégien\nFigure 1: Our method does not rely on parallel train-\ning data and can align distant language pairs (German-\nUzbek, top) and even mixed sentences (bottom). Exam-\nple sentence is manually created. Algorithm: Itermax.\nStatistical word aligners such as the IBM mod-\nels (Brown et al., 1993) and their implementations\nGiza++ (Och and Ney, 2003), fast-align (Dyer\net al., 2013), as well as newer models such as eﬂo-\nmal ( ¨Ostling and Tiedemann, 2016) are widely used\nfor alignment. With the rise of NMT (Bahdanau\net al., 2014), attempts have been made to interpret\nattention matrices as soft word alignments (Cohn\net al., 2016; Koehn and Knowles, 2017; Ghader\nand Monz, 2017). Several methods create align-\nments from attention matrices (Peter et al., 2017;\nZenkel et al., 2019) or pursue a multitask approach\nfor alignment and translation (Garg et al., 2019).\nHowever, most systems require parallel data (in suf-\nﬁcient amount to train high quality NMT systems)\nand their performance deteriorates when parallel\ntext is scarce (Tables 1–2 in (Och and Ney, 2003)).\nRecent unsupervised multilingual embedding al-\ngorithms that use only non-parallel data provide\nhigh quality static (Artetxe et al., 2018; Conneau\net al., 2018) and contextualized embeddings (De-\nvlin et al., 2019; Conneau et al., 2020). Our key\nidea is to leverage these embeddings for word align-\nments – by extracting alignments from similarity\nmatrices induced from embeddings – without rely-\ning on parallel data. Requiring no or little paral-\nlel data is advantageous, e.g., in the low-resource\ncase and in domain-speciﬁc settings without par-\nallel data. A lack of parallel data cannot be easily\n40",
      "page": 40,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1628\nremedied: mining parallel sentences is possible\n(Schwenk et al., 2019) but assumes that compara-\nble, monolingual corpora contain parallel sentences.\nFurther, we ﬁnd that large amounts of mined par-\nallel data do not necessarily improve alignment\nquality.\nOur main contribution is that we show that\nword alignments obtained from multilingual pre-\ntrained language models are superior for four and\ncomparable for two language pairs, compared to\nstrong statistical word aligners like eﬂomal even\nin high resource scenarios. Additionally, (1) we\nintroduce three new alignment methods based on\nthe matrix of embedding similarities and two ex-\ntensions that handle null words and integrate posi-\ntional information. They permit a ﬂexible tradeoff\nof recall and precision. (2) We provide evidence\nthat subword processing is beneﬁcial for aligning\nrare words. (3) We bundle the source code of our\nmethods in a tool called SimAlign, which is avail-\nable.1 An interactive online demo is available.2\n2\nMethods\n2.1\nAlignments from Similarity Matrices\nWe propose three methods to obtain alignments\nfrom similarity matrices. Argmax is a simple base-\nline, IterMax a novel iterative algorithm, and Match\na graph-theoretical method based on identifying\nmatchings in a bipartite graph.\nConsider parallel sentences s(e), s(f), with\nlengths le, lf in languages e, f. Assume we have\naccess to some embedding function E that maps\neach word in a sentence to a d-dimensional vector,\ni.e., E(s(k)) ∈Rlk×d for k ∈{e, f}. Let E(s(k))i\ndenote the vector of the i-th word in sentence s(k).\nFor static embeddings E(s(k))i depends only on the\nword i in language k whereas for contextualized\nembeddings the vector depends on the full context\ns(k). We deﬁne the similarity matrix as the matrix\nS ∈[0, 1]le×lf induced by the embeddings where\nSij := sim\n\u0000E(s(e))i, E(s(f))j\n\u0001\nis some normal-\nized measure of similarity, e.g., cosine-similarity\nnormalized to be between 0 and 1. We now de-\nscribe our methods for extracting alignments from\nS, i.e., obtaining a binary matrix A ∈{0, 1}le×lf .\nArgmax. A simple baseline is to align i and\nj when s(e)\ni\nis the most similar word to s(f)\nj\nand\n1https://github.com/cisnlp/simalign\n2https://simalign.cis.lmu.de/\nAlgorithm 1 Itermax.\n1: procedure ITERMAX(S, nmax, α ∈[0, 1])\n2:\nA, M = zeros like(S)\n3:\nfor n ∈[1, . . . , nmax] do\n4:\n∀i, j :\n5:\nMij =\n\n\n\n\n\n\n\n1 if max\n\u0010Ple\nl=0 Alj, Plf\nl=0 Ail\n\u0011\n= 0\n0 if min\n\u0010Ple\nl=0 Alj, Plf\nl=0 Ail\n\u0011\n> 0\nα otherwise\n6:\nAto add = get argmax alignments(S ⊙M)\n7:\nA = A + Ato add\n8:\nend for\n9:\nreturn A\n10: end procedure\nFigure 2: Description of the Itermax algorithm.\nze-\nros like yields a matrix with zeros and with same shape\nas the input, get argmax alignments returns alignments\nobtained using the Argmax Method, ⊙is elementwise\nmultiplication.\nvice-versa. That is, we set Aij = 1 if\n(i = arg max\nl\nSl,j) ∧(j = arg max\nl\nSi,l)\nand Aij = 0 otherwise. In case of ties, which\nare unlikely in similarity matrices, we choose the\nsmaller index. If all entries in a row i or column\nj of S are 0 we set Aij = 0 (this case can appear\nin Itermax). Similar methods have been applied\nto co-occurrences (Melamed, 2000) (“competitive\nlinking”), Dice coefﬁcients (Och and Ney, 2003)\nand attention matrices (Garg et al., 2019).\nItermax. There are many sentences for which\nArgmax only identiﬁes few alignment edges be-\ncause mutual argmaxes can be rare. As a remedy,\nwe apply Argmax iteratively. Speciﬁcally, we mod-\nify the similarity matrix conditioned on the align-\nment edges found in a previous iteration: if two\nwords i and j have both been aligned, we zero out\nthe similarity. Similarly, if neither is aligned we\nleave the similarity unchanged. In case only one of\nthem is aligned, we multiply the similarity with a\ndiscount factor α ∈[0, 1]. Intuitively, this encour-\nages the model to focus on unaligned word pairs.\nHowever, if the similarity with an already aligned\nword is exceptionally high, the model can add an\nadditional edge. Note that this explicitly allows\none token to be aligned to multiple other tokens.\nFor details on the algorithm see Figure 2.\nMatch. Argmax ﬁnds a local, not a global opti-\nmum and Itermax is a greedy algorithm. To ﬁnd\nglobal optima, we frame alignment as an assign-\n41",
      "page": 41,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1629\nment problem: we search for a maximum-weight\nmaximal matching (e.g., (Kuhn, 1955)) in the bi-\npartite weighted graph which is induced by the\nsimilarity matrix. This optimization problem is\ndeﬁned by\nA∗= argmaxA∈{0,1}le×lf\nle\nX\ni=1\nlf\nX\nj=1\nAijSij\nsubject to A being a matching (i.e., each node has at\nmost one edge) that is maximal (i.e., no additional\nedge can be added). There are known algorithms to\nsolve the above problem in polynomial time (e.g.,\n(Galil, 1986)).\nNote that alignments generated with the match\nmethod are inherently bidirectional. None of our\nmethods require additional symmetrization as post-\nprocessing.\n2.2\nDistortion and Null Extensions\nDistortion Correction [Dist]. Distortion, as intro-\nduced in IBM Model 2, is essential for alignments\nbased on non-contextualized embeddings since the\nsimilarity of two words is solely based on their\nsurface form, independent of position. To penalize\nhigh distortions, we multiply the similarity matrix\nS componentwise with\nPi,j = 1 −κ (i/le −j/lf)2 ,\nwhere κ is a hyperparameter to scale the dis-\ntortion matrix P between [(1 −κ), 1]. We use\nκ = 0.5. See supplementary for different val-\nues. We can interpret this as imposing a locality-\npreserving prior: given a choice, a word should\nbe aligned to a word with a similar relative posi-\ntion ((i/le −j/lf)2 close to 0) rather than a more\ndistant word (large (i/le −j/lf)2).\nNull. Null words model untranslated words and\nare an important part of alignment models. We\npropose to model null words as follows: if a word\nis not particularly similar to any of the words in\nthe target sentence, we do not align it. Speciﬁ-\ncally, given an alignment matrix A, we remove\nalignment edges when the normalized entropy of\nthe similarity distribution is above a threshold τ, a\nhyperparameter. We use normalized entropy (i.e.,\nentropy divided by the log of sentence length) to\naccount for different sentence lengths; i.e., we set\nAij = 0 if\nmin(−\nPlf\nk=1Sh\niklog Sh\nik\nlog lf\n,−\nPle\nk=1Sv\nkjlog Sv\nkj\nlog le\n)>τ,\nwhere Sh\nik\n:=\nSik/ Plf\nm=1 Sim, and Sv\nkj\n:=\nSkj/ Ple\nm=1 Smj. As the ideal value of τ depends\non the actual similarity scores we set τ to a per-\ncentile of the entropy values of the similarity dis-\ntribution across all aligned edges (we use the 95th\npercentile). Different percentiles are in the supple-\nmentary.\n3\nExperiments\n3.1\nEmbedding Learning\nStatic. We train monolingual embeddings with\nfastText (Bojanowski et al., 2017) for each lan-\nguage on its Wikipedia. We then use VecMap\n(Artetxe et al., 2018) to map the embeddings into\na common multilingual space. Note that this algo-\nrithm works without any crosslingual supervision\n(e.g., multilingual dictionaries). We use the same\nprocedure for word and subword levels. We use the\nlabel fastText to refer to these embeddings as well\nas the alignments induced by them.\nContextualized. We use the multilingual BERT\nmodel (mBERT).3 It is pretrained on the 104 largest\nWikipedia languages. This model only provides\nembeddings at the subword level. To obtain a word\nembedding, we simply average the vectors of its\nsubwords. We consider word representations from\nall 12 layers as well as the concatenation of all\nlayers. Note that the model is not ﬁnetuned. We\ndenote this method as mBERT[i] (when using em-\nbeddings from the i-th layer, where 0 means using\nthe non-contextualized initial embedding layer) and\nmBERT[conc] (for concatenation).\nIn addition, we use XLM-RoBERTa base (Con-\nneau et al., 2020), which is pretrained on 100 lan-\nguages on cleaned CommonCrawl data (Wenzek\net al., 2020). We denote alignments obtained using\nthe embeddings from the i-th layer by XLM-R[i].\n3.2\nWord and Subword Alignments\nWe investigate both alignments between subwords\nsuch as wordpiece (Schuster and Nakajima, 2012)\n(which are widely used for contextualized language\nmodels) and words. We refer to computing align-\nment edges between words as word level and be-\ntween subwords as subword level. Note that gold\nstandards are all word-level. In order to evaluate\nalignments obtained at the subword level we con-\nvert subword to word alignments using the heuristic\n“two words are aligned if any of their subwords are\n3https://github.com/google-research/\nbert/blob/master/multilingual.md\n42",
      "page": 42,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1630\nSubword-emb.\nWord-emb.\nEmbeddings\nAlignments\nGold Standard\nConvert by averaging, \nif required\nSubword-level\nWord-level\nConvert using a \nheuristic\nWord-level\nevaluate\nSki excursions are excellent .\nSki ##ausflüge sind hervor ##ragend .\nSki excursions are excellent .\nSkiausflüge sind hervorragend .\nFigure 3: Subword alignments are always converted to\nword alignments for evaluation.\naligned” (see Figure 3). As a result a single word\ncan be aligned with multiple other words.\nFor the word level, we use the NLTK tokenizer\n(Bird et al., 2009) (e.g., for tokenizing Wikipedia\nin order to train fastText). For the subword level,\nwe generally use multilingual BERT’s vocabulary3\nand BERT’s wordpiece tokenizer. For XLM-R we\nuse the XLM-R subword vocabulary. Since gold\nstandards are already tokenized, they do not require\nadditional tokenization.\n3.3\nBaselines\nWe compare to three popular statistical alignment\nmodels that all require parallel training data. fast-\nalign/IBM2 (Dyer et al., 2013) is an implemen-\ntation of an alignment algorithm based on IBM\nModel 2. It is popular because of its speed and high\nquality. eﬂomal4 (based on efmaral by ¨Ostling\nand Tiedemann (2016)), a Bayesian model with\nMarkov Chain Monte Carlo inference, is claimed\nto outperform fast-align on speed and quality. Fur-\nther we use the widely used software package\nGiza++/IBM4 (Och and Ney, 2003), which imple-\nments IBM alignment models. We use its standard\nsettings: 5 iterations each for the HMM model,\nIBM Models 1, 3 and 4 with p0 = 0.98.\nSymmetrization. Probabilistic word alignment\nmodels create forward and backward alignments\nand then symmetrize them (Och and Ney, 2003;\nKoehn et al., 2005). We compared the symmetriza-\ntion methods grow-diag-ﬁnal-and (GDFA) and in-\ntersection and found them to perform comparably;\nsee supplementary. We use GDFA throughout the\npaper.\n4github.com/robertostling/eflomal\n3.4\nEvaluation Measures\nGiven a set of predicted alignment edges A and\na set of sure, possible gold standard edges S, P\n(where S ⊂P), we use the following evaluation\nmeasures:\nprec = |A ∩P|\n|A|\n, rec = |A ∩S|\n|S|\n,\nF1 = 2 prec rec\nprec + rec,\nAER = 1 −|A ∩S| + |A ∩P|\n|A| + |S|\n,\nwhere | · | denotes the cardinality of a set. This is\nthe standard evaluation (Och and Ney, 2003).\n3.5\nData\nOur test data are a diverse set of 6 language pairs:\nCzech, German, Persian, French, Hindi and Roma-\nnian, always paired with English. See Table 11 for\ncorpora and supplementary for URLs.\nFor our baselines requiring parallel training data\n(i.e., eﬂomal, fast-align and Giza++) we select addi-\ntional parallel training data that is consistent with\nthe target domain where available. See Table 11\nfor the corpora. Unless indicated otherwise we use\nthe whole parallel training data. Figure 5 shows the\neffect of using more or less training data.\nGiven the large amount of possible experiments\nwhen considering 6 language pairs we do not have\nspace to present all numbers for all languages. If\nwe show results for only one pair, we choose ENG-\nDEU as it is an established and well-known dataset\n(EuroParl). If we show results for more languages\nwe fall back to DEU, CES and HIN, to show effects\non a mid-resource morphologically rich language\n(CES) and a low-resource language written in a\ndifferent script (HIN).\n4\nResults\n4.1\nEmbedding Layer\nFigure 4 shows a parabolic trend across layers of\nmBERT and XLM-R. We use layer 8 in this paper\nbecause it has best performance. This is consis-\ntent with other work (Hewitt and Manning, 2019;\nTenney et al., 2019): in the ﬁrst layers the contex-\ntualization is too weak for high-quality alignments\nwhile the last layers are too specialized on the pre-\ntraining task (masked language modeling).\n43",
      "page": 43,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1631\nGold\nGold St.\nParallel\nParallel\nWikipedia\nLang.\nStandard\nSize\n|S|\n|P \\ S|\nData\nData Size\nSize\nENG-CES\n(Mareˇcek, 2008)\n2500\n44292\n23132\nEuroParl (Koehn, 2005)\n646k\n8M\nENG-DEU\nEuroParl-baseda\n508\n9612\n921\nEuroParl (Koehn, 2005)\n1920k\n48M\nENG-FAS\n(Tavakoli and Faili, 2014)\n400\n11606\n0\nTEP (Pilevar et al., 2011)\n600k\n5M\nENG-FRA\nWPT2003, (Och and Ney, 2000),\n447\n4038\n13400\nHansards (Germann, 2001)\n1130k\n32M\nENG-HIN\nWPT2005b\n90\n1409\n0\nEmille (McEnery et al., 2000)\n3k\n1M\nENG-RON\nWPT2005b\n203\n5033\n0\nConstitution, Newspaperb\n50k\n3M\na www-i6.informatik.rwth-aachen.de/goldAlignment/\nb http://web.eecs.umich.edu/˜mihalcea/wpt05/\nTable 1: Overview of datasets. “Lang.” uses ISO 639-3 language codes. “Size” refers to the number of sentences.\n“Parallel Data Size” refers to the number of parallel sentences in addition to the gold alignments that is used for\ntraining the baselines. Our sentence tokenized version of the English Wikipedia has 105M sentences.\nENG-CES\nENG-DEU\nENG-FAS\nENG-FRA\nENG-HIN\nENG-RON\nMethod\nF1\nAER\nF1\nAER\nF1\nAER\nF1\nAER\nF1\nAER\nF1\nAER\nPrior Work\n( ¨Ostling, 2015a) Bayesian\n.94\n.06\n.57\n.43\n.73\n.27\n( ¨Ostling, 2015a) Giza++\n.92\n.07\n.51\n.49\n.72\n.28\n(Legrand et al., 2016) Ensemble Method\n.81\n.16\n.71\n.10\n( ¨Ostling and Tiedemann, 2016) efmaral\n.93\n.08\n.53\n.47\n.72\n.28\n( ¨Ostling and Tiedemann, 2016) fast-align\n.86\n.15\n.33\n.67\n.68\n.33\n(Zenkel et al., 2019) Giza++\n.21\n.06\n.28\n(Garg et al., 2019) Multitask\n.20\n.08\nBaselines\nWord\nfast-align/IBM2\n.76\n.25\n.71\n.29\n.57\n.43\n.86\n.15\n.34\n.66\n.68\n.33\nGiza++/IBM4\n.75\n.26\n.77\n.23\n.51\n.49\n.92\n.09\n.45\n.55\n.69\n.31\neﬂomal\n.85\n.15\n.77\n.23\n.61\n.39\n.93\n.08\n.51\n.49\n.71\n.29\nSubword\nfast-align/IBM2\n.78\n.23\n.71\n.30\n.58\n.42\n.85\n.16\n.38\n.62\n.68\n.32\nGiza++/IBM4\n.82\n.18\n.78\n.22\n.57\n.43\n.92\n.09\n.48\n.52\n.69\n.32\neﬂomal\n.84\n.17\n.76\n.24\n.63\n.37\n.91\n.09\n.52\n.48\n.72\n.28\nThis Work\nWord\nfastText - Argmax\n.70\n.30\n.60\n.40\n.50\n.50\n.77\n.22\n.49\n.52\n.47\n.53\nmBERT[8] - Argmax\n.87\n.13\n.79\n.21\n.67\n.33\n.94\n.06\n.54\n.47\n.64\n.36\nXLM-R[8] - Argmax\n.87\n.13\n.79\n.21\n.70\n.30\n.93\n.06\n.59\n.41\n.70\n.30\nSubword\nfastText - Argmax\n.58\n.42\n.56\n.44\n.09\n.91\n.73\n.26\n.04\n.96\n.43\n.58\nmBERT[8] - Argmax\n.86\n.14\n.81\n.19\n.67\n.33\n.94\n.06\n.55\n.45\n.65\n.35\nXLM-R[8] - Argmax\n.87\n.13\n.81\n.19\n.71\n.29\n.93\n.07\n.61\n.39\n.71\n.29\nTable 2: Comparison of our methods, baselines and prior work in unsupervised word alignment. Best result per\ncolumn in bold. A detailed version of the table with precision/recall and Itermax/Match results is in supplementary.\n0\n2\n4\n6\n8\n10\n12\nLayer\n0.4\n0.6\n0.8\nF1\nXLM\nR\neng_deu\neng_ces\neng_hin\nlayers\nconc\nFigure 4: Word alignment performance across layers\nof mBERT (top) and XLM-R (bottom). Results are F1\nwith Argmax at the subword level.\n4.2\nComparison with Prior Work\nContextual Embeddings.\nTable 2 shows that\nmBERT and XLM-R consistently perform well\nwith the Argmax method. XLM-R yields mostly\nhigher values than mBERT. Our three baselines,\neﬂomal, fast-align and Giza++, are always outper-\nformed (except for RON). We outperform all prior\nwork except for FRA where we match the perfor-\nmance and RON. This comparison is not entirely\nfair because methods relying on parallel data have\naccess to the parallel sentences of the test data dur-\ning training whereas our methods do not.\nRomanian might be a special case as it exhibits a\nlarge amount of many to one links and further lacks\ndeterminers. How determiners are handled in the\ngold standard depends heavily on the annotation\nguidelines. Note that one of our settings, XLM-\nR[8] with Itermax at the subword level, has an F1\nof .72 for ENG-RON, which comes very close to\nthe performance by ( ¨Ostling, 2015a) (see Table 3).\nIn summary, extracting alignments from similar-\nity matrices is a very simple and efﬁcient method\nthat performs surprisingly strongly. It outperforms\nstrong statistical baselines and most prior work in\nunsupervised word alignment for CES, DEU, FAS\nand HIN and is comparable for FRA and RON.\nWe attribute this to the strong contextualization in\nmBERT and XLM-R.\n44",
      "page": 44,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1632\n103\n104\n105\n106\n107\n#Parallel Sentences\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nF1\nmBERT[8](Argmax)\nfastText(Argmax+Dist)\nfast-align\neflomal\nword\nsubword\nFigure 5:\nLearning curves of fast-align/eﬂomal vs.\nembedding-based alignments. Results shown are F1\nfor ENG-DEU, contrasting subword and word repre-\nsentations. Up to 1.9M parallel sentences we use Eu-\nroParl. To demonstrate the effect with abundant paral-\nlel data we add up to 37M additional parallel sentences\nfrom ParaCrawl (Espl`a et al., 2019) (see grey area).\nStatic Embeddings. fastText shows a solid per-\nformance on word level, which is worse but comes\nclose to fast-align and outperforms it for HIN. We\nconsider this surprising as fastText did not have\naccess to parallel data or any multilingual signal.\nVecMap can also be used with crosslingual dictio-\nnaries. We expect this to boost performance and\nfastText could then become a viable alternative to\nfast-align.\nAmount of Parallel Data. Figure 5 shows that\nfast-align and eﬂomal get better with more train-\ning data with eﬂomal outperforming fast-align, as\nexpected. However, even with 1.9M parallel sen-\ntences mBERT outperforms both baselines. When\nadding up to 37M additional parallel sentences\nfrom ParaCrawl (Espl`a et al., 2019) performance\nfor fast-align increases slightly, however, eﬂomal\ndecreases (grey area in plot). ParaCrawl contains\nmined parallel sentences whose lower quality prob-\nably harms eﬂomal. fastText (with distortion) is\ncompetitive with eﬂomal for fewer than 1000 paral-\nlel sentences and outperforms fast-align even with\n10k sentences. Thus for very small parallel corpora\n(<10k sentences) using fastText embeddings is an\nalternative to fast-align.\nThe main takeaway from Figure 5 is that mBERT-\nbased alignments, a method that does not need any\nparallel training data, outperforms state-of-the-art\naligners like eﬂomal for ENG-DEU, even in the\nvery high resource case.\nENG-\nENG-\nENG-\nENG-\nENG-\nENG-\nEmb.\nMethod\nCES\nDEU\nFAS\nFRA\nHIN\nRON\nmBERT[8]\nArgmax\n.86\n.81\n.67\n.94\n.55\n.65\nItermax\n.86\n.81\n.70\n.93\n.58\n.69\nMatch\n.82\n.78\n.67\n.90\n.58\n.67\nXLM-R[8]\nArgmax\n.87\n.81\n.71\n.93\n.61\n.71\nItermax\n.86\n.80\n.72\n.92\n.62\n.72\nMatch\n.81\n.76\n.68\n.88\n.60\n.70\nTable 3: Comparison of our three proposed methods\nacross all languages for the best embeddings from Ta-\nble 2: mBERT[8] and XLM-R[8]. We show F1 at the\nsubword level. Best result per embedding type in bold.\nENG-DEU\nENG-CES\nENG-HIN\nEmb.\nnmax α\nPrec. Rec. F1 AER Prec. Rec. F1 AER Prec. Rec. F1 AER\nmBERT[8]\n1\n-\n.92\n.69 .79 .21\n.95\n.80 .87 .13\n.84\n.39 .54 .47\n2\n.90\n.85\n.77 .81 .19\n.87\n.87 .87 .14\n.75\n.47 .58 .42\n.95\n.83\n.80 .81 .19\n.85\n.89 .87 .13\n.73\n.48 .58 .42\n1\n.77\n.79 .78 .22\n.80\n.86 .83 .17\n.63\n.46 .53 .47\n3\n.90\n.81\n.80 .80 .20\n.83\n.88 .85 .15\n.70\n.49 .57 .43\n.95\n.78\n.83 .81 .20\n.81\n.91 .86 .15\n.68\n.52 .59 .41\n1\n.73\n.83 .77 .23\n.76\n.91 .82 .18\n.58\n.51 .54 .46\nfastText\n1\n-\n.81\n.48 .60 .40\n.86\n.59 .70 .30\n.75\n.36 .49 .52\n2\n.90\n.69\n.56 .62 .38\n.74\n.69 .72 .29\n.63\n.42 .51 .49\n.95\n.66\n.56 .61 .39\n.71\n.69 .70 .30\n.59\n.41 .48 .52\n1\n.59\n.55 .57 .43\n.62\n.65 .63 .37\n.53\n.39 .45 .55\n3\n.90\n.63\n.59 .61 .39\n.67\n.72 .70 .31\n.57\n.43 .49 .51\n.95\n.59\n.59 .59 .41\n.63\n.73 .68 .33\n.53\n.44 .48 .52\n1\n.53\n.58 .55 .45\n.55\n.70 .62 .39\n.48\n.43 .45 .55\nTable 4: Itermax with different number of iterations\n(nmax) and different α. Results are at the word level.\n4.3\nAdditional Methods and Extensions\nWe already showed that Argmax yields alignments\nthat are competitive with the state of the art. In this\nsection we compare all our proposed methods and\nextensions more closely.\nItermax. Table 4 shows results for Argmax\n(i.e., 1 Iteration) as well as Itermax (i.e., 2 or\nmore iterations of Argmax). As expected, with\nmore iterations precision drops in favor of recall.\nOverall, Itermax achieves higher F1 scores for the\nthree language pairs (equal for ENG-CES) both for\nmBERT[8] and fastText embeddings. For Hindi the\nperformance increase is the highest. We hypothe-\nsize that for more distant languages Itermax is more\nbeneﬁcial as similarity between wordpieces may\nbe generally lower, thus exhibiting fewer mutual\nargmaxes. For the rest of the paper if we use Iter-\nmax we use 2 Iterations with α = 0.9 as it exhibits\nbest performance (5 out of 6 wins in Table 4).\nArgmax/Itermax/Match. In Table 3 we com-\npare our three proposed methods in terms of F1\nacross all languages. We chose to show the two\n45",
      "page": 45,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1633\nENG-DEU\nENG-CES\nENG-HIN\nEmb.\nMethod\nPrec. Rec. F1 AER Prec. Rec. F1 AER Prec. Rec. F1 AER\nfastText\nArgmax\n.81\n.48 .60 .40\n.86\n.59 .70 .30\n.75\n.36 .49 .52\n+Dist\n.84\n.54 .65 .35\n.89\n.68 .77 .23\n.64\n.30 .41 .59\n+Null\n.81\n.46 .59 .41\n.86\n.56 .68 .32\n.74\n.34 .46 .54\nItermax\n.69\n.56 .62 .38\n.74\n.69 .72 .29\n.63\n.42 .51 .49\n+Dist\n.71\n.62 .66 .34\n.75\n.76 .76 .25\n.54\n.37 .44 .57\n+Null\n.69\n.53 .60 .40\n.74\n.66 .70 .30\n.63\n.40 .49 .51\nMatch\n.60\n.58 .59 .41\n.65\n.71 .68 .32\n.55\n.43 .48 .52\n+Dist\n.67\n.64 .65 .35\n.72\n.78 .75 .25\n.50\n.39 .43 .57\n+Null\n.61\n.56 .58 .42\n.66\n.69 .67 .33\n.56\n.41 .48 .52\nmBERT[8]\nArgmax\n.92\n.69 .79 .21\n.95\n.80 .87 .13\n.84\n.39 .54 .47\n+Dist\n.91\n.67 .77 .23\n.93\n.79 .85 .15\n.68\n.29 .41 .59\n+Null\n.93\n.67 .78 .22\n.95\n.77 .85 .15\n.85\n.38 .53 .47\nItermax\n.85\n.77 .81 .19\n.87\n.87 .87 .14\n.75\n.47 .58 .43\n+Dist\n.82\n.75 .79 .21\n.84\n.85 .85 .15\n.56\n.34 .43 .58\n+Null\n.86\n.75 .80 .20\n.88\n.84 .86 .14\n.76\n.45 .57 .43\nMatch\n.78\n.74 .76 .24\n.81\n.85 .83 .17\n.67\n.52 .59 .42\n+Dist\n.75\n.71 .73 .27\n.79\n.83 .81 .20\n.45\n.35 .39 .61\n+Null\n.80\n.73 .76 .24\n.83\n.83 .83 .17\n.68\n.51 .58 .42\nTable 5: Analysis of Null and Distortion Extensions.\nAll alignments are obtained at word-level. Best result\nper embedding type and method in bold.\nbest performing settings from Table 2: mBERT[8]\nand XLM-R[8] at the subword level. Itermax per-\nforms slightly better than Argmax with 6 wins, 4\nlosses and 2 ties. Itermax seems to help more for\nmore distant languages such as FAS, HIN and RON,\nbut harms for FRA. Match has the lowest F1, but\ngenerally exhibits a higher recall (see e.g., Table 5).\nNull and Distortion Extensions. Table 5 shows\nthat Argmax and Itermax generally have higher pre-\ncision, whereas Match has higher recall. Adding\nNull almost always increases precision, but at the\ncost of recall, resulting mostly in a lower F1 score.\nAdding a distortion prior boosts performance for\nstatic embeddings, e.g., from .70 to .77 for ENG-\nCES Argmax F1 and similarly for ENG-DEU. For\nHindi a distortion prior is harmful. Dist has little\nand sometimes harmful effects on mBERT indicat-\ning that mBERT’s contextualized representations\nalready match well across languages.\nSummary. Argmax and Itermax exhibit the best\nand most stable performance. For most language\npairs Itermax is recommended. If high recall align-\nments are required, Match is the recommended\nalgorithm. Except for HIN, a distortion prior is\nbeneﬁcial for static embeddings. Null should be ap-\nplied when one wants to push precision even higher\n(e.g., for annotation projection).\n4.4\nWords and Subwords\nTable 2 shows that subword processing slightly out-\nperforms word-level processing for most methods.\nOnly fastText is harmed by subword processing.\n0 <= x < 5\n(240)\n5 <= x < 25\n(331)\n25 <= x < 125\n(650)\n125 <= x\n(9312)\nFrequency Bin\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nF1\nmBERT[8](Argmax)\neflomal\nword\nsubword\nFigure 6: Results for different frequency bins on ENG-\nDEU. An edge in S, P, or A is attributed to exactly one\nbin based on the minimum frequency of the involved\nwords (denoted by x). Number of gold edges in brack-\nets. Eﬂomal is trained on all 1.9M parallel sentences.\nFrequencies are computed on the same corpus.\nADJ ADP ADV AUX NOUN PRON VERB\neﬂomal\nWord\n0.83\n0.69\n0.72\n0.63\n0.85\n0.79\n0.63\nSubword\n0.82\n0.68\n0.71\n0.57\n0.85\n0.77\n0.62\nmBERT[8] Word\n0.79\n0.74\n0.71\n0.71\n0.81\n0.84\n0.69\nSubword\n0.81\n0.75\n0.72\n0.72\n0.87\n0.84\n0.69\nTable 6: Alignment performance (F1) on ENG-DEU\nfor POS. We use mBERT[8](Argmax) and Eﬂomal\ntrained on 1.9M parallel sentences on the word level.\nWe use VecMap to match (sub)word distributions\nacross languages. We hypothesize that it is harder\nto match subword than word distributions – this\neffect is strongest for Persian and Hindi, proba-\nbly due to different scripts and thus different sub-\nword distributions. Initial experiments showed that\nadding supervision in form of a dictionary helps\nrestore performance. We will investigate this in\nfuture work.\nWe hypothesize that subword processing is ben-\neﬁcial for aligning rare words. To show this, we\ncompute our evaluation measures for different fre-\nquency bins. More speciﬁcally, we only consider\ngold standard alignment edges for the computation\nwhere at least one of the member words has a cer-\ntain frequency in a reference corpus (in our case all\n1.9M lines from the ENG-DEU EuroParl corpus).\nThat is, we only consider the edge (i, j) in A, S or\nP if the minimum of the source and target word\nfrequency is in [γl, γu) where γl and γu are bin\nboundaries.\nFigure 6 shows F1 for different frequency bins.\nFor rare words both eﬂomal and mBERT show a\nseverely decreased performance at the word level,\nbut not at the subword level. Thus, subword pro-\ncessing is indeed beneﬁcial for rare words.\n46",
      "page": 46,
      "type": "text",
      "language": "hi"
    },
    {
      "level": "H2",
      "text": "1634\nAt the same time , Regulation No 2078 of 1992 on \nenvironmentally compatible agricultural production methods \nadapted to the landscape has also contributed substantially to \nthis trend . \nDaneben hat die Verordnung 2078 aus dem Jahr 1992 über\numweltverträgliche und landschaftsgerechte\nProduktionsweisen in der Landwirtschaft ebenfalls erheblich\nzu dieser Entwicklung beigetragen .\nThe Commission , for its part , will continue to play an active \npart in the intergovernmental conference .\nDie Kommission wird bei der Regierungskonferenz auch\nweiterhin eine aktive Rolle spielen .\nFigure 7: Example alignment of auxiliary verbs. Same\nsetting as in Table 6. Solid lines: mBERT’s alignment,\nidentical to the gold standard. Dashed lines: eﬂomal’s\nincorrect alignment.\n4.5\nPart-Of-Speech Analysis\nTo analyze the performance with respect to differ-\nent part-of-speech (POS) tags, the ENG-DEU gold\nstandard was tagged with the Stanza toolkit (Qi\net al., 2020). We evaluate the alignment perfor-\nmance for each POS tag by only considering the\nalignment edges where at least one of their mem-\nber words has this tag. Table 6 shows results for\nfrequent POS tags. Compared to eﬂomal, mBERT\naligns auxiliaries, pronouns and verbs better. The\nrelative position of auxiliaries and verbs in German\ncan diverge strongly from that in English because\nthey occur at the end of the sentence (verb-end po-\nsition) in many clause types. Positions of pronouns\ncan also diverge due to a more ﬂexible word or-\nder in German. It is difﬁcult for an HMM-based\naligner like eﬂomal to model such high-distortion\nalignments, a property that has been found by prior\nwork as well (Ho and Yvon, 2019). In contrast,\nmBERT(Argmax) does not use distortion informa-\ntion, so high distortion is not a problem for it.\nFigure 7 gives an example for auxiliaries. The\ngold alignment (“has” – “hat”) is correctly identi-\nﬁed by mBERT (solid line). Eﬂomal generates an\nincorrect alignment (“time” – “hat”): the two words\nhave about the same relative position, indicating\nthat distortion minimization is the main reason for\nthis incorrect alignment. Analyzing all auxiliary\nalignment edges, the average absolute value of the\ndistance between aligned words is 2.72 for eﬂomal\nand 3.22 for mBERT. This indicates that eﬂomal\nis more reluctant than mBERT to generate high-\ndistortion alignments and thus loses accuracy.\n5\nRelated Work\nBrown et al. (1993) introduced the IBM models, the\nbest known statistical word aligners. More recent\naligners, often based on IBM models, include fast-\nalign (Dyer et al., 2013), Giza++ (Och and Ney,\n2003) and eﬂomal ( ¨Ostling and Tiedemann, 2016).\n( ¨Ostling, 2015a) showed that Bayesian Alignment\nModels perform well. Neural network based exten-\nsions of these models have been considered (Ayan\net al., 2005; Ho and Yvon, 2019). All of these mod-\nels are trained on parallel text. Our method instead\naligns based on embeddings that are induced from\nmonolingual data only. We compare with prior\nmethods and observe comparable performance.\nPrior work on using learned representations for\nalignment includes (Smadja et al., 1996; Och and\nNey, 2003) (Dice coefﬁcient), (Jalili Sabet et al.,\n2016) (incorporation of embeddings into IBM mod-\nels), (Legrand et al., 2016) (neural network align-\nment model) and (Pourdamghani et al., 2018) (em-\nbeddings are used to encourage words to align to\nsimilar words). Tamura et al. (2014) use recur-\nrent neural networks to learn alignments. They use\nnoise contrastive estimation to avoid supervision.\nYang et al. (2013) train a neural network that uses\npretrained word embeddings in the initial layer. All\nof this work requires parallel data. mBERT is used\nfor word alignments in concurrent work: Libovick´y\net al. (2019) use the high quality of mBERT align-\nments as evidence for the “language-neutrality” of\nmBERT. Nagata et al. (2020) phrase word align-\nment as crosslingual span prediction and ﬁnetune\nmBERT using gold alignments.\nAttention in NMT (Bahdanau et al., 2014) is\nrelated to a notion of soft alignment, but often de-\nviates from conventional word alignments (Ghader\nand Monz, 2017; Koehn and Knowles, 2017). One\ndifference is that standard attention does not have\naccess to the target word. To address this, Pe-\nter et al. (2017) tailor attention matrices to obtain\nhigher quality alignments. Li et al. (2018)’s and\nZenkel et al. (2019)’s models perform similarly\nto and Zenkel et al. (2020) outperform Giza++.\nDing et al. (2019) propose better decoding algo-\nrithms to deduce word alignments from NMT pre-\ndictions. Chen et al. (2016), Mi et al. (2016) and\nGarg et al. (2019) obtain alignments and transla-\ntions in a multitask setup. Garg et al. (2019) ﬁnd\nthat operating at the subword level can be bene-\nﬁcial for alignment models. Li et al. (2019) pro-\npose two methods to extract alignments from NMT\n47",
      "page": 47,
      "type": "text",
      "language": "de"
    },
    {
      "level": "H2",
      "text": "1635\nmodels, however they do not outperform fast-align.\nStengel-Eskin et al. (2019) compute similarity ma-\ntrices of encoder-decoder representations that are\nleveraged for word alignments, together with super-\nvised learning, which requires manually annotated\nalignment. We ﬁnd our proposed methods to be\ncompetitive with these approaches. In contrast to\nour work, they all require parallel data.\n6\nConclusion\nWe presented word aligners based on contextual-\nized embeddings that outperform in four and match\nthe performance of state-of-the-art aligners in two\nlanguage pairs; e.g., for ENG-DEU contextualized\nembeddings achieve an alignment F1 that is 5 per-\ncentage points higher than eﬂomal trained on 100k\nparallel sentences. Further, we showed that align-\nments from static embeddings can be a viable al-\nternative to statistical aligner when few parallel\ntraining data is available. In contrast to all prior\nwork our methods do not require parallel data for\ntraining at all. With our proposed methods and\nextensions such as Match, Itermax and Null it is\neasy to obtain higher precision or recall depending\non the use case.\nFuture work includes modeling fertility explic-\nitly and investigating how to incorporate parallel\ndata into the proposed methods.\nAcknowledgments\nWe gratefully acknowledge funding through a Zen-\ntrum Digitalisierung.Bayern fellowship awarded to\nthe second author. This work was supported by\nthe European Research Council (# 740516). We\nthank Matthias Huck, Jindˇrich Libovick´y, Alex\nFraser and the anonymous reviewers for interest-\ning discussions and valuable comments. Thanks\nto Jindˇrich for pointing out that mBERT can align\nmixed-language sentences as shown in Figure 1.\nReferences\nTamer Alkhouli, Gabriel Bretschner, and Hermann Ney.\n2018.\nOn the alignment problem in multi-head\nattention-based neural machine translation. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Research Papers, Belgium, Brussels. Associ-\nation for Computational Linguistics.\nTamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Pe-\nter, Mohammed Hethnawi, Andreas Guta, and Her-\nmann Ney. 2016. Alignment-based neural machine\ntranslation. In Proceedings of the First Conference\non Machine Translation: Volume 1, Research Pa-\npers, Berlin, Germany. Association for Computa-\ntional Linguistics.\nTamer Alkhouli and Hermann Ney. 2017.\nBiasing\nattention-based recurrent neural networks using ex-\nternal alignment information. In Proceedings of the\nSecond Conference on Machine Translation, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A Smith.\n2016.\nMassively multilingual word embeddings.\narXiv preprint arXiv:1602.01925.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings.\nIn\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), Melbourne, Australia. Association for\nComputational Linguistics.\nEhsaneddin Asgari and Hinrich Sch¨utze. 2017. Past,\npresent, future: A computational investigation of the\ntypology of tense in 1000 languages. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nNecip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.\n2005. NeurAlign: Combining word alignments us-\ning neural networks. In Proceedings of Human Lan-\nguage Technology Conference and Conference on\nEmpirical Methods in Natural Language Processing,\nVancouver, British Columbia, Canada. Association\nfor Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe International Conference on Learning Represen-\ntations.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.\nNatural language processing with Python: analyz-\ning text with the natural language toolkit. O’Reilly\nMedia, Inc.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2).\nWenhu Chen, Evgeny Matusov, Shahram Khadivi,\nand Jan-Thorsten Peter. 2016.\nGuided alignment\ntraining for topic-aware neural machine translation.\nAMTA 2016.\n48",
      "page": 48,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1636\nTrevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-\nmolova, Kaisheng Yao, Chris Dyer, and Gholamreza\nHaffari. 2016. Incorporating structural alignment bi-\nases into an attentional neural translation model. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 876–885, San Diego, California. Association\nfor Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, Online. Asso-\nciation for Computational Linguistics.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2018.\nWord translation without parallel data. In Proceed-\nings of the Sixth International Conference on Learn-\ning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nShuoyang Ding, Hainan Xu, and Philipp Koehn. 2019.\nSaliency-driven word alignment interpretation for\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n1: Research Papers), Florence, Italy. Association for\nComputational Linguistics.\nPhilipp Dufter, Mengjie Zhao, Martin Schmitt, Alexan-\nder Fraser, and Hinrich Sch¨utze. 2018. Embedding\nlearning through multilingual concept induction. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), Melbourne, Australia. Association for\nComputational Linguistics.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of IBM model 2. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Atlanta, Georgia. Associa-\ntion for Computational Linguistics.\nMiquel Espl`a, Mikel Forcada, Gema Ram´ırez-S´anchez,\nand Hieu Hoang. 2019. ParaCrawl: Web-scale paral-\nlel corpora for the languages of the EU. In Proceed-\nings of Machine Translation Summit XVII Volume 2:\nTranslator, Project and User Tracks, Dublin, Ireland.\nEuropean Association for Machine Translation.\nZvi Galil. 1986. Efﬁcient algorithms for ﬁnding maxi-\nmum matching in graphs. ACM Computing Surveys\n(CSUR), 18(1).\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), Hong Kong, China. As-\nsociation for Computational Linguistics.\nUlrich Germann. 2001. Aligned Hansards of the 36th\nparliament of Canada.\nHamidreza Ghader and Christof Monz. 2017.\nWhat\ndoes attention in neural machine translation pay at-\ntention to?\nIn Proceedings of the Eighth Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), Taipei, Taiwan.\nAsian Federation of Natural Language Processing.\nJiang Guo, Wanxiang Che, David Yarowsky, Haifeng\nWang, and Ting Liu. 2016. A representation learn-\ning framework for multi-source transfer parsing. In\nThirtieth AAAI Conference on Artiﬁcial Intelligence.\nJohn Hewitt and Christopher D. Manning. 2019.\nA\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAnh Khoa Ngo Ho and Franc¸ois Yvon. 2019. Neural\nbaselines for word alignment. In Proceedings of the\n16th International Workshop on Spoken Language\nTranslation.\nMatthias Huck, Diana Dutka, and Alexander Fraser.\n2019.\nCross-lingual annotation projection is ef-\nfective for neural part-of-speech tagging.\nIn Pro-\nceedings of the Sixth Workshop on NLP for Simi-\nlar Languages, Varieties and Dialects, pages 223–\n233, Ann Arbor, Michigan. Association for Compu-\ntational Linguistics.\nMasoud Jalili Sabet, Heshaam Faili, and Gholamreza\nHaffari. 2016.\nImproving word alignment of rare\nwords with word embeddings.\nIn Proceedings of\nCOLING 2016, the 26th International Conference\non Computational Linguistics:\nTechnical Papers,\nOsaka, Japan. The COLING 2016 Organizing Com-\nmittee.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Machine Transla-\ntion Summit, volume 5.\nPhilipp Koehn, Amittai Axelrod, Alexandra Birch\nMayne, Chris Callison-Burch, Miles Osborne, and\n49",
      "page": 49,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1637\nDavid Talbot. 2005.\nEdinburgh system descrip-\ntion for the 2005 IWSLT speech translation evalu-\nation.\nIn International Workshop on Spoken Lan-\nguage Translation (IWSLT) 2005.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, Vancouver. Association for Computational\nLinguistics.\nHarold W Kuhn. 1955. The Hungarian method for the\nassignment problem. Naval research logistics quar-\nterly, 2(1-2).\nJo¨el Legrand, Michael Auli, and Ronan Collobert.\n2016.\nNeural network-based word alignment\nthrough score aggregation.\nIn Proceedings of the\nFirst Conference on Machine Translation: Volume 1,\nResearch Papers, Berlin, Germany. Association for\nComputational Linguistics.\nWilliam D. Lewis and Fei Xia. 2008. Automatically\nidentifying computationally relevant typological fea-\ntures.\nIn Proceedings of the Third International\nJoint Conference on Natural Language Processing:\nVolume-II.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, Florence, Italy. Association for\nComputational Linguistics.\nXintong Li, Lemao Liu, Zhaopeng Tu, Shuming Shi,\nand Max Meng. 2018.\nTarget foresight based at-\ntention for neural machine translation. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJindˇrich Libovick´y, Rudolf Rosa, and Alexander Fraser.\n2019. How language-neutral is multilingual BERT?\narXiv preprint arXiv:1911.03310.\nLemao Liu, Masao Utiyama, Andrew Finch, and Ei-\nichiro Sumita. 2016.\nNeural machine translation\nwith supervised attention. In Proceedings of COL-\nING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, Os-\naka, Japan. The COLING 2016 Organizing Commit-\ntee.\nDavid Mareˇcek. 2008.\nAutomatic alignment of tec-\ntogrammatical trees from Czech-English parallel\ncorpus.\nMaster’s thesis, Charles University, MFF\nUK.\nAnthony McEnery, Paul Baker, Rob Gaizauskas, and\nHamish Cunningham. 2000. Emille: Building a cor-\npus of South Asian languages. VIVEK-BOMBAY-,\n13(3).\nI. Dan Melamed. 2000. Models of translation equiv-\nalence among words.\nComputational Linguistics,\n26(2).\nHaitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.\nSupervised attentions for neural machine translation.\nIn Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, Austin,\nTexas. Association for Computational Linguistics.\nRada Mihalcea and Ted Pedersen. 2003.\nAn evalua-\ntion exercise for word alignment.\nIn Proceedings\nof the HLT-NAACL 2003 Workshop on Building and\nUsing Parallel Texts: Data Driven Machine Transla-\ntion and Beyond.\nMasaaki Nagata,\nChousa Katsuki,\nand Masaaki\nNishino. 2020.\nA supervised word alignment\nmethod\nbased\non\ncross-language\nspan\npredic-\ntion using multilingual BERT.\narXiv preprint\narXiv:2004.14516.\nFranz Josef Och and Hermann Ney. 2000. Improved\nstatistical alignment models. In Proceedings of the\n38th Annual Meeting of the Association for Com-\nputational Linguistics, Hong Kong. Association for\nComputational Linguistics.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational Linguistics, 29(1).\nRobert ¨Ostling. 2015a. Bayesian models for multilin-\ngual word alignment. Ph.D. thesis, Department of\nLinguistics, Stockholm University.\nRobert ¨Ostling. 2015b. Word order typology through\nmultilingual word alignment. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), Beijing, China. Association\nfor Computational Linguistics.\nRobert ¨Ostling and J¨org Tiedemann. 2016. Efﬁcient\nword alignment with Markov Chain Monte Carlo.\nThe Prague Bulletin of Mathematical Linguistics,\n106(1).\nSebastian Pad´o and Mirella Lapata. 2009.\nCross-\nlingual annotation projection for semantic roles.\nJournal of Artiﬁcial Intelligence Research, 36.\nJan-Thorsten Peter, Arne Nix, and Hermann Ney.\n2017.\nGenerating alignments using target fore-\nsight in attention-based neural machine translation.\nThe Prague Bulletin of Mathematical Linguistics,\n108(1).\nMohammad Taher Pilevar, Heshaam Faili, and Ab-\ndol Hamid Pilevar. 2011.\nTEP: Tehran English-\nPersian parallel corpus. In International Conference\non Intelligent Text Processing and Computational\nLinguistics. Springer.\n50",
      "page": 50,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1638\nNima Pourdamghani,\nMarjan Ghazvininejad,\nand\nKevin Knight. 2018. Using word vectors to improve\nword alignments for low resource machine transla-\ntion.\nIn Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nOﬁr Press and Noah A Smith. 2018. You may not need\nattention. arXiv preprint arXiv:1810.13409.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020.\nStanza:\nA\nPython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, Online. Asso-\nciation for Computational Linguistics.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search.\nIn 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE.\nHolger Schwenk,\nVishrav Chaudhary,\nShuo Sun,\nHongyu Gong, and Francisco Guzm´an. 2019. Wiki-\nmatrix: Mining 135m parallel sentences in 1620\nlanguage pairs from wikipedia.\narXiv preprint\narXiv:1907.05791.\nFrank Smadja, Kathleen R. McKeown, and Vasileios\nHatzivassiloglou. 1996. Translating collocations for\nbilingual lexicons: A statistical approach. Computa-\ntional Linguistics, 22(1).\nElias Stengel-Eskin, Tzu-ray Su, Matt Post, and Ben-\njamin Van Durme. 2019.\nA discriminative neural\nmodel for cross-lingual word alignment. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), Hong Kong, China. As-\nsociation for Computational Linguistics.\nAkihiro Tamura, Taro Watanabe, and Eiichiro Sumita.\n2014.\nRecurrent neural networks for word align-\nment model.\nIn Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), Baltimore, Mary-\nland. Association for Computational Linguistics.\nLeila Tavakoli and Heshaam Faili. 2014. Phrase align-\nments in parallel corpus using bootstrapping ap-\nproach.\nInternational Journal of Information &\nCommunication Technology Research, 6(3).\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline.\nIn\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, Florence,\nItaly. Association for Computational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm´an, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of The 12th Lan-\nguage Resources and Evaluation Conference, Mar-\nseille, France. European Language Resources Asso-\nciation.\nNan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-\nhai Yu. 2013. Word alignment modeling with con-\ntext dependent deep neural network. In Proceedings\nof the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nDavid Yarowsky, Grace Ngai, and Richard Wicen-\ntowski. 2001.\nInducing multilingual text analysis\ntools via robust projection across aligned corpora. In\nProceedings of the First International Conference on\nHuman Language Technology Research.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2019. Adding interpretable attention to neural trans-\nlation models improves word alignment.\narXiv\npreprint arXiv:1901.11359.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2020.\nEnd-to-end neural word alignment outper-\nforms GIZA++. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1605–1617, Online. Association for\nComputational Linguistics.\nA\nAdditional Non-central Results\nA.1\nComparison with Prior Work\nA more detailed version of Table 2 from the main\npaper that includes precision and recall and results\non Itermax can be found in Table 7.\nA.2\nRare Words\nFigure 8 shows the same as Figure 6 from the\nmain paper but now with a reference corpus of\n100k/1000k instead of 1920k parallel sentences.\nThe main takeaways are similar.\nA.3\nSymmetrization\nFor asymmetric alignments different symmetriza-\ntion methods exist. Dyer et al. (2013) provide an\noverview and implementation (fast-align) for these\nmethods, which we use. We compare intersection\nand grow-diag-ﬁnal-and (GDFA) in Table 9. In\nterms of F1, GDFA performs better (Intersection\nwins four times, GDFA eleven times, three ties).\nAs expected, Intersection yields higher precision\nwhile GDFA yields higher recall. Thus intersection\nis preferable for tasks like annotation projection,\n51",
      "page": 51,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1639\nENG-CES\nENG-DEU\nENG-FAS\nENG-FRA\nENG-HIN\nENG-RON\nMethod\nPrec.Rec.F1 AERPrec.Rec.F1 AERPrec.Rec.F1 AERPrec.Rec. F1 AERPrec.Rec.F1 AERPrec.Rec.F1 AER\nPrior Work\n( ¨Ostling, 2015a) Bayesian\n.96 .92 .94 .06\n.85 .43 .57 .43\n.91 .61 .73 .27\n( ¨Ostling, 2015a) Giza++\n.98 .87 .92 .07\n.63 .44 .51 .49\n.85 .63 .72 .28\n(Legrand et al., 2016) Ensemble Method\n.79 .83 .81 .16\n.59 .90 .71 .10\n( ¨Ostling and Tiedemann, 2016) efmaral\n.93 .08\n.53 .47\n.72 .28\n( ¨Ostling and Tiedemann, 2016) fast-align\n.86 .15\n.33 .67\n.68 .33\n(Zenkel et al., 2019) Giza++\n.21\n.06\n.28\n(Garg et al., 2019) Multitask\n.20\n.08\nBaselines\nWord\nfast-align/IBM2\n.71 .81 .76 .25\n.70 .73 .71 .29\n.60 .54 .57 .43\n.81 .93 .86 .15\n.34 .33 .34 .66\n.69 .67 .68 .33\nGiza++/IBM4\n.71 .79 .75 .26\n.79 .75 .77 .23\n.55 .48 .51 .49\n.90 .95 .92 .09\n.47 .43 .45 .55\n.74 .64 .69 .31\neﬂomal\n.84 .86 .85 .15\n.80 .75 .77 .23\n.68 .55 .61 .39\n.91 .94 .93 .08\n.61 .44 .51 .49\n.81 .63 .71 .29\nSubword\nfast-align/IBM2\n.72 .84 .78 .23\n.67 .74 .71 .30\n.60 .56 .58 .42\n.80 .92 .85 .16\n.39 .37 .38 .62\n.69 .67 .68 .32\nGiza++/IBM4\n.79 .86 .82 .18\n.78 .78 .78 .22\n.58 .56 .57 .43\n.89 .95 .92 .09\n.52 .44 .48 .52\n.74 .64 .69 .32\neﬂomal\n.80 .88 .84 .17\n.74 .78 .76 .24\n.66 .60 .63 .37\n.88 .95 .91 .09\n.58 .47 .52 .48\n.78 .67 .72 .28\nThis Work\nWord\nfastText - Itermax\n.74 .69 .72 .29\n.69 .56 .62 .38\n.63 .45 .53 .48\n.74 .78 .76 .24\n.63 .42 .51 .49\n.64 .40 .50 .51\nmBERT[8] - Itermax\n.87 .87 .87 .14\n.85 .77 .81 .19\n.80 .63 .70 .30\n.91 .95 .93 .08\n.75 .47 .58 .43\n.82 .58 .68 .32\nXLM-R[8] - Itermax\n.89 .85 .87 .13\n.86 .73 .79 .21\n.84 .63 .72 .28\n.91 .93 .92 .08\n.79 .49 .61 .39\n.87 .61 .71 .29\nfastText - Argmax\n.86 .59 .70 .30\n.81 .48 .60 .40\n.75 .38 .50 .50\n.85 .71 .77 .22\n.75 .36 .49 .52\n.77 .34 .47 .53\nmBERT[8] - Argmax\n.95 .80 .87 .13\n.92 .69 .79 .21\n.88 .54 .67 .33\n.97 .91 .94 .06\n.84 .39 .54 .47\n.90 .50 .64 .36\nXLM-R[8] - Argmax\n.96 .80 .87 .13\n.93 .68 .79 .22\n.91 .57 .70 .30\n.96 .91 .93 .06\n.88 .45 .59 .41\n.94 .56 .70 .30\nSubword\nfastText - Itermax\n.61 .57 .59 .41\n.63 .54 .58 .42\n.20 .07 .11 .90\n.70 .76 .73 .28\n.14 .05 .07 .93\n.56 .38 .45 .55\nmBERT[8] - Itermax\n.84 .89 .86 .14\n.83 .80 .81 .19\n.76 .65 .70 .30\n.91 .96 .93 .08\n.71 .49 .58 .42\n.79 .62 .69 .31\nXLM-R[8] - Itermax\n.84 .89 .86 .14\n.83 .78 .80 .20\n.79 .67 .72 .28\n.89 .94 .92 .09\n.75 .52 .62 .39\n.83 .64 .72 .28\nfastText - Argmax\n.72 .48 .58 .42\n.75 .45 .56 .44\n.27 .06 .09 .91\n.80 .67 .73 .26\n.14 .02 .04 .96\n.67 .31 .43 .58\nmBERT[8] - Argmax\n.92 .81 .86 .14\n.92 .72 .81 .19\n.85 .56 .67 .33\n.96 .92 .94 .06\n.81 .41 .55 .45\n.88 .51 .65 .35\nXLM-R[8] - Argmax\n.92 .83 .87 .13\n.92 .72 .81 .19\n.87 .59 .71 .30\n.95 .91 .93 .07\n.86 .47 .61 .39\n.91 .59 .71 .29\nTable 7: Comparison of word and subword levels. Best overall result per column in bold.\nENG-DEU\nENG-CES\nENG-HIN\nEmb. Method\nPrec. Rec. F1 AER Prec. Rec. F1 AER Prec. Rec. F1 AER\nfastText\nArgmax\n.75\n.45 .56\n.44\n.72\n.48 .58\n.42\n.14\n.02 .04\n.96\n+Dist\n.79\n.51 .62\n.38\n.77\n.58 .66\n.34\n.16\n.04 .06\n.94\n+Null\n.76\n.43 .55\n.45\n.74\n.47 .57\n.42\n.14\n.02 .04\n.96\nItermax\n.63\n.54 .58\n.42\n.61\n.57 .59\n.41\n.14\n.05 .07\n.93\n+Dist\n.67\n.60 .64\n.36\n.63\n.66 .65\n.36\n.15\n.07 .09\n.91\n+Null\n.64\n.52 .57\n.43\n.62\n.56 .59\n.41\n.14\n.04 .07\n.93\nMatch\n.51\n.58 .54\n.46\n.44\n.61 .52\n.49\n.10\n.08 .09\n.91\n+Dist\n.59\n.66 .62\n.38\n.54\n.71 .61\n.39\n.10\n.09 .09\n.91\n+Null\n.52\n.57 .54\n.46\n.46\n.60 .52\n.48\n.10\n.08 .09\n.91\nmBERT[8]\nArgmax\n.92\n.72 .81\n.19\n.92\n.81 .86\n.14\n.81\n.41 .55\n.45\n+Dist\n.90\n.70 .79\n.21\n.91\n.80 .85\n.15\n.65\n.30 .41\n.59\n+Null\n.93\n.70 .80\n.20\n.92\n.78 .85\n.15\n.82\n.40 .54\n.47\nItermax\n.83\n.80 .81\n.19\n.84\n.89 .86\n.14\n.71\n.49 .58\n.42\n+Dist\n.81\n.77 .79\n.21\n.82\n.87 .84\n.16\n.53\n.35 .42\n.58\n+Null\n.85\n.77 .81\n.20\n.84\n.86 .85\n.15\n.72\n.47 .57\n.43\nMatch\n.75\n.80 .78\n.23\n.76\n.90 .82\n.18\n.64\n.52 .58\n.43\n+Dist\n.72\n.77 .75\n.26\n.74\n.88 .80\n.20\n.45\n.37 .40\n.60\n+Null\n.77\n.78 .78\n.23\n.77\n.88 .82\n.19\n.65\n.51 .57\n.43\nTable 8: Comparison of methods for inducing align-\nments from similarity matrices.\nAll results are\nsubword-level. Best result per embedding type across\ncolumns in bold.\nwhereas GDFA is typically used in statistical ma-\nchine translation.\nA.4\nAlignment Examples for Different\nMethods\nWe show examples in Figure 10, Figure 11, Fig-\nure 12, and Figure 13. They provide an overview\nhow the methods actually affect results.\n0 <= x < 5\n5 <= x < 25\n25 <= x < 125\n125 <= x\nFrequency Bin\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nF1\nmBERT[8](Argmax)\neflomal\nword\nsubword\n0 <= x < 5\n5 <= x < 25\n25 <= x < 125\n125 <= x\nFrequency Bin\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nF1\nmBERT[8](Argmax)\neflomal\nword\nsubword\nFigure 8: Results for different frequency bins. An edge\nin S, P, or A is attributed to exactly one bin based on\nthe minimum frequency of the involved words (denoted\nby x). Top: Eﬂomal trained and frequencies computed\non 100k parallel sentences. Bottom: 1000k parallel sen-\ntences.\nB\nHyperparameters\nB.1\nOverview\nWe provide a list of customized hyperparameters\nused in our computations in Table 10. There are\nthree options how we came up with the hyperpa-\nrameters: a) We simply used default values of 3rd\nparty software. b) We chose an arbitrary value.\n52",
      "page": 52,
      "type": "text",
      "language": "hi"
    },
    {
      "level": "H2",
      "text": "1640\nENG-CES\nENG-DEU\nENG-FAS\nENG-FRA\nENG-HIN\nENG-RON\nMethod Symm. Prec. Rec. F1 AER Prec. Rec. F1 AER Prec. Rec. F1 AER Prec. Rec. F1 AER Prec. Rec. F1 AER Prec. Rec. F1 AER\neﬂomal\nInters.\n.95\n.79 .86 .14\n.91\n.66 .76 .24\n.88\n.43 .58 .42\n.96\n.90 .93 .07\n.81\n.37 .51 .49\n.91\n.56 .70 .31\nGDFA\n.84\n.86 .85 .15\n.80\n.75 .77 .23\n.68\n.55 .61 .39\n.91\n.94 .93 .08\n.61\n.44 .51 .49\n.81\n.63 .71 .29\nfast-align Inters.\n.89\n.69 .78 .22\n.87\n.60 .71 .29\n.78\n.43 .55 .45\n.93\n.84 .88 .11\n.55\n.22 .31 .69\n.89\n.50 .64 .36\nGDFA\n.71\n.81 .76 .25\n.70\n.73 .71 .29\n.60\n.54 .57 .43\n.81\n.93 .86 .15\n.34\n.33 .34 .66\n.69\n.67 .68 .33\nGIZA++ Inters.\n.95\n.60 .74 .26\n.92\n.62 .74 .26\n.89\n.26 .40 .60\n.97\n.89 .93 .06\n.82\n.25 .38 .62\n.95\n.47 .63 .37\nGDFA\n.71\n.79 .75 .26\n.79\n.75 .77 .23\n.55\n.48 .51 .49\n.90\n.95 .92 .09\n.47\n.43 .45 .55\n.74\n.64 .69 .31\nTable 9: Comparison of symmetrization methods at the word level. Best result across rows per method in bold.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.60\n0.65\nF1\nArgmax\nMatch\n80\n85\n90\n95\n100\n(percentile)\n0.6\n0.8\nPrec.\nRec.\nF1\nFigure 9: Top: F1 for ENG-DEU with fastText at word-\nlevel for different values of κ. Bottom: Performance\nfor ENG-DEU with mBERT[8] (Match) at word-level\nwhen setting the value of τ to different percentiles. τ\ncan be used for trading precision against recall. F1 re-\nmains stable although it decreases slightly when assign-\ning τ the value of a smaller percentile (e.g., 80).\nUsually we fell back to well-established and rather\nconventional values (e.g., embedding dimension\n300 for static embeddings). c) We deﬁned a reason-\nable but arbitrary range, out of which we selected\nthe best value using grid search. Table 10 lists\nthe ﬁnal values we used as well as how we came\nup with the speciﬁc value. For option c) the corre-\nsponding analyses are in Figure 4 and Table 3 in the\nmain paper as well as in §B.2 in this supplementary\nmaterial.\nB.2\nNull and Distortion Extensions\nIn Figure 9 we plot the performance for different\nvalues of κ. We observe that introducing distortion\nindeed helps (i.e., κ > 0) but the actual value is not\ndecisive for performance. This is rather intuitive,\nas a small adjustment to the similarities is sufﬁcient\nwhile larger adjustments do not necessarily change\nthe argmax or the optimal point in the matching\nalgorithm. We choose κ = 0.5.\nFor τ in null-word extension, we plot precision,\nrecall and F1 in Figure 9 when assigning τ different\npercentile values. Note that values for τ depend\non the similarity distribution of all aligned edges.\nAs expected, when using the 100th percentile no\nedges are removed and thus the performance is\nnot changed compared to not having a null-word\nextension. When decreasing the value of τ the\nprecision increases and recall goes down, while F1\nremains stable. We use the 95th percentile for τ .\nC\nReproducibility Information\nC.1\nComputing Infrastructures, Runtimes,\nNumber of Parameters\nWe did all computations on up to 48 cores of In-\ntel(R) Xeon(R) CPU E7-8857 v2 with 1TB mem-\nory and a single GeForce GTX 1080 GPU with\n8GB memory.\nRuntimes for aligning 500 parallel sentences on\nENG-DEU are reported in Table 12. mBERT and\nXLM-R computations are done on the GPU. Note\nthat fast-align, GIZA++ and eﬂomal usually need\nto be trained on much more parallel data to achieve\nbetter performance: this increases their runtime.\nAll our proposed methods are parameter-free.\nIf we consider the parameters of the pretrained lan-\nguage models and pretrained embeddings then fast-\nText has around 1 billion parameters (up to 500k\nwords per language, 7 languages and embedding\ndimension 300), mBERT has 172 million, XLM-R\n270 million parameters.\nMethod\nRuntime[s]\nfast-align\n4\nGIZA++\n18\neﬂomal\n5\nmBERT[8] - Argmax\n15\nXLM-R[8] - Argmax\n22\nTable 12: Runtime (average across 5 runs) in seconds\nfor each method to align 500 parallel sentences.\nC.2\nData\nTable 11 provides download links to all data used.\n53",
      "page": 53,
      "type": "text",
      "language": "hi"
    },
    {
      "level": "H2",
      "text": "1641\nSystem\nParameter\nValue\nfastText\nVersion\n0.9.1\nCode URL\nhttps://github.com/facebookresearch/fastText/archive/v0.9.1.zip\nDownloaded on\n11.11.2019\nEmbedding Dimension\n300\nmBERT,XLM-R\nCode: Huggingface Transformer\nVersion 2.3.1\nMaximum Sequence Length\n128\nfastalign\nCode URL\nhttps://github.com/clab/fast align\nGit Hash\n7c2bbca3d5d61ba4b0f634f098c4fcf63c1373e1\nFlags\n-d -o -v\neﬂomal\nCode URL\nhttps://github.com/robertostling/eﬂomal\nGit Hash\n9ef1ace1929c7687a4817ec6f75f47ee684f9aff\nFlags\n–model 3\nGIZA++\nCode URL\nhttp://web.archive.org/web/20100221051856/http://code.google.com/p/giza-pp\nVersion\n1.0.3\nIterations\n5 iter. HMM, 5 iter. Model 1, 5 iter. Model3, 5 iter. Model 4 (DEFAULT)\np0\n0.98\nVecmap\nCode URL\nhttps://github.com/artetxem/vecmap.git\nGit Hash\nb82246f6c249633039f67fa6156e51d852bd73a3\nManual Vocabulary Cutoff\n500000\nDistortion Ext.\nκ\n0.5 (chosen ouf of [0.0, 0.1, . . . , 1.0] by grid search, criterion: F1)\nNull Extension\nτ\n95th percentile of similarity distribution of aligned edges (chosen out of [80, 90, 95, 98, 99,\n99.5] by grid search, criterion: F1)\nArgmax\nLayer\n8 (for mBERT and XLM-R, chosen out of [0, 1, . . . , 12] by grid search, criterion: F1 )\nVecmap\nα\n0.9 (chosen out of [0.9, 0.95, 1] by grid search, criterion: F1)\nIterations nmax\n2 (chosen out of [1,2,3] by grid search, criterion: F1)\nTable 10: Overview on hyperparameters. We only list parameters where we do not use default values. Shown are\nthe values which we use unless speciﬁcally indicated otherwise.\nLang.\nName\nDescription\nLink\nENG-CES\n(Mareˇcek, 2008)\nGold Alignment\nhttp://ufal.mff.cuni.cz/czech-english-manual-word-alignment\nENG-DEU\nEuroParl-based\nGold Alignment\nwww-i6.informatik.rwth-aachen.de/goldAlignment/\nENG-FAS\n(Tavakoli and Faili, 2014)\nGold Alignment\nhttp://eceold.ut.ac.ir/en/node/940\nENG-FRA\nWPT2003, (Och and Ney, 2000),\nGold Alignment\nhttp://web.eecs.umich.edu/ mihalcea/wpt/\nENG-HIN\nWPT2005\nGold Alignment\nhttp://web.eecs.umich.edu/ mihalcea/wpt05/\nENG-RON\nWPT2005 (Mihalcea and Pedersen, 2003)\nGold Alignment\nhttp://web.eecs.umich.edu/ mihalcea/wpt05/\nENG-CES\nEuroParl (Koehn, 2005)\nParallel Data\nhttps://www.statmt.org/europarl/\nENG-DEU\nEuroParl (Koehn, 2005)\nParallel Data\nhttps://www.statmt.org/europarl/\nENG-DEU\nParaCrawl\nParallel Data\nhttps://paracrawl.eu/\nENG-FAS\nTEP (Pilevar et al., 2011)\nParallel Data\nhttp://opus.nlpl.eu/TEP.php\nENG-FRA\nHansards (Germann, 2001)\nParallel Data\nhttps://www.isi.edu/natural-language/download/hansard/index.html\nENG-HIN\nEmille (McEnery et al., 2000)\nParallel Data\nhttp://web.eecs.umich.edu/ ˜mihalcea/wpt05/\nENG-RON\nConstitution, Newspaper\nParallel Data\nhttp://web.eecs.umich.edu/ mihalcea/wpt05/\nAll langs.\nWikipedia (downloaded October 2019)\nMonolingual Text\ndownload.wikimedia.org/[X]wiki/latest/[X]wiki-latest-pages-articles.xml.bz2\nTable 11: Overview of datasets. “Lang.” uses ISO 639-3 language codes.\n54",
      "page": 54,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "1642\nWir\nglauben\nnicht\n,\ndaß\nwir\nnur\nRosinen\nherauspicken\nsollten\n.\nWe\ndo\nnot\nbelieve\nthat\nwe\nshould\ncherry-pick\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nArgmax(circle) vs. Itermax(box)\nWir\nglauben\nnicht\n,\ndaß\nwir\nnur\nRosinen\nherauspicken\nsollten\n.\nWe\ndo\nnot\nbelieve\nthat\nwe\nshould\ncherry-pick\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nMatch(circle) vs. Match+Null(box)\nWir\nglauben\nnicht\n,\ndaß\nwir\nnur\nRosinen\nherauspicken\nsollten\n.\nWe\ndo\nnot\nbelieve\nthat\nwe\nshould\ncherry-pick\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nfastText(circle) vs. fastText+Dist(box)\nFigure\n10:\nComparison\nof\nalignment\nmethods.\nDark/light green: sure/possible edges in the gold stan-\ndard. Circles are alignments from the ﬁrst mentioned\nmethod in the subﬁgure title, boxes alignments from\nthe second method.\nNicht\nnur\nin\nEuropa\ngibt\nes\nnoch\nkeine\nharmonisierten\nRegelungen\n.\nThe\nabsence\nof\nharmonized\nrules\ngoverning\nintellectual\nproperty\nis\nnot\nunique\nto\nEurope\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nArgmax(circle) vs. Itermax(box)\nNicht\nnur\nin\nEuropa\ngibt\nes\nnoch\nkeine\nharmonisierten\nRegelungen\n.\nThe\nabsence\nof\nharmonized\nrules\ngoverning\nintellectual\nproperty\nis\nnot\nunique\nto\nEurope\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nMatch(circle) vs. Match+Null(box)\nNicht\nnur\nin\nEuropa\ngibt\nes\nnoch\nkeine\nharmonisierten\nRegelungen\n.\nThe\nabsence\nof\nharmonized\nrules\ngoverning\nintellectual\nproperty\nis\nnot\nunique\nto\nEurope\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nfastText(circle) vs. fastText+Dist(box)\nFigure 11: More examples.\n55",
      "page": 55,
      "type": "text",
      "language": "de"
    },
    {
      "level": "H2",
      "text": "1643\nDadurch\nwürde\ndie\nZahl\nder\nKindesentführungen\nwesentlich\ngesenkt\nwerden\n.\nThis\nwould\ncut\ndown\na\ngreat\ndeal\non\nabductions\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nArgmax(circle) vs. Itermax(box)\nDadurch\nwürde\ndie\nZahl\nder\nKindesentführungen\nwesentlich\ngesenkt\nwerden\n.\nThis\nwould\ncut\ndown\na\ngreat\ndeal\non\nabductions\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nfastText(circle) vs. fastText+Dist(box)\nDadurch\nwürde\ndie\nZahl\nder\nKindesentführungen\nwesentlich\ngesenkt\nwerden\n.\nThis\nwould\ncut\ndown\na\ngreat\ndeal\non\nabductions\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nMatch(circle) vs. Match+Null(box)\nFigure 12: More examples.\nWir\nalle\nsind\ndavon\nbetroffen\n.\nThese\nthings\nconcern\nall\nof\nus\n.\nx\nx\nx\nx\nx\nx\nx\nx\nx\nArgmax(circle) vs. Itermax(box)\nWir\nalle\nsind\ndavon\nbetroffen\n.\nThese\nthings\nconcern\nall\nof\nus\n.\nx\nx\nx\nx\nx\nx\nx\nx\nfastText(circle) vs. fastText+Dist(box)\nWir\nalle\nsind\ndavon\nbetroffen\n.\nThese\nthings\nconcern\nall\nof\nus\n.\nx\nx\nx\nx\nx\nx\nx\nx\nMatch(circle) vs. Match+Null(box)\nFigure 13: More examples.\n56",
      "page": 56,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Chapter 3\n\nParCourE: A Parallel Corpus\nExplorer for a Massively\nMultilingual Corpus",
      "page": 57,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing: System Demonstrations, pages 63–72, August 1st - August 6th, 2021.\n©2021 Association for Computational Linguistics\n63\nParCourE: A Parallel Corpus Explorer for\na Massively Multilingual Corpus\nAyyoob Imani1, Masoud Jalili Sabet1, Philipp Dufter1,\nMichael Cysouw2, Hinrich Sch¨utze1\n1Center for Information and Language Processing (CIS), LMU Munich, Germany\n2Research Center Deutscher Sprachatlas, Philipps University Marburg, Germany.\n{ayyoob, masoud, philipp}@cis.lmu.de\nAbstract\nWith\nmore\nthan\n7000\nlanguages\nworld-\nwide, multilingual natural language process-\ning (NLP) is essential both from an academic\nand commercial perspective. Researching ty-\npological properties of languages is fundamen-\ntal for progress in multilingual NLP. Exam-\nples include assessing language similarity for\neffective transfer learning, injecting inductive\nbiases into machine learning models or creat-\ning resources such as dictionaries and inﬂec-\ntion tables. We provide ParCourE, an online\ntool that allows to browse a word-aligned par-\nallel corpus, covering 1334 languages.\nWe\ngive evidence that this is useful for typologi-\ncal research. ParCourE can be set up for any\nparallel corpus and can thus be used for typo-\nlogical research on other corpora as well as for\nexploring their quality and properties.\n1\nIntroduction\nWhile ≈7000 languages are spoken (Eberhard et al.,\n2020), the bulk of NLP research addresses English\nonly. However, multilinguality is an essential ele-\nment of NLP. It not only supports exploiting com-\nmon structures across languages and eases mainte-\nnance for globally operating companies, but also\nhelps save languages from digital extinction and\nfosters more diversity in NLP techniques.\nThere are extensive resources that can be used\nfor massively multilingual typological research,\nsuch as WALS (Dryer and Haspelmath, 2013), Glot-\ntolog (Hammarstrm et al., 2020), BabelNet (Nav-\nigli and Ponzetto, 2012) or http://panlex.org. Many\nof them are manually created or crowdsourced,\nwhich guarantees high quality, but limits coverage,\nboth in terms of content and languages.\nWe work on the Parallel Bible Corpus (PBC)\n(Mayer and Cysouw, 2014), covering 1334 lan-\nguages. More speciﬁcally, we provide a word-\naligned version of PBC, created using state-of-the-\nart word alignment tools. As word alignments\nFigure 1: Screenshot of the ParCourE interface. It pro-\nvides a word-aligned version of the Parallel Bible Cor-\npus (PBC) spanning 1334 languages. Users can search\nfor sentences in any language and see their alignments\nin other languages from MULTALIGN page. Alterna-\ntively they can feed their parallel sentences to INTER-\nACTIVE view and see their word level alignments. They\ncan look up translations of words in other languages,\nautomatically induced from word alignments, from the\nLEXICON view (This page is interconnected with MUL-\nTALIGN).\nStatistics of the corpus is calculated and\nshown in the Stats view.\nthemselves are only of limited use, we provide an\ninteractive online tool1 that allows effective brows-\ning of the alignments.\nThe main contributions of this work are: i) We\nprovide a word-aligned version of the Parallel Bible\nCorpus (PBC) spanning 1334 languages and a total\nof 20M sentences (‘verses’). For the alignment we\nuse the state-of-the-art alignment methods SimA-\nlign (Jalili Sabet et al., 2020) and Eﬂomal ( ¨Ostling\nand Tiedemann, 2016a). ii) We release ParCourE,\n1http://parcoure.cis.lmu.de/\n58",
      "page": 58,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "64\na user interface for browsing word alignments, see\nthe MULTALIGN view in Figure 1. We demon-\nstrate the usefulness of ParCourE for typological\nresearch by presenting use cases in §6. iii) In addi-\ntion to browsing word alignments, we provide an\naggregated version in a LEXICON view and com-\npute statistics that support assessing the quality of\nthe word alignments. The two views (MULTALIGN\nand LEXICON views) are interlinked, resulting in a\nricher user experience. iv) ParCourE has a generic\ndesign and can be set up for any parallel corpus.\nThis is useful for analyzing and managing paral-\nlel corpora; e.g., errors in an automatically mined\nparallel corpus can be inspected and ﬂagged for\ncorrection.\n2\nRelated Work\nWord Alignment is an important tool for typolog-\nical analysis (Lewis and Xia, 2008) and annotation\nprojection (Yarowsky et al., 2001; ¨Ostling, 2015;\nAsgari and Sch¨utze, 2017).\nStatistical models\nsuch as IBM models (Brown et al., 1993), Giza++\n(Och and Ney, 2003), fast-align (Dyer et al., 2013)\nand Eﬂomal ( ¨Ostling and Tiedemann, 2016b) are\nwidely used. Recently, neural models were pro-\nposed, such as SimAlign (Jalili Sabet et al., 2020),\nAwesome-align (Dou and Neubig, 2021), and meth-\nods that are based on neural machine translation\n(Garg et al., 2019; Zenkel et al., 2020). We use\nEﬂomal and SimAlign for generating alignments.\nResources. There are many online resources\nthat enable typological research. WALS (Dryer\nand Haspelmath, 2013) provides manually created\nfeatures for more than 2000 languages. We pre-\npare a multiparallel corpus for investigating these\nfeatures on real data. http://panlex.org is an on-\nline dictionary project with 2500 dictionaries cov-\nering 5700 languages and BabelNet (Navigli and\nPonzetto, 2012) is a large semantic network cover-\ning 500 languages, but their information is gener-\nally on the type level, without access to example\ncontexts. In contrast, ParCourE supports the explo-\nration of word translations across 1334 languages\nin context.\nAnother line of work uses the Parallel Bible\nCorpus (PBC) for analysis. Asgari and Sch¨utze\n(2017) investigate tense typology across PBC lan-\nguages. Xia and Yarowsky (2017) created a mul-\ntiway alignment based on fast-align (Dyer et al.,\n2013) and extracted resources such as paraphrases\nfor 27 Bible editions. Wu et al. (2018) used align-\nments to extract names from the PBC.\nOne of the ﬁrst attempts to index the Bible and\nalign words in multiple languages were Strong’s\nnumbers (Strong, 2009[1890]); they tag words with\nsimilar meanings with the same ID. Mayer and\nCysouw (2014) created an inverted index of word\nforms. ¨Ostling (2014) align massively parallel cor-\npora simultaneously. We use the Eﬂomal word\naligner by the same authorsostling2016efﬁcient.\nFinally, we review work on Word Alignment\nBrowsers. Gilmanov et al. (2014)’s tool supports\nvisualization and editing of word alignments. Ak-\nbik and Vollgraf (2017) use co-occurrence weights\nfor word alignment and provide a tool for the in-\nspection of annotation projection. Aulamo et al.\n(2020)’s ﬁltering tool increases the quality of\n(mined) parallel corpora. Gra¨en et al. (2017) rely\non linguistic preprocessing, target corpus and word\nalignment exploration, do not show the graph of\nalignment edges and do not provide a dictionary\nview. While there is commonality with this prior\nwork, ParCourE is distinguished by both its func-\ntionality and its motivating use cases: an important\nuse case for us are typological searches; linguis-\ntic preprocessing is not available for many PBC\nlanguages; ParCourE can be used as an interactive\nexplorer (but is not a fully-automated pipeline for\na speciﬁc use case); our goal is not annotation;\nwe use state-of-the-art word alignment methods.\nHowever, much of the complementary functional-\nity in prior work would be useful additions to Par-\nCourE. Another source of useful additional func-\ntionality would be work on embedding learning\n(Dufter et al., 2018; Kurfal and ¨Ostling, 2018) and\nmachine translation (Tiedemann, 2018; Santy et al.,\n2019; Mueller et al., 2020) for PBC.\n3\nFeatures\nParCourE’s user facing functionality can be divided\ninto three main parts: MULTALIGN and LEXICON\nviews and interconnections between the two.\n3.1\nMultiparallel Alignment Browser:\nMULTALIGN\nParCourE allows the user to search through the\nparallel corpus and check word alignments in a\nmultiparallel corpus. An overview of MULTALIGN\nis shown in Figure 2.\nIn the search ﬁeld (a(1)), the user can enter a\ntext query and select (a(2)) multiple sentences for\nalignment. For narrowing the search scope, the\n59",
      "page": 59,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "65\nFigure 2: An overview of the MULTALIGN view. a)\nSearch ﬁeld for selecting sentences [a(1)] and the list\nof selected sentences [a(2)]. Any language can be used\nfor the source sentence – in this case, it is English. b)\nSearch bar for selecting the target languages. c) The\nalignment graph for the selected sentences in the source\nand the target languages. d) Switch button for simple\nview / cluster view. e) Save and retrieve search results\nlanguage and edition of the text segment can be\nspeciﬁed in the beginning, e.g., by typing l:eng-\nnewworld2013. Similarly, v:40002017 speciﬁes a\nverse ID.\nPBC has 1334, so showing alignments for all\ntranslations of a sentence is difﬁcult. We provide a\ndrop-down (b) to select a subset of target languages\nfor display.\nFor each sentence, a graph of alignment edges\nbetween selected languages is shown (c). By hover-\ning over a word, the alignments of that word will be\nhighlighted. Above each alignment graph, there is\na button to switch between Simple view and Clus-\nter view (d). In the simple view, when hovering\nover a word, only the alignment edges connected\nto that word are highlighted; in the cluster view,\nall words in a cluster (neighbors of neighbors) that\nare aligned together will be highlighted. We do not\nactually run any clustering algorithm on the align-\nment graph. Instead we simply highlight words that\nare up to two hops away from the hovered word.\nThis helps spot a group of words across languages\nthat have the same meaning.\nCreating queries for typology research can take\ntime. Thus, MULTALIGN allows the user to save\nand retrieve (e) queries.\nFigure 3: LEXICON view example: for the English\nword “confusion”, there are ﬁve frequent translations in\nGerman. “Unordnung” literally means “disorder” and\n“Verwirrung” means “bewilderment”.\n3.2\nLexicon View: LEXICON\nThe MULTALIGN view allows the user to focus on\nword alignments on the sentence level and study the\ntypological structure of languages in context. The\nLEXICON view focuses on word translations. The\nuser can specify a source language by selecting\nthe language code. This is to distinguish words\nwith the same spelling in different languages. The\nuser can search for one or multiple word(s) and\nspecify target language(s). A pie chart for each\ntarget language depicting translations of the word\nis generated. Figure 3 shows German translations\nof “confusion” and the number of alignment edges\nfor each. Word alignments are not perfect, so pie\ncharts may also contain errors.\n3.3\nInterconnections\nBoth MULTALIGN and LEXICON views provide\nimportant features to the user for exploring the par-\nallel corpus. For many use cases (cf. §6), the user\nmay need to go back and forth between the views.\nFor example, if she notices an error in the word\nalignment, she may want to check the LEXICON\nstatistics to see if one of the typical translations of\nan incorrectly aligned word occurs in the sentence.\nThus, the two views are interconnected. In the\nMULTALIGN view, the user will be transferred to\nthe LEXICON statistics of a word by clicking on\nit. This will open the LEXICON view, showing the\nsearch results for the selected word. Conversely,\nif the user clicks on one of the target translations\nin the LEXICON view, the MULTALIGN view will\nshow sentences where this correspondence is part\nof the word alignment between source and target\ntranslation.\n60",
      "page": 60,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "66\n# editions\n1758\n# verses\n20,470,892\n# languages\n1334\n# verses / # editions\n11,520\n# tokens / # verses\n28.6\nTable 1: PBC corpus statistics\n3.4\nAlignment Generation View:\nINTERACTIVE\nThe views mentioned so far provide the ability to\nsearch over the indexed corpus. This is useful when\nthe main corpus of interest is ﬁxed and the user has\ngenerated its alignments.\nThe INTERACTIVE view allows the user to study\nthe alignments between arbitrary input sentences\nthat are not necessarily in the corpus. Since the\ninput sentences are not part of a corpus, INTERAC-\nTIVE uses SimAlign to generate alignments for all\npossible pairs of sentences. Similar to MULTAL-\nIGN, the INTERACTIVE view shows the alignment\nbetween the input sentences.\n4\nExperimental Setup\nCorpus. We set up ParCourE on the PBC corpus\nprovided by Mayer and Cysouw (2014). The ver-\nsion we use consists of 1758 editions (i.e., transla-\ntions) of the Bible in 1334 languages (distinct ISO\n639-3 codes). Table 1 shows corpus statistics. We\nuse the PBC tokenization, which contains errors\nfor a few languages (e.g., Thai). We extract word\nalignments for all possible language pairs. Since\nnot all Bible verses are available in all languages,\nfor each language pair we only consider mutually\navailable verses.\nPBC aligns Bible editions on the verse level by\nusing verse-IDs that indicate book, chapter and\nverse (see below). Although one verse may contain\nmultiple sentences, we do not split verses into in-\ndividual sentences and consider each verse as one\nsentence.\nRetrieval. Elasticsearch2 is a fast and scalable\nopen source search engine that provides distributed\nfulltext search. The setup is straightforward using\nan easy-to-use JSON web interface. We use it as the\nback-end for ParCourE’s search requirement. We\nﬁnd that a single instance is capable of handling the\nwhole PBC corpus efﬁciently, so we do not need a\ndistributed setup. For bigger corpora, a distributed\nsetup may be required. We created two types of\ninverted indices for our data: an edge-ngram in-\n2https://www.elastic.co/\ndex to support search-as-you-type capability and a\nstandard index for normal queries.\nAlignment Generation. SimAlign (Jalili Sabet\net al., 2020) is a recent word alignment method\nthat uses representations from pretrained language\nmodels to align sentences. It has achieved bet-\nter results than statistical word aligners. For the\nlanguages that multilingual BERT (Devlin et al.,\n2019) supports, we use SimAlign to generate word\nalignments. For the remaining languages, we use\nEﬂomal ( ¨Ostling and Tiedemann, 2016a), an efﬁ-\ncient word aligner using a Bayesian model with\nMarkov Chain Monte Carlo (MCMC) inference.\nThe alignments generated by SimAlign are sym-\nmetric. We use atools3 and the grow-diag-ﬁnal-and\nheuristic to symmetrize Eﬂomal alignments.\nLexicon Induction. We exploit the generated\nword alignments to induce lexicons for all 889,111\nlanguage pairs. To this end, we consider aligned\nwords as translations of each other. For a given\nword from the source language, we count the num-\nber of times a word from the target language is\naligned with it. The higher the number of align-\nments between two words, the higher the probabil-\nity that the two have the same meaning. We ﬁlter\nout translations with frequency less than 5%.\n5\nBackend Design\nAn overview of our architecture can be found in\nFigure 4. The code is available online.4\nParallel Data Format. We use the PBC corpus\nformat (Mayer and Cysouw, 2014): each verse has\na unique ID across languages / editions, the verse-\nID. The verse-ID is an 8-digit number, consisting\nof two digits for the book (e.g., 41 for the Gospel of\nMark), three digits for the Chapter, and two digits\nfor the verse itself. There are separate ﬁles for each\nedition. In each edition ﬁle, a line consists of the\nID and the verse, separated by a tab.\nIndexing. We identify a PBC verse using the\nfollowing format: {verse-ID}@{language-code}-\n{edition-name}. We use this identiﬁer to save and\nretrieve sentences with Elasticsearch. In addition,\nwe store all metadata identiﬁers within Elastic-\nsearch. Thus, we can search for a sentence by\nkeyword, sentence number (= verse-ID), language\ncode, or edition name.\nParCourE also supports the Corpus Alignment\n3https://github.com/clab/fast_align\n4https://github.com/cisnlp/parcoure\n61",
      "page": 61,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "67\nPresentation\nBootstrap\nHTML\nJavascript\nUser \nInteraction\nGunicorn/\nFlask\nAnalysis\nElastic\nAlignments\nMultiparallel\nCorpus\nEflomal\nSimAlign\nLogic\nData\nD3.js\nController\nCaching\nFigure 4: Overview of the system architecture.\nWe\nuse a standard front-end stack with d3.js for visual-\nization. The backend is written in Python, which we\nuse for computing alignments and performing analy-\nses such as lexicon induction. We use Elasticsearch\nfor search.\nThe input is a multiparallel corpus for\nwhich all alignments are precomputed. For speeding\nup the system we use smart caching algorithms for our\nanalyses. Icons taken without changes from https:\n//fontawesome.com/license.\nEncoding (CES)5 format. One can download par-\nallel corpora in CES format and use our tools to\nadapt them to ParCourE’s input format.\nAlignment Computation. Since Eﬂomal’s per-\nformance depends on the amount of data it uses\nfor training, we concatenate all editions to create\na bigger training corpus for languages that have\nmore than one edition. If language l1 has two, and\nlanguage l2 three different editions, then the ﬁnal\ntraining corpus for this language pair will contain\nsix aligned edition pairs.\nSystem Architecture. ParCourE is built on top\nof modern open source technologies, see Figure 4.\nThe back-end uses the Flask web framework,6 Gu-\nnicorn web server,7 and Elasticsearch.8 The front-\nend utilizes the Bootstrap CSS framework,9 and\nthe d3 visualization library.10 Since all these tools\nare free and open-source, there is no restriction on\nsetting up and releasing a new ParCourE instance.\nTo extract word alignments, one can use any tool,\nsuch as Eﬂomal, fast align or SimAlign.\nPerformance Improvements.\nFor good run-\ntime performance, we precompute the word align-\nments. Regarding LEXICON, given a query word\nand a target language, ParCourE ﬁrst looks for a\nprecomputed lexicon ﬁle; if it does not exist, Par-\n5https://www.cs.vassar.edu/CES/\n6https://flask.palletsprojects.com\n7https://gunicorn.org/\n8https://www.elastic.co/\n9https://getbootstrap.com/\n10https://d3js.org/\nCourE obtains the translations for the query word\nonline. To accelerate the translation process, Par-\nCourE employs Python’s multiprocessing library.\nThe number of CPU cores is decided online based\non the number of editions available for source and\ntarget languages.\nFor a corpus with 1334 languages, we will end\nup with 890,445 alignment ﬁles and the same num-\nber of lexicon ﬁles. We cache alignment / lexicon\nﬁles to speed up access. We use the Last Recently\nUsed (LRU) cache replacement algorithm.\n6\nParCourE Use Cases\nLanguages differ in how they encode mean-\nings/functions. There are various aspects that make\nsuch differences an interesting problem when deal-\ning with a dataset that has good coverage of the\nentire variation of the world’s languages. (i) Many\nsuch differences between languages are not widely\nacknowledged in linguistic theory, so to document\nthe extent of variation becomes a discovery of sorts.\nFor example, the fact that interrogative words might\ndistinguish between singular and plural (Figure 6)\nturns out to be a typologically salient differentiation\n(Mayer and Cysouw, 2012). (ii) The variation of\nlinguistic marking is even stronger in the domain\nof grammatical function, like the differentiation\nbetween the interrogative and relative pronoun in\nFigure 6. (iii) In lexical semantics, ParCourE sup-\nports the investigation of how languages carve up\nthe meaning space differently (cf. Figure 5), espe-\ncially when it comes to the ≈1000 low-resource\nlanguages covered in PBC. Massively parallel texts\nare an ideal resource to investigate such variation\n(Haspelmath, 2003).\nGrammatical differences between languages,\nlike differences in word order, have a long his-\ntory in research on worldwide linguistic variation\n(Greenberg, 1966; Dryer, 1992). However, being\nable to look at the usage of word order in speciﬁc\ncontexts (and being able to directly compare ex-\nactly the same context across languages) is only\npossible by using parallel texts. For example, spe-\nciﬁc orders of more than two elements can be di-\nrectly extracted from the parallel texts, like the\norder of demonstrative, numeral and noun “these\ntwo commandments” in Figure 7 (Cysouw, 2010).\nFor lack of space, we describe four more use\ncases only brieﬂy: grammatical markers vs. mor-\nphology as devices to express grammatical features\n(Figure 8); differences in how languages use gram-\n62",
      "page": 62,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "68\nFigure 5: Use case 1, lexical differentiation. French\n“femme” has two different translations in English\n(“wife” and “woman”) whereas German also conﬂates\nthe two different meanings.\nFigure 6: Use case 2, grammatical differentiation. En-\nglish “who” has three different translations in this Span-\nish example: relative pronoun (“que”), and singular\n(“qui´en)” and plural (“qui´enes”) interrogative pronoun.\nmatical case (Figure 9, ablative/dative in Latin can\ncorrespond to ﬁve different cases in Croatian); and\nexploration of paraphrases (Figure 10). See the\ncaptions of the ﬁgures for more details.\n7\nExtension to Other Corpora\nOur code is available on GitHub and can be generi-\ncally applied: you can create a ParCourE instance\nfor your own parallel corpus. Parallel corpora are\nessential for machine translation (MT); ParCourE’s\nfunctionality is useful for analyzing the quality of\na parallel corpus and the difﬁculty of the transla-\ntion problem it poses. We give three examples\ni) Incorrect sentence alignments can be identiﬁed,\ne.g., cases in which a target sentence is matched\nwith the merger of two sentences in the source:\ncf. Figure 11 where a short sentence in English\nis aligned with German and French sentences that\nalso contain a second sentence that is missing in\nEnglish. This functionality is particularly helpful\nfor mined parallel corpora that tend to contain er-\nFigure 7: Use case 3, word order variation. The En-\nglish order is demonstrative, numeral, noun whereas\nSwahili has noun, demonstrative, numeral.\nFigure 8: Use case 4, grammatical markers. In contrast\nto English, Seychelles Creole does not inﬂect verbs for\ntense and uses the past tense marker “ti” instead.\nroneous sentence pairs. ii) Suppose an MT system\ntrained on the parallel corpus makes a lexical error\nin a particular context c by mistranslating source\nword ws with target word wt. The LEXICON view\ncan be consulted for ws and the user can then click\non the erroneous target word wt to get back to a\nMULTALIGN view of aligned sentence pairs con-\ntaining ws and wt. She can then analyze why the\nMT system mismatched c with these contexts. Ex-\namples of the desired translation are easy to ﬁnd\nand inspect to support the formation of hypotheses\nas to the source of the error. iii) For multi-source\napproaches to MT (Zoph and Knight, 2016; Fi-\nrat et al., 2016; Libovick´y and Helcl, 2017; Crego\net al., 2010), ParCourE supports the inspection of\nall input sentences together. The MT system output\ncan also be loaded into ParCourE for a view that\ncontains all input sentences and the output sentence.\nSince any of the input sentences can be responsible\nfor an error in multi-source MT, this facilitates anal-\nysis and hypothesis formation as to what caused a\nspeciﬁc error.\n7.1\nComputing Infrastructure and Runtime\nWe did all computations on a machine with 48\ncores of Intel(R) Xeon(R) CPU E7-8857 v2 with\n1TB memory. In this experiment only one core was\nused.\nWe created a corpus of 5 translations in 4 lan-\nguages, with around 31k parallel sentences (over-\nally 155k sentences) and applied the ParCourE\npipeline to it. Runtimes for different parts of the\n63",
      "page": 63,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "69\nFigure 9: Use case 5, morphology. The Latin ending\n“ibus” in “fratribus” (dative/ablativ plural) corresponds\nto ﬁve different cases in Croatian: accusative, loca-\ntive/dative, nominative, genitive, instrumental (clock-\nwise starting from “bra´cu”).\nFigure 10: Use case 6, paraphrases. PBC is a rich\nsource of paraphrases since high-resource languages\nhave several translations (32 for English). ParCourE\ncan be used to explore these paraphrases. Here, the\nparaphrases “kill” and “murder” are correctly aligned,\n“always ready” and “run quickly” are not.\npipeline are reported in Table 2. The installation\nof the package is straightforward and as shown\nin the table, it takes around 12 minutes to initiate\nParCourE on a small corpus with 4 languages.\nMethod\nRuntime\nConversion from CES to ParCourE format\n153\nIndexing with Elasticsearch\n14\nAlignment generation with Eﬂomal\n537\nStats calculation\n22\nOverall\n726\nTable 2:\nRuntime in seconds for each part of the\npipeline to initiate a ParCourE instance on a corpus\nwith 4 languages and 31K parallel sentences.\n8\nConclusion\nProgress in multilingual NLP is an important goal\nof NLP and requires researching typological prop-\nerties of languages. Examples include assessing\nlanguage similarity for effective transfer learning,\ninjecting inductive biases into machine learning\nmodels and creating resources such as dictionaries\nand inﬂection tables. To serve such use cases, we\nFigure 11: Use case 7, quality analysis.\nParCourE\nmakes it easy to analyze the quality of the parallel cor-\npus. For this sentence, part of a Bible verse present in\nGerman and French is missing in English. Note that\nthe alignment of holy, heiligen to French fraternel is\nnot discovered.\nhave created ParCourE, an online tool for browsing\na word-aligned parallel corpus of 1334 languages,\nand given evidence that it is useful for typological\nresearch. ParCourE can be set up for any other par-\nallel corpus, e.g., for quality control and improve-\nment of automatically mined parallel corpora.\nAcknowledgments\nThis work was supported by the European Research\nCouncil (ERC, Grant No. 740516) and the Ger-\nman Federal Ministry of Education and Research\n(BMBF, Grant No. 01IS18036A). The third author\nwas supported by the Bavarian research institute for\ndigital transformation (bidt) through their fellow-\nship program. We thank the anonymous reviewers\nfor their constructive comments.\n9\nEthical Considerations\nWord alignments and lexicon induction as tasks\nthemselves may not have ethical implications.\nHowever, working on a biblical corpus requires\nspecial consideration of the following issues.\ni) The Bible is the central religious text of Chris-\ntianity and the Hebrew Bible that of Judaism. It\ncontains strong opinions and world views (e.g., on\ndivorce and homosexuality) that are not generally\nshared. We would like to emphasize that we treat\nthe PBC simply as a multiparallel corpus, and the\ncorpus does not necessarily reﬂect the opinions of\nthe authors nor of the institutions funding the au-\nthors. ii) In a similar vein, while the PBC has great\nlanguage coverage and allows for typological anal-\nysis, we need to be aware that languages might not\nbe accurately and completely reﬂected in the PBC.\nThe language used in the PBC might be outdated\nand is restricted to a relatively small subset of top-\nics and thus cannot be considered a balanced and\ncomplete view of the language. iii) We also need to\n64",
      "page": 64,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "70\nbe aware of selection bias. The PBC only covers a\nsubset of the world’s languages. The selection cri-\nteria are unknown and may be based on historical\nand cultural biases that we are not able to assess.\nReferences\nAlan Akbik and Roland Vollgraf. 2017.\nThe projec-\ntor: An interactive annotation projection visualiza-\ntion tool. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 43–48, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nEhsaneddin Asgari and Hinrich Sch¨utze. 2017. Past,\npresent, future: A computational investigation of the\ntypology of tense in 1000 languages. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 113–124,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nMikko Aulamo, Sami Virpioja, and J¨org Tiedemann.\n2020.\nOpusFilter: A conﬁgurable parallel corpus\nﬁltering toolbox. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, pages 150–156,\nOnline. Association for Computational Linguistics.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2).\nJosep Maria Crego, Aur´elien Max, and Franc¸ois Yvon.\n2010. Local lexical adaptation in machine transla-\ntion through triangulation: SMT helping SMT. In\nProceedings of the 23rd International Conference\non Computational Linguistics (Coling 2010), pages\n232–240, Beijing, China. Coling 2010 Organizing\nCommittee.\nMichael Cysouw. 2010.\nDealing with diversity: to-\nwards an explanation of NP word order frequencies.\nLinguistic Typology, 14(2):253–287.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZi-Yi Dou and Graham Neubig. 2021.\nWord align-\nment by ﬁne-tuning embeddings on parallel corpora.\nCoRR, abs/2101.08231.\nMatthew S. Dryer. 1992. The Greenbergian word order\ncorrelations. Language, 68(1):80–138.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online. Max Planck Institute for Evo-\nlutionary Anthropology, Leipzig.\nPhilipp Dufter, Mengjie Zhao, Martin Schmitt, Alexan-\nder Fraser, and Hinrich Sch¨utze. 2018. Embedding\nlearning through multilingual concept induction. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1520–1530, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013.\nA simple, fast, and effective reparameter-\nization of IBM model 2.\nIn Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nDavid M. Eberhard, F. Simons Gary, and D. Fen-\nnig (eds.) Charles. 2020. Ethnologue: Languages\nof the World, 23rd edition. SIL International.\nOrhan Firat, Baskaran Sankaran, Yaser Al-onaizan,\nFatos T. Yarman Vural, and Kyunghyun Cho. 2016.\nZero-resource translation with multi-lingual neural\nmachine translation.\nIn Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 268–277, Austin, Texas.\nAssociation for Computational Linguistics.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Lin-\nguistics.\nTimur Gilmanov, Olga Scrivner, and Sandra K¨ubler.\n2014.\nSWIFT aligner, a multifunctional tool for\nparallel corpora:\nVisualization, word alignment,\nand (morpho)-syntactic cross-language transfer. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14),\npages 2913–2919, Reykjavik, Iceland. European\nLanguage Resources Association (ELRA).\nJohannes Gra¨en, Dominique Sandoz, and Martin Volk.\n2017.\nMultilingwis2 ˙extendash explore your par-\nallel corpus.\nIn Proceedings of the 21st Nordic\nConference on Computational Linguistics, NODAL-\nIDA 2017, Gothenburg, Sweden, May 22-24, 2017,\nvolume 131 of Link¨oping Electronic Conference\nProceedings, pages 247–250. Link¨oping University\nElectronic Press / Association for Computational\nLinguistics.\nJoseph H. Greenberg. 1966.\nLanguage Universals:\nwith special reference to feature hierarchies. Janua\nLinguarum, Series Minor. Mouton, The Hague.\n65",
      "page": 65,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "71\nHarald Hammarstrm, Robert Forkel, Martin Haspel-\nmath, and Sebastian Bank. 2020.\nGlottolog 4.3.\nMax Planck Institute for the Science of Human His-\ntory.\nMartin Haspelmath. 2003. The geometry of grammat-\nical meaning: Semantic maps and cross-linguistic\ncomparison.\nIn Michael Tomasello, editor, The\nNew Psychology of Language: Cognitive and Func-\ntional Approaches to Language Structure (Volume\n2), pages 211–242. Erlbaum, Mahwah, NJ.\nMasoud Jalili Sabet, Philipp Dufter, Franc¸ois Yvon,\nand Hinrich Sch¨utze. 2020. SimAlign: High qual-\nity word alignments without parallel training data us-\ning static and contextualized embeddings. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 1627–1643, Online. As-\nsociation for Computational Linguistics.\nMurathan Kurfal and Robert ¨Ostling. 2018. Word em-\nbeddings for 1250 languages through multi-source\nprojection. In Seventh Swedish Language Technol-\nogy Conference.\nWilliam D. Lewis and Fei Xia. 2008. Automatically\nidentifying computationally relevant typological fea-\ntures.\nIn Proceedings of the Third International\nJoint Conference on Natural Language Processing:\nVolume-II.\nJindrich Libovick´y and Jindrich Helcl. 2017. Attention\nstrategies for multi-source sequence-to-sequence\nlearning. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 2: Short Papers, pages 196–202. Associa-\ntion for Computational Linguistics.\nThomas Mayer and Michael Cysouw. 2012. Language\ncomparison through sparse multilingual word align-\nment. In Proceedings of the EACL 2012 Joint Work-\nshop of LINGVIS & UNCLH, pages 54–62, Avignon,\nFrance. Association for Computational Linguistics.\nThomas Mayer and Michael Cysouw. 2014. Creating\na massively parallel Bible corpus. In Proceedings\nof the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 3158–\n3163, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nAaron Mueller, Garrett Nicolai, Arya D. McCarthy,\nDylan Lewis, Winston Wu, and David Yarowsky.\n2020.\nAn analysis of massively multilingual neu-\nral machine translation for low-resource languages.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 3710–3718, Mar-\nseille, France. European Language Resources Asso-\nciation.\nRoberto Navigli and Simone Paolo Ponzetto. 2012. Ba-\nbelNet: The automatic construction, evaluation and\napplication of a wide-coverage multilingual seman-\ntic network. Artif. Intell., 193:217–250.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational Linguistics, 29(1).\nRobert ¨Ostling. 2014.\nBayesian word alignment for\nmassively parallel texts. In Proceedings of the 14th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, EACL 2014,\nApril 26-30, 2014, Gothenburg, Sweden, pages 123–\n127. The Association for Computer Linguistics.\nRobert ¨Ostling. 2015.\nWord order typology through\nmultilingual word alignment. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2:\nShort Papers), pages 205–211, Beijing,\nChina. Association for Computational Linguistics.\nRobert ¨Ostling and J¨org Tiedemann. 2016a.\nEfﬁ-\ncient word alignment with Markov Chain Monte\nCarlo. Prague Bulletin of Mathematical Linguistics,\n106:125–146.\nRobert ¨Ostling and J¨org Tiedemann. 2016b. Efﬁcient\nword alignment with Markov Chain Monte Carlo.\nThe Prague Bulletin of Mathematical Linguistics,\n106(1).\nSebastin Santy, Sandipan Dandapat, Monojit Choud-\nhury, and Kalika Bali. 2019. INMT: Interactive neu-\nral machine translation prediction. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP): System Demonstrations,\npages 103–108, Hong Kong, China. Association for\nComputational Linguistics.\nJames Strong. 2009[1890]. Strong’s exhaustive concor-\ndance of the Bible. Hendrickson Publishers.\nJ¨org Tiedemann. 2018.\nEmerging language spaces\nlearned from massively multilingual corpora.\nIn\nProceedings of the Digital Humanities in the Nordic\nCountries 3rd Conference, DHN 2018, Helsinki, Fin-\nland, March 7-9, 2018, volume 2084 of CEUR Work-\nshop Proceedings, pages 188–197. CEUR-WS.org.\nWinston Wu, Nidhi Vyas, and David Yarowsky. 2018.\nCreating a translation matrix of the Bible’s names\nacross 591 languages.\nIn Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nPatrick Xia and David Yarowsky. 2017. Deriving con-\nsensus for multi-parallel corpora: an English Bible\nstudy. In Proceedings of the Eighth International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 448–453, Taipei,\nTaiwan. Asian Federation of Natural Language Pro-\ncessing.\n66",
      "page": 66,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "72\nDavid Yarowsky, Grace Ngai, and Richard Wicen-\ntowski. 2001.\nInducing multilingual text analysis\ntools via robust projection across aligned corpora. In\nProceedings of the First International Conference on\nHuman Language Technology Research.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2020.\nEnd-to-end neural word alignment outper-\nforms GIZA++. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1605–1617, Online. Association for\nComputational Linguistics.\nBarret Zoph and Kevin Knight. 2016.\nMulti-source\nneural translation. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 30–34, San Diego, Cali-\nfornia. Association for Computational Linguistics.\n67",
      "page": 67,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Chapter 4\n\nGraph Algorithms for Multiparallel\nWord Alignment",
      "page": 69,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Graph Algorithms for Multiparallel Word Alignment\nAyyoob Imani∗1, Masoud Jalili Sabet∗1, LütﬁKerem ¸Senel1,\nPhilipp Dufter1, François Yvon2, Hinrich Schütze1\n1Center for Information and Language Processing (CIS), LMU Munich, Germany\n2Université Paris-Saclay, CNRS, LISN, France\n{ayyoob, masoud, lksenel, philipp}@cis.lmu.de,\nfrancois.yvon@limsi.fr\nAbstract\nWith the advent of end-to-end deep learning\napproaches in machine translation, interest in\nword alignments initially decreased; however,\nthey have again become a focus of research\nmore recently. Alignments are useful for ty-\npological research, transferring formatting like\nmarkup to translated texts, and can be used in\nthe decoding of machine translation systems.\nAt the same time, massively multilingual pro-\ncessing is becoming an important NLP sce-\nnario, and pretrained language and machine\ntranslation models that are truly multilingual\nare proposed. However, most alignment algo-\nrithms rely on bitexts only and do not leverage\nthe fact that many parallel corpora are multi-\nparallel. In this work, we exploit the multi-\nparallelity of corpora by representing an ini-\ntial set of bilingual alignments as a graph and\nthen predicting additional edges in the graph.\nWe present two graph algorithms for edge pre-\ndiction: one inspired by recommender systems\nand one based on network link prediction. Our\nexperimental results show absolute improve-\nments in F1 of up to 28% over the baseline\nbilingual word aligner in different datasets.\n1\nIntroduction\nWord alignment is a challenging NLP task that\nplays an essential role in statistical machine trans-\nlation and is useful for neural machine translation\n(Alkhouli and Ney, 2017; Alkhouli et al., 2016;\nKoehn et al., 2003). Other applications of word\nalignments include bilingual lexicon induction, an-\nnotation projection, and typological analysis (Shi\net al., 2021; Rasooli et al., 2018; Müller, 2017;\nLewis and Xia, 2008). With the advent of deep\nlearning, interest in word alignment initially de-\ncreased. However, recently a new wave of publica-\ntions has again drawn attention to the task (Jalili Sa-\nbet et al., 2020; Dou and Neubig, 2021; Marchisio\net al., 2021; Wu and Dredze, 2020).\n∗Equal contribution.\nFigure 1: Bilingual alignments of a verse in English,\nGerman, Spanish, and French. Two of the alignment\nedges not found by the bilingual method are German\n“Schritt” to French “pas” and Spanish “largo” to En-\nglish “thousand miles”. By looking at the structure of\nthe entire graph, one can infer the correctness of these\ntwo edges.\nIn this paper we propose MPWA (MultiParal-\nlel Word Alignment), a framework that employs\ngraph algorithms to exploit the information latent\nin a multiparallel corpus to achieve better word\nalignments than aligning pairs of languages in iso-\nlation. Starting from translations of a sentence\nin multiple languages in a multiparallel corpus,\nMPWA generates bilingual word alignments for all\nlanguage pairs using any available bilingual word\naligner. MPWA then improves the quality of word\nalignments for a target language pair by inspect-\ning how they are aligned to other languages. The\ncentral idea is to exploit the graph structure of an\ninitial multiparallel word alignment to improve the\nalignment for a target language pair. To this end,\nMPWA casts the multiparallel word alignment task\nas a link (or edge) prediction problem. We explore\nstandard algorithms for this purpose: Adamic-Adar\nand matrix factorization. While these two graph-\nbased algorithms are quite different and are used in\ndifferent applications, we will show that MPWA ef-\nfectively leverages them for high-performing word\nalignment.\narXiv:2109.06283v1  [cs.CL]  13 Sep 2021\n70",
      "page": 70,
      "type": "text",
      "language": "nl"
    },
    {
      "level": "H2",
      "text": "Link prediction methods are used to predict\nwhether there should be a link between two nodes\nin a graph. They have various applications like\nmovie recommendations, knowledge graph comple-\ntion, and metabolic network reconstruction (Zhang\nand Chen, 2018). We use the Adamic-Adar index\n(Adamic and Adar, 2003); it is a second-order link\nprediction algorithm, i.e., it exploits the informa-\ntion of neighbors that are up to two hops aways\nfrom the starting target nodes (Zhou et al., 2009).\nWe use a second-order algorithm since a set of\naligned words in multiple languages (representing\na concept) tends to establish a clique (Dufter et al.,\n2018). This means that exploring the inﬂuence of\nnodes at a distance of two in the graph provides\ninformative signals while at the same time keeping\nruntime complexity low.\nMatrix factorization is a collaborative ﬁltering\nalgorithm that is most prominently used in rec-\nommender systems where it provides users with\nproduct recommendations based on their interac-\ntions with other products. This method is especially\nuseful if the matrix is sparse (Koren et al., 2009).\nThis is true for our application: Given two transla-\ntions of a sentence with lengths M and N, among\nall possible alignment links (M × N), only a few\n(O(M + N)) are correct. This is partly due to fer-\ntility: words in the source language generally have\nonly a few possible matches in the target language\n(Zhao and Gildea, 2010).\nA multiparallel corpus provides parallel sen-\ntences in more than two languages. This type of\ncorpus facilitates the study of multiple languages\ntogether, which is especially important for research\non low resource languages. As far as we know, out\nof all available multiparallel corpora, the Parallel\nBible Corpus (Mayer and Cysouw, 2014) (PBC)\nprovides the highest language coverage, supporting\n1334 different languages, many of which belong to\ncategories 0 and 1 (Joshi et al., 2020) – that is, they\nare languages for which no language technologies\nare available and that are severely underresourced.\nMPWA has especially strong word alignment\nimprovements for distant language pairs for which\nexisting bilingual word aligners perform poorly.\nMuch work that addresses low resource languages\nrelies on the availabiliy of monolingual corpora.\nComplementarily, MPWA assumes the existence\nof a very small (a few 10,000s of sentences in our\ncase) parallel corpus and then takes advantage of\ninformation from the other languages in the paral-\nlel corpus. This is an alternative approach that is\nespecially important for low resource languages for\nwhich monolingual data often are not available.\nThe PBC corpus does not contain a word align-\nment gold standard. To conduct the comparative\nevaluation of our new method, we port three exist-\ning word alignment gold standards of Bible trans-\nlations to PBC, for the language pairs English-\nFrench, Finnish-Hebrew and Finnish-Greek. We\nalso create artiﬁcial multiparallel datasets for four\nwidely used word alignment datasets using ma-\nchine translation. We evaluate our method with\nall seven datasets. Results demonstrate substantial\nimprovements in all scenarios.\nOur main contributions are:\n1. We propose two graph-based algorithms for\nlink prediction (i.e., the prediction of word\nalignment edges in the alignment graph), one\nbased on second-order link prediction and one\nbased on recommender systems for improving\nword alignment in a multiparallel corpus and\nshow that they perform better than established\nbaselines.\n2. We port and publish three word alignment\ngold standards for the Parallel Bible Corpus.\n3. We show that our method is also applicable,\nusing machine translation, to scenarios where\nmultiparallel data is not available.\n4. We publish our code1 and data.\n2\nRelated Work\nBilingual Word Aligners take different ap-\nproaches. Some are based on statistical analysis,\nlike IBM models (Brown et al., 1993), Giza++ (Och\nand Ney, 2003a), fast-align (Dyer et al., 2013) and\nEﬂomal (Östling and Tiedemann, 2016). Another\nmore recent group, including SimAlign (Jalili Sa-\nbet et al., 2020) and Awesome-align (Dou and Neu-\nbig, 2021), utilizes neural language models. The\nlast group is based on neural machine translation\n(Garg et al., 2019; Zenkel et al., 2020). While neu-\nral models outperform statistical models, for cases\nwhere only a small parallel dataset is available, sta-\ntistical models are still superior. In this paper we\nuse PBC, a corpus with 1334 languages, of which\nonly about two hundred are supported by multilin-\ngual language models like Bert and XLM-R (De-\nvlin et al., 2019; Conneau et al., 2020). MPWA can\n1https://github.com/cisnlp/graph-align\n71",
      "page": 71,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "leverage multiparallelism on top of any bilingual\nword aligner; in this paper, we use Eﬂomal and\nSimAlign.\nMultiparallel corpus alignment. Most work\non word alignment has focused on bilingual cor-\npora.\nTo the best of our knowledge, only one\nmethod speciﬁcally designed for multiparallel cor-\npora was previously proposed: (Östling, 2014).2\nHowever this method is outperformed by a “bipar-\nallel” method by the same author, Eﬂomal (Östling\nand Tiedemann, 2016). We compare with Eﬂomal\nin our experiments.\nCohn and Lapata (2007) make use of multipar-\nallel corpora to obtain more reliable translations\nfrom small datasets. Kumar et al. (2007) show\nthat multiparallel corpora can be of beneﬁt to reach\nbetter performance in phrase-based statistical ma-\nchine translation (SMT). Filali and Bilmes (2005)\npresent a multilingual SMT-based word alignment\nmodel, extending IBM models, based on HMM\nmodels and a two step alignment procedure. Since\nthe goal of this research is to tackle word alignment\ndirectly without considering machine translation,\nthese works are not considered here.\nIn another line of research, Lardilleux and Lep-\nage (2008a) introduce a corpus splitting method to\ncome up with a perfect alignment of multiwords.\nLardilleux and Lepage (2008b), and Lardilleux and\nLepage (2009) suggest to rely only on low fre-\nquency terms for a similar purpose: sub-sentential\nalignment. These methods solve a somewhat differ-\nent problem than what is addressed by us. Other us-\nages of multiparallel corpora are language compar-\nison (Mayer and Cysouw, 2012), typology studies\n(Östling, 2015; Asgari and Schütze, 2017; Imani-\nGooghari et al., 2021) and SMT (Nakov and Ng,\n2012; Bertoldi et al., 2008; Dyer et al., 2013)\nMatrix factorization and link prediction. Ma-\ntrix factorization is a technique that factors, in the\nmost typical case, a matrix into two lower-ranked\nmatrices in which the latent factors of the original\nmatrix are represented. Matrix factorization ap-\nproaches have been widely used in document clus-\ntering (Xu et al., 2003; Shahnaz et al., 2006), topic\nmodeling (Kuang et al., 2015; Choo et al., 2013)\ninformation retrieval (Zamani et al., 2016; Deer-\nwester et al., 1990) and NLP tasks like word sense\ndisambiguation (Schütze, 1998). In 2009, Netﬂix’s\nrecommender system competition revealed that this\n2https://github.com/robertostling/\neflomal\ntechnique effectively works for collaborative ﬁlter-\ning (Koren et al., 2009). Since then it has been a\nstate of the art method in recommender systems.\nLink prediction algorithms are widely used in\ndifferent areas of science since many social, biolog-\nical, and information systems can be described as\nnetworks with nodes and connecting links (Zhou\net al., 2009). Link prediction algorithms compute\nthe likelihood of links based on different heuris-\ntics. One can categorize available methods based\non the maximum number of hops they consider\nin their computations for each node (Zhang and\nChen, 2018). First order algorithms, such as com-\nmon neighbors (CN), only consider one hop neigh-\nborhoods, e.g., (Barabási and Albert, 1999). Sec-\nond order methods consider two hops, e.g., (Zhou\net al., 2009). Finally, higher order methods take\nthe whole network into account for making predic-\ntions (Brin and Page, 1998; Jeh and Widom, 2002;\nRothe and Schütze, 2014). In this paper, we use\na two-hop method since it offers a good tradeoff\nbetween effectiveness and efﬁciency.\n3\nMethods\n3.1\nThe MPWA framework\nWhile a bilingual aligner considers each language\npair separately, MPWA utilizes the synergy be-\ntween all language pairs to improve word align-\nment performance. In Figure 1, Eﬂomal alignments\nof a sentence from PBC in four different languages\nare depicted. Although Eﬂomal has failed to ﬁnd\nthe link between German “Schritt” and French\n“pas”, we can easily ﬁnd this relation by observ-\ning that the four nodes “step”, “Schritt”, “paso”,\nand “pas” are fully connected, except for the edge\nfrom “Schritt” to “pas”. In this case, the inference\namounts to a completion of a clique. However,\nmost cases are not that simple. In the ﬁgure, En-\nglish “thousand miles” is mistakenly aligned to\nSpanish “siempre” although its alignments to Ger-\nman “lange” and French “mille” are correct. We\nwould like to infer that “thousand miles” should be\naligned to “largo”, but in this case creating a fully\nconnected subgraph, i.e., a clique (which would in-\nclude “siempre”), would add many incorrect edges.\nGiven the complexity and error-proneness of ini-\ntial bilingual alignments, inferring an alignment\nbetween two languages from a multiparallel align-\nment in general is a complex problem.\nStarting from a multiparallel corpus, we ﬁrst gen-\nerate bilingual alignments for all language pairs.\n72",
      "page": 72,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "MPWA then employs a prediction algorithm to ﬁnd\nand add new alignment links. In this paper, we\nfocus on two prediction algorithms: non-negative\nmatrix factorization and Adamic-Adar link predic-\ntion.\n3.2\nNon-negative matrix factorization\nNon-negative matrix factorization (NMF) has been\nused in many different applications. After discov-\nery of its effectiveness for collaborative recommen-\ndation (Koren et al., 2009), it was widely accepted\nas a standard method for recommender systems.\nIn a standard recommender system with m users\nand n items, ratings (a number from 1 to 5) from\neach user for the items they have seen so far are\nknown. The aim is to predict the ratings the user\nwould give to unseen items and, based on these\npredictions, recommend new items to the user. As\ndescribed by (Luo et al., 2014), let W = [wu,i] ∈\nRm×n be the matrix of ratings. For NMF to work\nit is essential that the matrix be sparse, thus if a\nuser’s rating for an item is unknown, the corre-\nsponding cell is zeroed. The matrix W is then\ndecomposed into two low-rank non-negative ma-\ntrices, T = [tu,k] ∈Rm×r and V = [vk,i] ∈Rr×n\nsuch that TV ≈W and r ≪min(m, n). r is a\nhyperparameter. By multiplication of these two ma-\ntrices we end up with a reduced matrix W ′ = TV\nin which each zeroed cell wu,i from matrix W is\nreplaced with a value w′\nu,i that represents a predic-\ntion for the rating that user u would give to item i.\nNMF solves the following optimization program:\nargmin\nT,V\n\u0000∥W −TV ∥2\u0001\nsubject to T, V ⩾0\nThis optimization problem can be solved by gra-\ndient descent using the following updates:\ntu,k ←tu,k + ηu,k((WV T )u,k −(TV V T )u,k)\nvk,i ←vk,i + ηk,i((T T W)k,i −(T T TV )k,i)\nIn this equation, η is the learning rate. To guar-\nantee non-negativity, it is deﬁned as:\nηu,k =\ntu,k\n(TV V T )u,k\n, ηk,i =\nvk,i\n(T T TV )k,i\nNote that the objective function only takes ac-\ncount of non-zero cells. Luo et al. (2014) propose\nan approach that takes advantage of the sparseness\nof the matrix for faster computation. In addition,\nI\ncan\nsee\nich\nkann\nes\nsehen\nje\nvois\nI\n5\n1\n5\n1\n5\n1\ncan\n5\n1\n5\n1\nsee\n1 1\n5\n1\n5\n1\n5\nich\n5 1\n5\n1\n5\n1\nkann\n1 5\n1\n5\nes\n5\n1\nsehen\n1\n5\n1\n5\n1\nje\n5 1\n5\n1\n5\n1\nvois\n1\n5\n1\n5\n1\n5\nFigure 2: An example of how the original matrix is\nﬁlled for a sentence in three languages. Zero entries\nare left blank for readability.\nTikhonov regularization is integrated to improve\nprecision, recall, and convergence rate.\nWe use the implementaion of NMF provided by\nthe Surprise3 library, with default hyperparameters\n(r = 15, n_epochs = 50).\n3.2.1\nNMF in MPWA framework\nWe create a separate matrix W for each sentence\nin the multiparallel corpus. Tokens in the sentence\nplay the role of both users and items, i.e., we con-\nsider each token both as a row and as a column.\nFigure 2 shows an example of a sentence in a toy\nEnglish-German-French multiparallel corpus. If\ntwo tokens are aligned using the bilingual aligner,\nwe ﬁll the corresponding cell with the highest rat-\ning (5). To give a few negative examples to the\nalgorithm, if a token x from language L1 is aligned\nto token y in language L2, we pick another ran-\ndom token z from L2 and ﬁll the corresponding\ncell of x to z with the lowest rating (1). We zero\nout all other cells. Next we apply the matrix fac-\ntorization algorithm to this matrix and then com-\npute the reduced matrix W ′ from the factors. Now\nwe grab the predicted alignment scores between\nsource and target languages from W ′. To extract\nnew alignment edges we apply the Argmax algo-\nrithm (Jalili Sabet et al., 2020). Argmax creates an\nalignment edge between word wi from language\nL1 and word wj from language L2 if among all\nwords from L2, wi has the highest alignment score\nwith wj, and among all words from L1, wj has the\nhighest alignment score with wi.\n3.3\nLink Prediction\nA multiparallel sentence annotated with bilingual\nword alignments can be considered to be a graph\nwith words from all translations as nodes and the\n3http://surpriselib.com/\n73",
      "page": 73,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "word alignments as edges.\nLink prediction al-\ngorithms such as Common Neighbors (CN) and\nAdamic-Adar (AdAd) estimate the likelihood of\nhaving an edge between two nodes x and y in the\ngraph based on the similarity of their neighbor-\nhoods. The CN index weights all common neigh-\nbors equally. In contrast, AdAd gives higher weight\nto neighbors with low degrees because sharing a\nneighbor that in turn has few neighbors is more\nsigniﬁcant. Therefore, we use the AdAd index. It\nis deﬁned as:\nAdAdx,y =\nX\nz∈Γ(x)∩Γ(y)\n1\nlog |Γ(z)|\n(1)\nwhere Γ(x) is the neighborhood of x.\nIf we use a word aligner that produces a score for\neach alignment edge, we can use Weighted Adamic-\nAdar (Lü and Zhou, 2010):\nWAdAdx,y =\nX\nz∈Γ(x)∩Γ(y)\nw(x, z) + w(z, y)\nlog(1 + S(z))\n(2)\nwhere w(x, z) is the similarity score of x and z gen-\nerated by the aligner and S(x) = P\nz∈Γ(x) w(x, z).\nFor embedding-based aligners we use embedding\nsimilarity as the score w(x, z). If an aligner does\nnot provide scores, we set all weights to 1.0.\nGiven a scored word alignment, we create a mul-\ntilingual word alignment matrix W for each sen-\ntence as shown in Figure 2. Each cell contains 0\nor 1 for Adamic-Adar or the alignment score for\nWeighted Adamic-Adar. We again apply Argmax\nto extract new alignment edges and then add them\nto the original alignment.\n4\nExperimental setup\n4.1\nPBC\nThe PBC corpus (Mayer and Cysouw, 2014) con-\ntains 1758 editions of the Bible in 1334 languages.\nThe editions are aligned at the verse level and to-\nkenized. A verse can contain more than one sen-\ntence, but we treat it as one unit in the parallel\ncorpus since a true sentence level alignment is not\navailable. There are some errors in tokenization\n(e.g., for Tibetan, Khmer and Chinese), but the\noverall quality of the corpus is good. For the ma-\njority of languages one edition is provided, while a\nfew languages (in particular, English, French and\nGerman) contain up to dozens of editions. The\nverse coverage also differs from language to lan-\nguage. Some languages have translations of both\nNew Testament and Hebrew Bible while others\ncontain only one. Table 2 gives corpus statistics.\n4.2\nWord alignment datasets\nPBC does not provide gold word alignments. To\nevaluate MPWA, we port two word alignment gold\ndatasets of the Bible to PBC: Blinker (Melamed,\n1998) and the recently published HELFI (Yli-Jyrä\net al., 2020). We further experiment with bilin-\ngual datasets, using Machine Translation (MT) to\ncreate multiparallel corpora. Table 1 gives dataset\nstatistics.\nThe HELFI dataset consists of the Greek New\nTestament, the Hebrew Bible and translations of\nboth into Finnish. In addition, morpheme align-\nments are provided for Finnish-Greek and Finnish-\nHebrew. We reformatted this dataset to the format\nused by PBC. In more detail, we added three new\neditions for the three languages to PBC. We iden-\ntiﬁed the PBC verse identiﬁer for each verse of\nHELFI to ensure proper verse alignment of these\nthree new editions. The Finnish-Hebrew dataset\nhas 22,291 verses and the Finnish-Greek dataset\n7,909. We split these datasets 80/10/10 into train,\nvalidation and test.\nThe Blinker Bible dataset provides word level\nalignments of 250 Bible verses between English\nand French. The French side of this dataset matches\nwith the edition Louis Segond 1910 in PBC. How-\never, the tokenizations (Blinker vs PBC) are differ-\nent. We therefore create a mapping of the tokens\nusing character n-gram matching. For English, we\ncreated and added a new edition to PBC.\nMT datasets. To more broadly evaluate MPWA,\nwe also create multiparallel datasets for four non-\nBible word alignment gold standards; these are\nlisted in Table 1 as “Non-Bible” corpora. For these\ngold standards, we translate from English to all lan-\nguages available in Google Translate, using their\nAPI.4 For the added languages, we create align-\nments for the gold standard sentences using SimA-\nlign.\n4.3\nInitial word alignments\nWe compare with two state of the art models, one\nstatistical, one neural. Eﬂomal (Östling and Tiede-\nmann, 2016) is a Bayesian statistical word aligner\nusing Markov Chain Monte Carlo inference. SimA-\nlign (Jalili Sabet et al., 2020) obtains word align-\n4https://cloud.google.com/translate/\ndocs/basic/translating-text\n74",
      "page": 74,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Language Pair\nName\n# Sentences (train/valid./test)\nBible\nFIN-HEB\nHELFI (Yli-Jyrä et al., 2020)\n22291 (17832/2229/2230)\nFIN-GRC\nHELFI (Yli-Jyrä et al., 2020)\n7909 (6327/791/791)\nENG-FRA\nBLINKER (Melamed, 1998)\n250\nNon-\nBible\nENG-DEU\nEuroParl-baseda\n508\nENG-FAS\n(Tavakoli and Faili, 2014)\n400\nENG-HIN\nWPT2005b\n90\nENG-RON\nWPT2005b\n203\na www-i6.informatik.rwth-aachen.de/goldAlignment/\nb http://web.eecs.umich.edu/~mihalcea/wpt05/\nTable 1: Overview of datasets. We use ISO 639-3 language codes. # Sentences: the number of available verses\n(i.e., sentences). FIN-HEB and FIN-GRC datasets split into train, validation and test.\n# editions\n1758\n# languages\n1334\n# verses\n20,470,892\n# verses / # editions\n11,520\n# tokens / # verses\n28.6\nTable 2: PBC corpus statistics\nments from multilingual pretrained language mod-\nels with no need for parallel data. For the sym-\nmetrization of Eﬂomal, we use grow-diag-ﬁnal-and\n(GDFA) and intersection, and for SimAlign we use\nArgmax and Itermax. Intersection and Argmax gen-\nerate accurate alignments while GDFA and Itermax\nare less accurate but have better coverage (Jalili Sa-\nbet et al., 2020).\nWe evaluate on a target language pair parallel\nsentence as follows: First, we create the matrix\n(Figure 2) for this sentence for all languages in the\nmultiparallel corpus. Then we run link prediction\non the matrix – this accumulates evidence from a\nset of languages in the multiparallel corpus. Finally,\nwe take the predictions for the target language pair\nand add them to the original (bilingual) alignment.\nNMF works best if it starts with high-accuracy\n(i.e., non-noisy) bilingual alignments – errors can\nresult in incorrectly predicted alignment edges. We\ntherefore use SimAlign Argmax and Eﬂomal In-\ntersection, two word alignment methods with high\nprecision, to create the initial alignments that are\nthen fed into NMF. We then add the predictions to\nany desired original alignments; e.g., NMF (GDFA)\nuses Eﬂomal Intersection as the initial alignments\nand adds the predictions to Eﬂomal GDFA. See the\nAppendix for more details.\nSimAlign offers high quality word alignments\nfor well-represented languages from pretrained lan-\nguage models; however, our experiments show\nthat its performance is far behind Eﬂomal for less\nwell resourced languages like Biblical Hebrew and\nKoine Greek. Also, Eﬂomal is a better match for\nMPWA because it can provide word alignments\nfor all languages available in a multiparallel cor-\npus. In contrast, SimAlign is limited to languages\nsupported by pretrained multilingual embeddings.\nTo feed Eﬂomal with enough training data for a\ntarget language pair, we use all available data from\ndifferent translations of the language pair. For ex-\nample if one language has two translations and the\nother one has three translations, Eﬂomal’s training\ndata will contain six aligned translation pairs for\nthese two languages.\nWe use the standard evaluation measures for\nword alignment: precision, recall, F1 and Align-\nment Error Rate (AER) (Och and Ney, 2003b;\nÖstling and Tiedemann, 2016; Jalili Sabet et al.,\n2020).\n5\nResults\n5.1\nMultiparallel corpus results\nWe perform the ﬁrst set of experiments on the\nBlinker Bible and the HELFI gold standards in\nthe PBC. The baseline results are calculated on the\noriginal language pairs. MPWA can be applied\nto both Eﬂomal and SimAlign alignments. Since\nthe default version of SimAlign can only generate\nalignments for the 84 languages that multilingual\nBERT supports,5 for a better comparison, we use\nthe same set of languages in the alignment graph\nfor both SimAlign and Eﬂomal.\nTable 3 shows the results for our methods ap-\nplied on SimAlign and Eﬂomal baselines.6 AdAd,\nNMF and WAdAd substantially improve the per-\nformance for all language pairs. SimAlign gener-\nates high-quality alignments for the English-French\ndataset, but cannot properly align underresourced\nlanguages like Biblical Hebrew and Koine Greek.\n5https://github.com/google-research/\nbert/blob/master/multilingual.md\n6We only consider SimAlign IterMax, not SimAlign\nArgMax, because IterMax performed better throughout.\n75",
      "page": 75,
      "type": "text",
      "language": "hi"
    },
    {
      "level": "H2",
      "text": "FIN-HEB\nFIN-GRC\nENG-FRA\nMethod\nPrec.\nRec.\nF1\nAER\nPrec.\nRec.\nF1\nAER\nPrec.\nRec.\nF1\nAER\nBaseline\nEﬂomal (intersection)\n0.818\n0.269\n0.405\n0.595\n0.897\n0.506\n0.647\n0.353\n0.971\n0.521\n0.678\n0.261\nEﬂomal (GDFA)\n0.508\n0.448\n0.476\n0.524\n0.733\n0.671\n0.701\n0.300\n0.856\n0.710\n0.776\n0.221\nSimAlign\n0.190\n0.113\n0.142\n0.858\n0.366\n0.265\n0.307\n0.693\n0.886\n0.692\n0.777\n0.221\nInit SimAlign\nAdAd\n0.199\n0.127\n0.155\n0.845\n0.402\n0.289\n0.336\n0.664\n0.878\n0.731\n0.798\n0.200\nWAdAd\n0.186\n0.165\n0.175\n0.825\n0.353\n0.350\n0.351\n0.649\n0.856\n0.752\n0.801\n0.197\nNMF\n0.122\n0.100\n0.110\n0.890\n0.396\n0.337\n0.364\n0.636\n0.835\n0.700\n0.762\n0.236\nInit Eﬂomal\nWAdAd (intersection)\n0.781\n0.612\n0.686\n0.314\n0.849\n0.696\n0.765\n0.235\n0.938\n0.689\n0.794\n0.203\nNMF (intersection)\n0.78\n0.576\n0.663\n0.337\n0.864\n0.669\n0.754\n0.248\n0.948\n0.624\n0.753\n0.245\nWAdAd (GDFA)\n0.546\n0.693\n0.611\n0.389\n0.707\n0.783\n0.743\n0.257\n0.831\n0.796\n0.813\n0.186\nNMF (GDFA)\n0.548\n0.646\n0.593\n0.407\n0.72\n0.759\n0.739\n0.261\n0.844\n0.767\n0.804\n0.195\nTable 3: Comparison of results from different methods on PBC. The best results in each column are in bold. The\nthree methods exploiting multiparallelism (AdAd, WAdAd, NMF) generally outperform the baselines on F1 and\nAER.\nIn such cases, MPWA uses the accumulated infor-\nmation from all other language pairs in the graph\nto improve the performance. When starting with\nthe SimAlign alignment (“Init SimAlign”), both\nmethods improve the result for both FIN-HEB and\nFIN-GRC.\nEﬂomal generates better alignments for FIN-\nHEB and FIN-GRC. This means that Eﬂomal also\ngenerates better alignments between FIN, HEB and\nGRC on the one hand and the other languages in\nthe graph on the other hand and therefore can pro-\nvide a better signal for MPWA. The improvements\nof our models applied on Eﬂomal are higher than\nthe ones applied on SimAlign for these language\npairs.\nWhen changing the initial alignments from Eﬂo-\nmal (intersection) to Eﬂomal (GDFA), we see dif-\nferent behaviors: GDFA improves the results for\nBlinker while it does not help for HELFI. We be-\nlieve this is caused by the different ways the two\ndatasets were annotated. In Blinker, many phrases\nare “exhaustively” aligned: if a phrase DE in En-\nglish is aligned with FG in French then all four\nalignment edges (D-F, D-G, E-F, E-G) are given as\ngold edges.7\nSo Blinker contains a lot of many-to-many links.\nIn contrast, most alignments are one-to-one in\nHELFI. This partially explains why intersection\nas initial alignment works much better for HELFI\nthan GDFA and vice versa for Blinker.\nIn summary, compared to the baselines, we see\nvery large improvements through exploiting mul-\ntiparallelism for one type of alignment methodol-\nogy (HELFI, F1 improved by up to 20% for FIN-\n7For example, the alignment of the phrases “trembled vio-\nlently” and “fut saisi d’und grande, d’une violente émotion”\nconsists of 2 · 8 gold edges.\nHEB) and improvements of up to 3.5% for the other\n(ENG-FRA).\n5.2\nMT dataset results\nWe perform the second set of experiments on gold\nstandard alignments for language pairs that are not\npart of a multiparallel corpus such as PBC. To this\nend, we create artiﬁcial multiparallel corpora by\ntranslating the English side to all languages avail-\nable in the Google Translate API. The main goal\nis to give broader evidence for the effectiveness of\nour method, beyond the specialized domain of the\nBible.\nEﬂomal’s alignments generally have good qual-\nity. However, they get worse when less parallel\ndata is available (Jalili Sabet et al., 2020). Since\nthe size of the multiparallel corpus created by ma-\nchine translation is rather small, we use SimAlign\nfor generating initial alignments. SimAlign has\nbeen shown to have good performance even for\nvery small parallel corpora; in fact, it does not need\nany parallel data at all.\nTable 4 shows the results of the experiments.\nBoth NMF and WAdAd, improve the performance\nof the baseline by using the alignment graph. Im-\nprovements range from 0.8% (ENG-DEU) to 3.3%\n(ENG-HIN). This again demonstrates the utility of\nexploiting multiparallelism for word alignment. It\nis worth mentioning that in this case the translations\nare noisy since they were automatically generated.\nBut even with these noisy translations (instead of a\n“true” multiparallel corpus), our models effectively\nleverage multiparallelism.\n76",
      "page": 76,
      "type": "text",
      "language": "ur"
    },
    {
      "level": "H2",
      "text": "ENG-PES\nENG-HIN\nENG-RON\nENG-DEU\nMethod\nPrec.\nRec.\nF1\nAER\nPrec.\nRec.\nF1\nAER\nPrec.\nRec.\nF1\nAER\nPrec.\nRec.\nF1\nAER\nBaseline\nSimAlign\n0.756\n0.645\n0.696\n0.304\n0.709\n0.493\n0.582\n0.418\n0.807\n0.663\n0.728\n0.272\n0.829\n0.795\n0.812\n0.188\nInit SimAlign\nAdAd\n0.751\n0.700\n0.725\n0.276\n0.693\n0.544\n0.610\n0.390\n0.799\n0.696\n0.744\n0.256\n0.818\n0.823\n0.820\n0.179\nWAdAd\n0.705\n0.740\n0.722\n0.278\n0.643\n0.574\n0.607\n0.394\n0.725\n0.717\n0.721\n0.279\n0.749\n0.844\n0.794\n0.207\nNMF\n0.734\n0.698\n0.716\n0.284\n0.684\n0.559\n0.615\n0.385\n0.780\n0.696\n0.736\n0.265\n0.804\n0.827\n0.815\n0.185\nTable 4: Results with gold standards translated into other languages using machine translation. The best results in\neach column are in bold. The three methods exploiting multiparallelism (AdAd, WAdAd, NMF) outperform the\nbaselines on F1 and AER.\nFigure 3: F1 of MPWA for the target language pair\nFIN-HEB as a function of the number of additional lan-\nguages. There is a clear rise initially. The curve ﬂattens\naround 75.\n5.3\nAnalysis\n5.3.1\nEffect of number of languages\nThe effect of adding more languages to the align-\nment graph is depicted in Figure 3. This plot shows\nF1 for FIN-HEB. As seen in the ﬁgure, the slope\nis pretty steep up to 25 languages, but even adding\njust three languages can still improve the results.\nFor 75 languages we have almost reached the peak\nand after 100, adding more languages is not im-\nproving the results. This means that MPWA can\nalso be helpful for corpora with a smaller number\nof languages – a massively parallel corpus with\nthousands of languages is not required.\n5.3.2\nSize of the training set\nTo assess the effect of dataset size on the perfor-\nmance of MPWA, we perform a set of experiments\non ENG-FRA and NMF. To this end, we take the\ntraining data for ENG-FRA and train models on\nsubsets of it. The training data consists of 6.4M\nsentence pairs – this number is so high because we\nuse the crossproduct of all editions in English and\nFrench (§4.3).\nThe results are shown in Figure 4. Eﬂomal per-\nformance increases with training set size initially\nFigure 4: Word alignment F1 on ENG-FRA as a func-\ntion of the size of the training set, ranging from 30K to\n6.4M training sentence pairs\nand is then less predictable. NMF consistently im-\nproves the scores.\n5.3.3\nEffect of task difﬁculty\nTable 3 shows large improvements for all datasets,\nand especially for FIN-HEB and FIN-GRC. To get\nmore insight into the reasons for this improvement,\nwe stratify FIN-HEB verses by dividing the interval\n[0, 1] of initial F1 performance of Eﬂomal into ﬁve\nequal-sized subintervals: [0, 0.2], ..., (0.8, 1].\nFigure 5 indicates that MPWA is most effective\nfor difﬁcult verses, but brings little improvement\nfor easy verses. We attribute this to two reasons:\n1. An easy to align verse in a language pair can-\nnot use help from other languages since it al-\nready has good alignment links (although the\nlanguage pair would still be of beneﬁt in im-\nproving alignments for the sentence in other\nlanguages). So there is no way for MPWA to\nget better results in this scenario.\n2. MPWA only tries to get better results by\nadding new alignments, and as an easy verse\nalready has many alignment links, adding new\nlinks almost inevitably results in a drop in pre-\n77",
      "page": 77,
      "type": "text",
      "language": "hi"
    },
    {
      "level": "H2",
      "text": "Figure 5: How helpful is MPWA for different difﬁculty\nlevels? For this analysis, FIN-HEB verses were strat-\niﬁed according to Eﬂomal F1 (x-axis).\nWe see that\nMPWA brings the largest improvements for difﬁcult\nsentences.\nENG-FRA\nFIN-HEB\nFIN-GRC\nLang.\n∆\nLang.\n∆\nLang.\n∆\nSPA\n+2.0%\nTGL\n+17.7%\nLAT\n+7.5%\nITA\n+1.9%\nFRY,ELL\n+17.3%\nELL\n+6.6%\nDEU\n+1.8%\nSWE\n+17.3%\nENG\n+6.1%\nNLD\n+1.4%\nNLD\n+16.8%\nFRY\n+5.8%\nAFR\n+1.3%\nYOR\n+14.2%\nBEL\n+5.7%\nTable 5: The ﬁve most helpful languages and WAdAd’s\nabsolute improvements in F1 over the initial bilingual\naligner SimAlign. For example, MPWA improves the\nbilingual FIN-GRC alignment by 7.5% if applied to\nthe trilingual corpus FIN-GRC-LAT, i.e., Latin can be\nviewed as the best bridge between Finnish and Greek.\ncision. It may also be possible to inspect and\nprune existing Eﬂomal links using MPWA to\nget better results in this scenario.\n5.3.4\nMost helpful languages\nFor each dataset, the ﬁve most helpful languages\nwith their corresponding improvements are listed\nin Table 5. We hypothesize that these languages\nserve to bridge the typological gap between the two\ntarget languages. Table 5 suggests one should be\nable to achieve excellent results – even for a corpus\nwith a small number of languages – if we utilize an\nintelligent selection of languages.\n5.3.5\nMultiple translations in two languages\nThere are some datasets that contain few languages,\nbut many translations of a text in one language.\nPBC is one example of such a dataset, many liter-\nary works another (e.g., many novels have many\ntranslations in English). To see whether MPWA\ncan also help in this scenario, we picked all avail-\nable 49 English and French editions from PBC and\nused them as additional translations for the ENG-\nFRA dataset. The outcome of this experiment is\nPrec.\nRec.\nF1\nAER\nEﬂomal (intersection)\n0.971\n0.521\n0.678\n0.319\nEﬂomal (GDFA)\n0.856\n0.710\n0.776\n0.221\nNMF (target languages)\n0.830\n0.749\n0.787\n0.213\nNMF (other languages)\n0.837\n0.753\n0.793\n0.205\nTable 6: F1 for ENG-FRA. MPWA can exploit a mul-\ntiparallel corpus with languages different from the tar-\nget languages (“other languages”) better than one that\ncontains additional translations in the target languages\n(“target languages”).\ncompared with the outcome of the same setup, but\nwith translations from languages other than French\nand English in Table 6. From this table we can\nconclude that translations from the target language\npair can also assist, but not as much as translations\nfrom other languages.\n6\nConclusion and Future Work\nWe presented MPWA, a framework for leverag-\ning multiparallel corpora for word alignment. We\nused two prediction methods, one based on recom-\nmender systems and one based on link prediction\nalgorithms. By adding new alignment edges to the\noutput of bilingual aligners, both methods show\nlarge improvements over the bilingual baselines,\nwith absolute improvements of F1 of up to 20%.\nWe have also ported Blinker and HELFI word align-\nment gold standards to the Parallel Bible Corpus\nin the hope that this will help foster more work on\nexploiting multiparallel corproa.\nFuture work. In this paper, we have mainly fo-\ncused on adding new alignment edges to baseline\nword alignments based on properties of the mul-\ntiparallel alignment graph. This increases recall,\nbut can harm precision. In future work, we plan to\nexpand on the possibility of deleting edges based\non evidence from the multiparallel alignment graph\n(cf. 5.3.3), thereby potentially improving both pre-\ncision and recall.\nAcknowledgments\nThis work was supported by the European Research\nCouncil (ERC, Grant No. 740516) and the Ger-\nman Federal Ministry of Education and Research\n(BMBF, Grant No. 01IS18036A). The fourth author\nwas supported by the Bavarian research institute for\ndigital transformation (bidt) through their fellow-\nship program. We thank the anonymous reviewers\nfor their constructive comments.\n78",
      "page": 78,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "References\nLada A Adamic and Eytan Adar. 2003. Friends and\nneighbors on the web. Social networks, 25(3):211–\n230.\nTamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Pe-\nter, Mohammed Hethnawi, Andreas Guta, and Her-\nmann Ney. 2016. Alignment-based neural machine\ntranslation. In Proceedings of the First Conference\non Machine Translation: Volume 1, Research Pa-\npers, pages 54–65, Berlin, Germany. Association for\nComputational Linguistics.\nTamer Alkhouli and Hermann Ney. 2017.\nBiasing\nattention-based recurrent neural networks using ex-\nternal alignment information. In Proceedings of the\nSecond Conference on Machine Translation, pages\n108–117, Copenhagen, Denmark. Association for\nComputational Linguistics.\nEhsaneddin Asgari and Hinrich Schütze. 2017. Past,\npresent, future: A computational investigation of the\ntypology of tense in 1000 languages. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 113–124,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nAlbert-László Barabási and Réka Albert. 1999. Emer-\ngence of scaling in random networks.\nscience,\n286(5439):509–512.\nNicola Bertoldi, Madalina Barbaiani, Marcello Fed-\nerico, and Roldano Cattoni. 2008. Phrase-based sta-\ntistical machine translation with pivot languages. In\nInternational Workshop on Spoken Language Trans-\nlation (IWSLT) 2008.\nSergey Brin and Lawrence Page. 1998. The anatomy of\na large-scale hypertextual web search engine. Com-\nputer networks and ISDN systems, 30(1-7):107–117.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2).\nJaegul Choo, Changhyun Lee, Chandan K Reddy, and\nHaesun Park. 2013.\nUtopian: User-driven topic\nmodeling based on interactive nonnegative matrix\nfactorization.\nIEEE transactions on visualization\nand computer graphics, 19(12):1992–2001.\nTrevor Cohn and Mirella Lapata. 2007. Machine trans-\nlation by triangulation:\nMaking effective use of\nmulti-parallel corpora. In Proceedings of the 45th\nAnnual Meeting of the Association of Computational\nLinguistics, pages 728–735, Prague, Czech Repub-\nlic. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nScott Deerwester, Susan T Dumais, George W Fur-\nnas, Thomas K Landauer, and Richard Harshman.\n1990. Indexing by latent semantic analysis. Jour-\nnal of the American society for information science,\n41(6):391–407.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZi-Yi Dou and Graham Neubig. 2021. Word alignment\nby ﬁne-tuning embeddings on parallel corpora. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 2112–2128, Online.\nAssociation for Computational Linguistics.\nPhilipp Dufter, Mengjie Zhao, Martin Schmitt, Alexan-\nder Fraser, and Hinrich Schütze. 2018. Embedding\nlearning through multilingual concept induction. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1520–1530, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013.\nA simple, fast, and effective reparameter-\nization of IBM model 2.\nIn Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nKarim Filali and Jeff Bilmes. 2005. Leveraging multi-\nple languages to improve statistical MT word align-\nments.\nIn IEEE Workshop on Automatic Speech\nRecognition and Understanding, 2005., pages 92–\n97. IEEE.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAyyoob ImaniGooghari, Masoud Jalili Sabet, Philipp\nDufter, Michael Cysou, and Hinrich Schütze. 2021.\nParCourE: A parallel corpus explorer for a massively\nmultilingual corpus. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\n79",
      "page": 79,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Linguistics and the 11th International Joint Con-\nference on Natural Language Processing: System\nDemonstrations, pages 63–72, Online. Association\nfor Computational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, François Yvon,\nand Hinrich Schütze. 2020. SimAlign: High qual-\nity word alignments without parallel training data us-\ning static and contextualized embeddings. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 1627–1643, Online. As-\nsociation for Computational Linguistics.\nGlen Jeh and Jennifer Widom. 2002. Simrank: a mea-\nsure of structural-context similarity.\nIn Proceed-\nings of the eighth ACM SIGKDD international con-\nference on Knowledge discovery and data mining,\npages 538–543.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld.\nIn Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation. In Proceedings\nof the 2003 Human Language Technology Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 127–133.\nYehuda Koren, Robert Bell, and Chris Volinsky. 2009.\nMatrix factorization techniques for recommender\nsystems. Computer, 42(8):30–37.\nDa Kuang, Jaegul Choo, and Haesun Park. 2015. Non-\nnegative matrix factorization for interactive topic\nmodeling and document clustering. In Partitional\nClustering Algorithms, pages 215–243. Springer.\nShankar Kumar, Franz J. Och, and Wolfgang Macherey.\n2007. Improving word alignment with bridge lan-\nguages. In Proceedings of the 2007 Joint Confer-\nence on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning (EMNLP-CoNLL), pages 42–50, Prague,\nCzech Republic. Association for Computational Lin-\nguistics.\nAdrien Lardilleux and Yves Lepage. 2008a. A truly\nmultilingual, high coverage, accurate, yet simple,\nsub-sentential alignment method.\nIn The 8th con-\nference of the Association for Machine Transla-\ntion in the Americas (AMTA 2008), pages 125–132,\nWaikiki, Honolulu, United States.\nAdrien Lardilleux and Yves Lepage. 2008b. Multilin-\ngual alignments by monolingual string differences.\nIn Coling 2008: Companion volume: Posters, pages\n55–58, Manchester, UK. Coling 2008 Organizing\nCommittee.\nAdrien Lardilleux and Yves Lepage. 2009. Sampling-\nbased multilingual alignment. In Proceedings of the\nInternational Conference RANLP-2009, pages 214–\n218, Borovets, Bulgaria. Association for Computa-\ntional Linguistics.\nWilliam D. Lewis and Fei Xia. 2008. Automatically\nidentifying computationally relevant typological fea-\ntures.\nIn Proceedings of the Third International\nJoint Conference on Natural Language Processing:\nVolume-II.\nLinyuan Lü and Tao Zhou. 2010. Link prediction in\nweighted networks: The role of weak ties. EPL (Eu-\nrophysics Letters), 89(1):18001.\nXin Luo, Mengchu Zhou, Yunni Xia, and Qing-\nsheng Zhu. 2014. An efﬁcient non-negative matrix-\nfactorization-based approach to collaborative ﬁlter-\ning for recommender systems. IEEE Transactions\non Industrial Informatics, 10(2):1273–1284.\nKelly Marchisio, Conghao Xiong, and Philipp Koehn.\n2021.\nEmbedding-enhanced GIZA++: Improving\nalignment in low-and high-resource scenarios us-\ning embedding space geometry.\narXiv preprint\narXiv:2104.08721.\nThomas Mayer and Michael Cysouw. 2012. Language\ncomparison through sparse multilingual word align-\nment. In Proceedings of the EACL 2012 Joint Work-\nshop of LINGVIS & UNCLH, pages 54–62, Avignon,\nFrance. Association for Computational Linguistics.\nThomas Mayer and Michael Cysouw. 2014. Creating\na massively parallel Bible corpus. In Proceedings\nof the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 3158–\n3163, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nI. Dan Melamed. 1998. Manual annotation of transla-\ntional equivalence: The blinker project. CoRR, cmp-\nlg/9805005.\nMathias Müller. 2017.\nTreatment of markup in sta-\ntistical machine translation. In Proceedings of the\nThird Workshop on Discourse in Machine Transla-\ntion, pages 36–46, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nPreslav Nakov and Hwee Tou Ng. 2012. Improving sta-\ntistical machine translation for a resource-poor lan-\nguage using related resource-rich languages. Jour-\nnal of Artiﬁcial Intelligence Research, 44:179–222.\nFranz Josef Och and Hermann Ney. 2003a. A system-\natic comparison of various statistical alignment mod-\nels. Computational Linguistics, 29(1).\nFranz Josef Och and Hermann Ney. 2003b. A system-\natic comparison of various statistical alignment mod-\nels. Computational Linguistics, 29(1):19–51.\n80",
      "page": 80,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Robert Östling. 2014.\nBayesian word alignment for\nmassively parallel texts. In Proceedings of the 14th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, EACL 2014,\nApril 26-30, 2014, Gothenburg, Sweden, pages 123–\n127. The Association for Computer Linguistics.\nRobert Östling. 2015.\nWord order typology through\nmultilingual word alignment. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2:\nShort Papers), pages 205–211, Beijing,\nChina. Association for Computational Linguistics.\nRobert Östling and Jörg Tiedemann. 2016. Efﬁcient\nword alignment with Markov Chain Monte Carlo.\nThe Prague Bulletin of Mathematical Linguistics,\n106(1).\nMohammad Sadegh Rasooli, Noura Farra, Axinia\nRadeva, Tao Yu, and Kathleen McKeown. 2018.\nCross-lingual sentiment transfer with limited re-\nsources. Machine Translation, 32(1):143–165.\nSascha Rothe and Hinrich Schütze. 2014. CoSimRank:\nA ﬂexible & efﬁcient graph-theoretic similarity mea-\nsure. In Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1392–1402, Baltimore,\nMaryland. Association for Computational Linguis-\ntics.\nHinrich Schütze. 1998. Automatic word sense discrim-\nination. Computational Linguistics, 24(1):97–123.\nFarial Shahnaz, Michael W Berry, V Paul Pauca, and\nRobert J Plemmons. 2006. Document clustering us-\ning nonnegative matrix factorization.\nInformation\nProcessing & Management, 42(2):373–386.\nHaoyue Shi, Luke Zettlemoyer, and Sida I. Wang. 2021.\nBilingual lexicon induction via unsupervised bitext\nconstruction and word alignment. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 813–826, Online.\nAssociation for Computational Linguistics.\nLeila Tavakoli and Heshaam Faili. 2014. Phrase align-\nments in parallel corpus using bootstrapping ap-\nproach.\nInternational Journal of Information &\nCommunication Technology Research, 6(3).\nShijie Wu and Mark Dredze. 2020. Do explicit align-\nments robustly improve multilingual encoders?\nIn\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4471–4482, Online. Association for Computa-\ntional Linguistics.\nWei Xu, Xin Liu, and Yihong Gong. 2003.\nDocu-\nment clustering based on non-negative matrix factor-\nization. In Proceedings of the 26th annual interna-\ntional ACM SIGIR conference on Research and de-\nvelopment in informaion retrieval, pages 267–273.\nAnssi Yli-Jyrä, Josi Purhonen, Matti Liljeqvist, Arto\nAntturi, Pekka Nieminen, Kari M. Räntilä, and Valt-\nter Luoto. 2020. HELFI: a Hebrew-Greek-Finnish\nparallel Bible corpus with cross-lingual morpheme\nalignment.\nIn Proceedings of the 12th Language\nResources and Evaluation Conference, pages 4229–\n4236, Marseille, France. European Language Re-\nsources Association.\nHamed Zamani, Javid Dadashkarimi, Azadeh Shakery,\nand W Bruce Croft. 2016. Pseudo-relevance feed-\nback based on matrix factorization. In Proceedings\nof the 25th ACM international on conference on in-\nformation and knowledge management, pages 1483–\n1492.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2020.\nEnd-to-end neural word alignment outper-\nforms GIZA++. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1605–1617, Online. Association for\nComputational Linguistics.\nMuhan Zhang and Yixin Chen. 2018. Link prediction\nbased on graph neural networks. Advances in Neu-\nral Information Processing Systems, 31:5165–5175.\nShaojun Zhao and Daniel Gildea. 2010.\nA fast fer-\ntility hidden Markov model for word alignment us-\ning MCMC. In Proceedings of the 2010 Conference\non Empirical Methods in Natural Language Process-\ning, pages 596–605, Cambridge, MA. Association\nfor Computational Linguistics.\nTao Zhou, Linyuan Lü, and Yi-Cheng Zhang. 2009.\nPredicting missing links via local information. The\nEuropean Physical Journal B, 71(4):623–630.\nA\nPipeline Details\nThere are several elements of the MPWA pipeline\nthat can be conﬁgured by the user, e.g., depending\non whether precision or recall are more important\nfor an application. Here we show in Figures 6 and\n7 the two pipeline conﬁgurations we used for the\nresults in the paper.\nFigure 6: The pipeline for NMF alignments\n81",
      "page": 81,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Initial Alignments\n\n(W)AdAd\n(GDFA)\n(W)AdAd\n(intersection)\n\nFigure 7: The pipeline for AdAd and WAdAd align-\nments\n\n82",
      "page": 82,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Chapter 5\nAligning Very Small Parallel\nCorpora Using Cross-Lingual Word\nEmbeddings and a Monogamy\nObjective\n83",
      "page": 83,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Aligning Very Small Parallel Corpora Using\nCross-Lingual Word Embeddings and a Monogamy Objective\nNina Poerner, Masoud Jalili Sabet, Benjamin Roth and Hinrich Sch¨utze\nCenter for Information and Language Processing\nLMU Munich, Germany\npoerner@cis.uni-muenchen.de\nAbstract\nCount-based word alignment methods, such as\nthe IBM models or fast-align, struggle on very\nsmall parallel corpora. We therefore present\nan alternative approach based on cross-lingual\nword embeddings (CLWEs), which are trained\non purely monolingual data. Our main con-\ntribution is an unsupervised objective to adapt\nCLWEs to parallel corpora.\nIn experiments\non between 25 and 500 sentences, our method\noutperforms fast-align. We also show that our\nﬁne-tuning objective consistently improves a\nCLWE-only baseline.\n1\nIntroduction\nSome parallel corpora, such as the Universal Dec-\nlaration of Human Rights, are too small to apply\ncount-based word alignment algorithms.\nSabet et al. (2016) show that integrating mono-\nlingual word embeddings into IBM Model 1\n(Brown et al., 1990) decreases word alignment er-\nror rate on a parallel corpus of 1000 sentences.\nPourdamghani et al. (2018) exploit monolingual\nembedding similarity scores to create synthetic\ntraining data for Statistical Machine Translation\n(SMT), and report an increase in alignment F1.\nRecent advances have made it possible to cre-\nate cross-lingual word embeddings (CLWEs) from\npurely monolingual data (Zhang et al. (2017a),\nZhang et al. (2017b), Conneau et al. (2017),\nArtetxe et al. (2018a)). We propose to leverage\nsuch CLWEs for a similarity-based word align-\nment method, which works on corpora as small as\n25 sentences. Like Sabet et al. (2016), our method\nrelies only on monolingual data (to train the em-\nbeddings) and on the small parallel corpus itself.\nOur CLWE-only baseline aligns source and\ntarget words in a parallel corpus if their CLWEs\nhave maximum cosine similarity. This approach is\nindependent from the size of the parallel corpus,\nbut has the following problems:\n• Semantics may differ between the embedding\ntraining domain and the parallel corpus.\n• CLWEs sometimes fail to discriminate be-\ntween words with similar contexts, e.g.,\nantonyms.\nWe therefore propose to ﬁne-tune the CLWEs\non the small parallel corpus using an unsuper-\nvised embedding monogamy objective. To eval-\nuate the proposed method, we simulate sparse\ndata settings using Europarl sentences and Bible\nverses. Our method outperforms the count-based\nfast-align model (Dyer et al., 2013) for corpus\nsizes up to 500 (resp., 250) sentences.\nThe\nproposed ﬁne-tuning method improves over the\nCLWE-only baseline in terms of both precision\nand recall.\na)\nb)\nc)\nd)\nFigure 1: Schematic representation of the monogamy\nobjective. a) one-to-one (“monogamous”) alignment:\nl(s, t) = 0, b) many-to-many alignment: l(s, t) = 1,\nc) one-to-many alignment: l(s, t) = 1, d) minimiz-\ning l(s, t) means making the red nodes more similar\nto each other, and less similar to the white nodes.\n2\nMethod\n2.1\nCLWE-only baseline\nOur CLWE-only baseline uses a cross-lingual em-\nbedding space derived from purely monolingual\ndata (Artetxe et al., 2018a). Let D be our small\ncorpus, and let s (source) and t (target) be parallel\nsentences from D. Let clwe(si) and clwe(tj) be\nthe embedding vectors of tokens si and tj. We\nalign si to argmaxtj∈t[cos(clwe(si), clwe(tj)].\narXiv:1811.00066v1  [cs.CL]  31 Oct 2018\n84",
      "page": 84,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Any ties are broken by proximity to the diagonal\nof the alignment matrix.\n2.2\nFine-tuning method\nIntuition.\nAssume that we have the following\nsentence pair: aaa bbb xxx ||| 111 000 222. As-\nsume further that we know from CLWEs that aaa\n≈111 and bbb ≈222, but we lack informative\nembeddings for 000 and xxx. We may hypothesize\nthat xxx ≈000, as they are the only tokens that\nlack translations. We may also hypothesize that\nxxx ̸≈111, xxx ̸≈222, as 111 and 222 already\nhave translations of their own.\nIn the following, we will refer to this principle\nas embedding monogamy. We assume that in the\nabsence of evidence to the contrary, a source em-\nbedding should have\n• high similarity to one target embedding\n• low similarity to other target embeddings1\nThis principle is related to the IBM Model (Brown\net al., 1990), where Expectation Maximization in-\ncreases p(f|e) if e and f co-occur in sentences\nwhere f is not explained by other source words.\nEmbedding monogamy objective.\nWe deﬁne\nthe probability of tj given si as:\np(tj|si, t) =\ne\n1\nτ cos(clwe(si),clwe(tj))\nP\nj′ e\n1\nτ cos(clwe(si),clwe(tj′))\n(1)\nwhere τ is a temperature hyperparameter.\nThis\ndeﬁnition is similar to the deﬁnition of translation\nprobability in Artetxe et al. (2018b) and Lample\net al. (2018). But while they normalize over the\nvocabulary, we normalize over the target sentence.\nAs a consequence, the probability of tj depends\nnot only on si, but also on competitor tokens in t.\nWith these translation probabilities, we model\na two-step random walker Rs→t→s that starts at\nsi, steps to a random target word and then to si′:\nrs→t→s\nii′\n= Plen(t)\nj=1 p(tj|si, t)p(si′|tj, s). To max-\nimize monogamy, we maximize the entries on the\ndiagonal of Rs→t→s, i.e., the probability of the\nwalker returning to its origin. To avoid penaliz-\ning long sentences, we minimize the negative log-\narithm to the base of the source sentence length:\nl(s, t) = 1 −loglen(s)\nPlen(s)\ni=1\nrs→t→s\nii\n. This loss\nhas the following properties:\n1 Of course, this assumption is over-simplistic, as one-to-\nn alignments exist (e.g., English not should be similar to both\nFrench ne and pas).\n• In a fully “monogamous” situation (see Fig-\nure 1 a), rs→t→s\nii\n→1 =⇒l(s, t) →0.\n• In a situation where all source words are\nequidistant from all target words (see Figure\n1 b), rs→t→s\nii\n=\n1\nlen(s) =⇒l(s, t) = 1.\nReversing the roles of source and target results\nin the following bidirectional loss: Lbi(s, t) =\n1\n2[l(s, t) + l(t, s)]. Both terms are necessary, since\na given alignment may appear highly monoga-\nmous from the perspective of one sentence but\nnot the other (especially when there are left-over\nwords due to a difference in length).\nAdding position information.\nAt this point, our\nobjective ignores word positions, which we know\nto be useful from count-based methods (e.g., Dyer\net al. (2013)). Therefore, we add position embed-\ndings inside the translation probability equation:\np(tj|si, t) =\ne\n1\nτ cos[clwe(si)+a(i),clwe(tj)+a(j)]\nP\nj′ e\n1\nτ cos[clwe(si)+a(i),clwe(tj′)+a(j′)]\nwhere a(i) is a sinusoid embedding vector for\nposition i (Vaswani et al., 2017).\nAs a result,\nword pairs near the diagonal have higher round\ntrip probabilities initially. Since the monogamy\nobjective aims to strengthen strong links, simi-\nlar position embeddings act as attractors for non-\npositional embeddings. Note that we use only the\nnon-positional embeddings for alignment, as the\nposition prior is too strong at test time.\nAlignment retention objective.\nIn initial exper-\niments, we found that the monogamy objective in-\ncreases recall but risks losing precision, relative\nto the CLWE-only baseline. Therefore, we add\nan additional objective that aims to increase round\ntrip probability for alignments made by the base-\nline, but does not inﬂuence unaligned words:\nLret(s, t) = 1\n2[lret(s, t) + lret(t, s)]\nlret(s, t) = −log\nP\ni,j p(tj|si, t)p(si|tj, s)mst\nij\nP\ni,j mst\nij\nmst\nij = I[(si, tj) ∈align0]\nwhere align0 is the intersection of the s-to-t and t-\nto-s alignments made with the initial CLWEs (see\nSection 2.1). Our ﬁnal loss function is: L(D) =\n1\n|D|\nP\n(s,t)∈D[Lbi(s, t) + αLret(s, t)].\n85",
      "page": 85,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Figure 2: Alignment precision, recall and F1 as a func-\ntion of corpus size.\n3\nEvaluation\nWe evaluate our model on subsets of different\nsizes from the English-German Europarl gold\nalignments2 and French-English Bible gold align-\nments (Melamed, 1998)3. We initialize CLWEs\nwith the unsupervised algorithm of Artetxe et al.\n(2018a) on monolingual FastText embeddings\n(Bojanowski et al., 2017)4. Fine-tuning is done\nin keras, using the adam optimizer (Kingma and\nBa, 2014). We set α = 1.0 and τ = 0.001, and\napply 50% dropout to the embeddings.\nWe use fast-align (Dyer et al., 2013) as a count-\nbased baseline, since it outperformed the IBM\nmodels in initial experiments.\nWe symmetrize\nalignments by either intersection or the grow-diag-\nﬁnal-and (GDFA) heuristic (Koehn et al., 2007).\nWe train fast-align and our ﬁne-tuning method for\n500 iterations.\n4\nDiscussion\n4.1\nCorpus size\nThe performance of fast-align is highly dependent\non corpus size, which is not surprising, seeing that\nit has to infer word semantics from the small cor-\npus alone. The CLWE-only baseline on the other\nhand is independent from corpus size, resulting in\ndecent performance even on 25 parallel sentences.\nImportantly, the positive effect of our ﬁne-tuning\nmethod seems to be robust to corpus size, as we\nsee improvements in F1 for all sizes.\n2www-i6.informatik.rwth-aachen.de/\ngoldAlignment/\n3nlp.cs.nyu.edu/blinker/.\nWe consider links\nwith inter-annotator agreement as sure, others as possible.\n4fasttext.cc, top-200000 words per language\n4.2\nBeneﬁts of ﬁne-tuning\nWe ﬁnd that the proposed ﬁne-tuning method has\na positive effect on alignment precision and recall,\nrelative to the CLWE-only baseline.\nWe assess\nsome sentence pairs qualitatively to ﬁnd reasons\nfor this improvement:\nResolution of ambiguities.\nWord embeddings\nsometimes fail to differentiate between words with\nsimilar contexts, such as antonyms.\nIn Figure\n3 (top), our ﬁne-tuning method resolves such an\nambiguity: Here, the initial CLWE of answer is\nslightly more similar to German frage (= question)\nthan to the true translation antwort. Since frage al-\nready has a round trip partner, the monogamy ob-\njective pushes answer away from frage, resulting\nin the addition of a correct alignment between an-\nswer and antwort.\nIn-domain word translations.\nSince word em-\nbeddings are trained on general-purpose corpora,\nCLWEs can fail to reﬂect domain-speciﬁc word\ntranslations.\nOne such example is the transla-\ntion of lord as French ´eternel (≈“eternal one”)\nin Figure 3 (bottom).\nWhile the translation\nis common in this particular Bible version, the\nCLWEs do not reﬂect it well (cos(lord, ´eternel) <\ncos(wicked, ´eternel)).\nThrough ﬁne-tuning, and\ndue to their frequent coocurrence in the small cor-\npus, the similarity between ´eternel and lord in-\ncreases enough for a successful alignment.\n5\nUse case: Aligning the UDHR\nIn practice, our method would not be applied to\nEnglish-German or English-French, as there is no\nlack of parallel data for these language pairs. For\na more realistic use case, we align the 50 articles\nof the Universal Declaration of Human Rights5 in\nMacedonian and Afrikaans. While we do not have\ngold alignments for an evaluation, a preliminary\nqualitative analysis suggests that our method ﬁnds\na reasonable semantic word alignment, while fast-\nalign mainly predicts the diagonal (see Figure 4\nfor examples).\n6\nRelated Work\nEmbeddings for word alignment.\nSabet et al.\n(2016) reformulate the IBM 1 model to predict\nthe probability of monolingual target embedding\nvectors. They report improvements in AER for\n5https://unicode.org/udhr/\n86",
      "page": 86,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Figure 3: Similarity matrices before (left) and after (right) ﬁne-tuning. Red dots: our alignment (intersection).\nWhite squares: sure gold alignments. Empty white squares: possible gold alignments.\nEnglish-French on parallel corpora between 1K\nand 40K sentences, as well as improvements in\nprecision on words with frequency ≤20.\nPourdamghani et al. (2018) exploit similar-\nity scores from monolingual embeddings to cre-\nate synthetic training data for an SMT sys-\ntem.\nThey report improvements for English-\nChinese, English-Arabic and English-Farsi align-\nment (∆F1 = 0.2%, 0.5%, 1.7%). Their small-\nest parallel corpus has 500K sentences, while we\nalign a few dozen to hundred sentences.\nTwo-step round trip objective.\nOur use of two-\nstep round trips is inspired by Haeusser et al.\n(2017). They optimize domain adaptation using a\nrandom walker that steps from image representa-\ntions with known labels to image representations\nwith unknown labels and back. While their tar-\nget is a uniform distribution over images with the\nsame label as the image of origin, ours is to have\nmaximum probability mass on the word of origin.\nLow resource CLWEs.\nOur approach relies on\nthe availability of high-quality CLWEs.\nWada\nand Iwata (2018) report that in settings with lit-\ntle monolingual data (< 1M sentences), mapping\napproaches like Artetxe et al. (2018a) are not ro-\nbust. Instead, they propose to learn CLWEs from a\nlanguage model trained on the union of two small\nmonolingual corpora. Their work is orthogonal\nto our ﬁne-tuning method, since we make no as-\nsumptions about how the CLWEs are created.\n7\nConclusion\nWe have presented a similarity-based method to\nproduce word alignments for very small paral-\nlel corpora, using monolingual data and the cor-\npus itself. Our CLWE-only baseline uses an un-\nsupervised mapping of monolingual embeddings\n(Artetxe et al., 2018a). Our main contribution is\nan unsupervised embedding monogamy objec-\ntive, which adapts CLWEs to the small parallel\ncorpus. Our model outperforms count-based fast-\nalign (Dyer et al., 2013) on parallel corpora up to\n500 (resp., 250) sentences.\nWe expect that our method will be useful in low-\nresource settings, e.g., when aligning the Univer-\nsal Declaration of Human Rights.\n87",
      "page": 87,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Figure 4: Articles 14(1) and 26(3) from the UDHR. Similarity matrices before (left) and after (right) ﬁne-tuning.\nRed dots: our alignment (intersection). Red boxes: fast-align (intersection). White squares: sure gold alignments.\nEmpty white squares: possible gold alignments (by the authors).\n88",
      "page": 88,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Acknowledgments.\nWe gratefully acknowledge\nfunding for this work by the European Research\nCouncil (ERC #740516).\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings.\nIn ACL, pages 789–798, Melbourne, Aus-\ntralia.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018b.\nUnsupervised statistical machine transla-\ntion. In EMNLP, pages 3632–3642, Brussels, Bel-\ngium.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion of Computational Linguistics, 5(1):135–146.\nPeter F Brown, John Cocke, Stephen A Della Pietra,\nVincent J Della Pietra, Fredrick Jelinek, John D Laf-\nferty, Robert L Mercer, and Paul S Roossin. 1990. A\nstatistical approach to machine translation. Compu-\ntational Linguistics, 16(2):79–85.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2017.\nWord translation without parallel data.\narXiv\npreprint arXiv:1710.04087.\nChris Dyer, Victor Chahuneau, and Noah A Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of IBM Model 2. In NAACL-HTL, pages 644–\n648, Atlanta, USA.\nPhilip Haeusser, Thomas Frerix, Alexander Mordvint-\nsev, and Daniel Cremers. 2017. Associative domain\nadaptation.\nIn ICCV, pages 2765–2773, Venice,\nItaly.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan,\nWade Shen,\nChristine Moran,\nRichard Zens, et al. 2007.\nMoses: Open source\ntoolkit for statistical machine translation. In ACL,\npages 177–180, Prague, Czech Republic.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. arXiv preprint arXiv:1804.07755.\nI Dan Melamed. 1998. Manual annotation of transla-\ntional equivalence: The Blinker project. Technical\nreport, University of Pennsylvania Institute for Re-\nsearch in Cognitive Science.\nNima Pourdamghani,\nMarjan Ghazvininejad,\nand\nKevin Knight. 2018. Using word vectors to improve\nword alignments for low resource machine transla-\ntion. In NAACL-HLT, pages 524–528, New Orleans,\nUSA.\nMasoud Jalili Sabet, Heshaam Faili, and Gholamreza\nHaffari. 2016.\nImproving word alignment of rare\nwords with word embeddings. In COLING 2016:\nTechnical Papers, pages 3209–3215, Osaka, Japan.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008, Long Beach,\nUSA.\nTakashi Wada and Tomoharu Iwata. 2018.\nUnsu-\npervised cross-lingual word embedding by multi-\nlingual neural language models.\narXiv preprint\narXiv:1809.02306.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017a. Adversarial training for unsupervised\nbilingual lexicon induction. In ACL, pages 1959–\n1970, Vancouver, Canada.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017b. Earth mover’s distance minimization\nfor unsupervised bilingual lexicon induction.\nIn\nEMNLP, pages 1934–1945, Copenhagen, Denmark.\n89",
      "page": 89,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Chapter 6\n\nSubword Sampling for Low\nResource Word Alignment",
      "page": 91,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Subword Sampling for Low Resource Word Alignment\nEhsaneddin Asgari∗†, Masoud Jalili Sabet ∗,⋄, Philipp Dufter⋄, Christopher Ringlstetter†, Hinrich Sch¨utze⋄\n⋄Center for Information and Language Processing, LMU Munich, Germany\n† NLP Expert Center, Data:Lab, Volkswagen AG, Munich, Germany\nehsaneddin.asgari@volkswagen.de masoud@cislmu.org\nAbstract\nAnnotation projection is an important area\nin NLP that can greatly contribute to creat-\ning language resources for low-resource lan-\nguages. Word alignment plays a key role in\nthis setting.\nHowever, most of the existing\nword alignment methods are designed for a\nhigh resource setting in machine translation\nwhere millions of parallel sentences are avail-\nable.\nThis amount reduces to a few thou-\nsands of sentences when dealing with low-\nresource languages failing the existing estab-\nlished IBM models.\nIn this paper, we pro-\npose subword sampling-based alignment of\ntext units. This method’s hypothesis is that the\naggregation of different granularities of text\nfor certain language pairs can help word-level\nalignment.\nFor certain languages for which\ngold-standard alignments exist, we propose an\niterative Bayesian optimization framework to\noptimize selecting possible subwords from the\nspace of possible subword representations of\nthe source and target sentences. We show that\nthe subword sampling method consistently\noutperforms word-level alignment on six lan-\nguage pairs: English-German, English-French,\nEnglish-Romanian, English-Persian, English-\nHindi, and English-Inuktitut. In addition, we\nshow that the hyperparameters learned for cer-\ntain language pairs can be applied to other lan-\nguages at no supervision and consistently im-\nprove the alignment results. We observe that\nusing 5K parallel sentences together with our\nproposed subword sampling approach, we ob-\ntain similar F1 scores to the use of 100K’s of\nparallel sentences in existing word-level fast-\nalign/eﬂomal alignment methods.\n1\nIntroduction\nAnnotation projection is an important area in Nat-\nural Language Processing (NLP), which aims to\n∗Equal contribution\nexploit existing linguistic resources of a particu-\nlar language for creating comparable resources in\nother languages (usually low resource languages)\nusing a mapping of words across languages. More\nprecisely, annotation projection is a speciﬁc use of\nparallel corpora, corpora containing pairs of trans-\nlated sentences from language ls to lt. In anno-\ntation projection, a set of labels available for lan-\nguage ls is projected to language lt via alignment\nlinks (the mapping between words in parallel cor-\npora of ls and lt). Ls labels can either be obtained\nthrough manual annotation or through an analysis\nmodule that may be available for ls, but not for lt.\nThe label here can be interpreted broadly, includ-\ning, e.g., part of speech labels, morphological tags\nand segmentation boundaries, sense labels, mood\nlabels, event labels, syntactic analysis, and coref-\nerence (Yarowsky et al., 2001; Diab and Resnik,\n2002; Agi´c et al., 2016).\nLanguage resource creation for low-resource lan-\nguages, for the purpose of automatic text analysis\ncan create ﬁnancial, cultural, scientiﬁc, and polit-\nical value. For instance, the creation of a senti-\nment lexicon for a low resource language would\nbe an excellent help for customer reviews analy-\nsis in big corporations having branches all over\nthe world, where 7000 languages are spoken, or\nsuch a resource can be used to predict stock market\nmovements from social media in a low resource\nsetting. Furthermore, such resources can contribute\nto creating knowledge ( ¨Ostling, 2015; Asgari and\nSch¨utze, 2017) for linguists, which pure scientiﬁc\nvalue aside, the linguistic knowledge can be incor-\nporated into machine learning models for natural\nlanguage understanding as well.\nThe mapping between words across languages\nas a basis for annotation projection is automatically\ngenerated using statistical word alignment, mod-\neled on parallel corpora. This means that given\nparallel corpora for a set of languages and linguis-\narXiv:2012.11657v2  [cs.CL]  15 Jun 2021\n92",
      "page": 92,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "tic resources for only one language, we can auto-\nmatically create resources for the other languages\nthrough annotation projection. One of the main\nchallenges for annotation projection is that cor-\npora are often relatively small for low resource lan-\nguages. The existing IBM-based alignment mod-\nels work well for high-resource settings, but they\nfail in the low-resource case (Poerner et al., 2018).\nThe most popular dataset for low resource align-\nment, the Bible Parallel Corpus, containing a large\nnumber (1000+) of languages, are characteristically\nlow-resource, i.e., having only around 5000-10000\nparallel sentences per language pair. This paper\naims to introduce a framework to reliably relate lin-\nguistic units, words, or subwords, in a low resource\nparallel corpus, based on sampling from the space\nof possible subwords.\n2\nMethods\n2.1\nDataset\nWe work on the word alignment gold standards\nfrom the WPT 2003 and 2005 shared tasks on\nword alignment. Those language pairs include\nFrench, Hindi, Romanian, and Inuktitut always\npaired with English. In addition, we add English-\nPersian and English-German. As per the standard\nscenario in the world alignment literature, we com-\npute an alignment model on an independent corpus\nof training materials. To simulate a low-resource\nscenario, we sample the number of parallel train-\ning sentences down to 5000, except for Hindi with\n3000 sentence pairs. This is the order of magnitude\nin training data when dealing with low-resource lan-\nguages contained, e.g., in the Bible Parallel Corpus.\nIn addition, we experiment on mid-resource cases\nwhen using the complete set of available training\nsentences in English-Romanian, English-Inuktitut,\nEnglish-Persian, in which their complete set is nei-\nther low-resource nor contain more than 1M paral-\nlel sentences. See Table 1 for details on the data.\n2.2\nEvaluation\nWe evaluate word alignments with F1 score com-\nputed by\nprec = |A ∩P|\n|A|\n, rec = |A ∩S|\n|S|\n, F1 = 2 prec rec\nprec + rec,\nwhere |A| is the set of predicted alignment edges,\n|S| the set of sure and |P| the set of possible align-\nment edges. Note that S ⊂P, and both are known\nfrom the gold standard.\n2.3\nSentence subword space\nFor splitting text into subwords, we use Byte-Pair-\nEncoding by Sennrich et al. (2016). The BPE algo-\nrithm for a certain random seed and a given vocab-\nulary size (analogous to the number of character\nmerging steps) breaks a sentence into a unique se-\nquence of subwords. Continuing the merging steps\nwould result in the enlargement of the subwords,\nresulting in fewer tokens.\nHypothesis: Let Spq = SN\nj=1(s(j)\np , s(j)\nq ) be a col-\nlection of N parallel paired sentences in the lan-\nguage pair lp and lq. We assume that for a certain\nSpq there exists an optimal segmentation scheme\nconstructed by accumulation of different granulari-\nties of (lp,lq), ξ∗, among all possible segmentation\nschemes (ξ’s), which depends on the morphologi-\ncal structures of this language pair.\nThe space of possible segmentations of a sen-\ntence s, denoted as Φl(s) for language l, is created\nby variations in the segmentation by varying the\nnumber of merging steps.\nIn this notation Φl(s) = SMl\ni=1 Φ(i)\nl (s), where\nΦ(k)\nl\n(s) refers to a speciﬁc vocabulary size selec-\ntion for the segmentation of s considering the ﬁrst\nk merging steps in the BPE algorithm for language\nl. Ml is the maximum number of merging steps in\nl. We deﬁne Φpq as the set of all possible segmen-\ntation pairs in language pair lp and lq:\nΦpq =\nMl1\n[\ni=1\nΦ(i)\np ×\nMl2\n[\ni=1\nΦ(i)\nq\nWhen we deal with a single language, to explore\nthe possible segmentations, Monte Carlo sampling\nfrom Φl can be used to have different views on\nthe segmentation, where the likelihood of certain\nsegmentation Φ(k)\nl\nis proportional to the number\nof sentences affected in the corpus by introduc-\ning the kth merging step, as proposed in (Asgari\net al., 2019a; Asgari, 2019; Asgari et al., 2019b) for\nthe segmentation of protein and DNA sequences.\nHowever, in the alignment problem, we deal with\na 2D space (can be represented as a grid as in Fig-\nure 4) of possibilities for the vocabulary sizes (≈\nthe number of merging steps in BPE) of lp and lq.\nThe inclusion of each cell in this grid introduces\nnew instances to the parallel corpus, potentially\ntransferring a low-resource setting to a high-or-\nmid resource setting. In this high resource setting,\nthe subwords of a certain sentence are assigned in\nT ways (the number of cells we select from the\n93",
      "page": 93,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Gold\nParallel Training\nLang.\nStandard\n# Sentences\n|S|\n|P \\ S|\nData\n# Sentences\nEnglish-German\nEuroParl-baseda\n508\n9612\n921\nEuroParl (Koehn, 2005)\n1920k\nEnglish-Persian\n(Tavakoli and Faili, 2014)\n400\n11606\n0\nTEP (Pilevar et al., 2011)\n600k\nEnglish-French\nWPT2003, (Och and Ney, 2000),\n447\n4038\n13400\nHansards (Germann, 2001)\n1130k\nEnglish-Hindi\nWPT2005b\n90\n1409\n0\nEmille (McEnery et al., 2000)\n3k\nEnglish-Inuktitut\nWPT2005b\n75\n293\n1679\nLegislative Assembly of Nunavutb\n340k\nEnglish-Romanian\nWPT2005b\n203\n5033\n0\nConstitution, Newspaperb\n50k\na www-i6.informatik.rwth-aachen.de/goldAlignment/\nb http://web.eecs.umich.edu/˜mihalcea/wpt05/\nTable 1: Details on gold standards and training data. |S| is the number of sure edges in the gold standard and\n|P \\ S| the number of additional possible edges.\nΦpq grid). Finally, to conﬁrm an alignment link\nat word-level, we set a threshold λ; λ is the min-\nimum ratio of subword segmentation is required\nto conﬁrm a word alignment link. Note that not\nnecessarily all cells of the grid improve the align-\nment, we thus need a strategy to pick a subset of\ncells ξ∗⊂Φpq maximizing the ultimate alignment\nscore. Having language pairs with ground-truth\nalignment, we can solve this problem via hyperpa-\nrameter optimization using Bayesian optimization.\nSubsequently, we investigate whether applying the\nsame hyperparameters, on another language pair\nyields improvements. To solve this the optimiza-\ntion problem for the supervised case, we propose\nan iterative greedy subword sampling algorithm.\n2.4\nIterative subword sampling algorithm\nTo maximize the alignment score for the known\nlinks (the ground-truth) at the word-level, we are\nseeking for ξ∗a set of cells in the Φpq grid, and\ntheir corresponding thresholds λ∗satisfying the\nfollowing equation:\nξ∗, λ∗=\nargmin\nξi,0≤λ≤1;i∈{1,2,..,T}\n−f(Φpq, Spq, ypq),\nwhere f refers to the alignment F1 score based\non ground-truth, which its underlying alignment\nmodel does not have any closed form nor gradi-\nent. ypq is the ground-truth we have for the lan-\nguage pair lp and lq, and Spq refers to the paral-\nlel sentences, which are going to be segmented\nin T different schemes (T cells from the Φpq\ngrid, 0 < i < T).\nThese T cells can be se-\nlected in any order. However, to reduce the search\nspace, we propose a sequential greedy selection\nof the segmentations (ξi, λ), and solve each step\nin a Bayesian optimization framework. The it-\nerative process is detailed in Algorithm 1. The\ncore computation of this algorithm is ξi, λ =\nargmin\nξi,λ\n−f(Φpq, Spq, ypq, ξ0:i−1), for which the\nselected vocabulary sizes up to the current iteration\n(ξ0:i−1) are used for segmentation and the measure-\nment of the alignment score. We perform Bayesian\noptimization to ﬁnd the next optimal values for ξi\nand λ. As discussed in §2.3, in the Bayesian op-\ntimization, we explore the cells from the grid of\nΦpq using logarithmic priors for each of Φp and\nΦq. We continue this process until the the moment\nwhere introducing more segmentations does not im-\nprove the alignment score, setting an early stopping\ncondition.\nAlgorithm 1: Iterative subword sampling\nResult: graph G of word-aligned sentence pairs\nf1prev = 0; i = 0;\nξ = < empty >; % history of selected cells\nλ = < empty >; % history of selected λ’s\nδ = +∞; E = early-stopping parameter;\nwhile ∃δ > 0 in the last E iterations do\nξi, λ = argmin\nξi,λ\n−f(Φpq, S, y, ξ, λ);\nξ.push(ξi);\nλ.push(λ);\nf1∗= f(S, y, ξ, λ);\nδ = f1∗−f1prev;\nf1prev = f1∗;\ni = i + 1;\nend\nG = alignment(segment(S, ξ, λ))\n2.5\nIntuition behind the use of logarithmic\npriors for the vocabulary size\nFigure 1 provides an intuition behind the use of\nlogarithmic priors. This diagram shows that by\nintroducing a new merging step (increasing the vo-\ncabulary size by one) in the BPE algorithm, which\nportion of sequences are affected. As proposed\nfor protein sequences (Asgari et al., 2019a), this\ncan be served as an approximation for the relative\nlikelihood of including a merging step (which is\nanalogous to introducing a new subword).\n94",
      "page": 94,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BPE merging steps\nNumber of sentences affected\nby the new merging step\n0\n10000\n20000\n30000\n40000\n50000\n2000\n4000\n6000\n8000\n10000\n0\nFigure 1: An example of English BPE on a collection\nof 10000 sentences. This diagram shows that with in-\ntroducing new merging steps how many sentences are\ngoing to be affected .\n2.6\nSubword sampling in other languages\nAfter training how to choose ξ∗, λ∗for a particu-\nlar language pair, we apply the same vocabulary\nsettings on new language pairs and evaluate the re-\nsulting alignment scores. This would help us in the\ninvestigation of the generalizability of a language\npair on other language pairs.\n2.7\nExperimental Setup\nEvaluate Low-Resource Alignment\nSince the main motivation of subword sampling\nalignment is for the low-resource scenario1, we\nﬁrst evaluate the method for an analogous use\ncase, where only a few thousands of parallel\nsentences are available, similar to the Bible Par-\nallel Corpus of 1000+ languages.\nTo produce\na similar scenario for the evaluation, we get\nsamples of 5K aligned sentences from the train-\ning datasets of English-German, English-French,\nEnglish-Romanian,\nEnglish-Persian,\nEnglish-\nHindi, and English-Inuktitut and concatenate the\ngold-standard datasets to them. The statistical word\naligners generate forward, and backward align-\nments need a post-processing step of symmetriza-\ntion (Koehn, 2010). We compared intersection and\ngrow-diag-ﬁnal-and (GDFA), which produce com-\nparable results in terms of F1 score, and the inter-\nsection method having a higher precision. Since the\nﬁnal alignments are produced from the aggregation\n1The language itself is not necessarily a low-resource lan-\nguage, but the number of sentence pairs is relatively low (less\nthan 10K)\nof all segmentations’ alignments, the intersection\nmethod with higher precision is a proper candidate.\nThus, we use the intersection method throughout\nthe experiments.\nFor each language pair, we evaluate the word-\nlevel alignment, as well as the Bayesian optimiza-\ntion subword sampling. In addition, in order to\ninvestigate how the vocabulary size of a particu-\nlar language pair generalizes to the other language\npairs, we also evaluate the optimized ξ∗\nl1,l2, λ∗\nl1,l2\nfor each pair on all other language pairs.\nEvaluate Mid-Resource Alignment\nIn addition to the low-resource alignment, we eval-\nuate our approach against the word-level align-\nment of fast-align and eﬂomal in the mid-resource\nscenario (having less than 1M sentence pairs).\nTherefore, From the six language pairs with gold-\nstandard alignment, we select English-Persian,\nEnglish-Inuktitut, and English Romanian, contain-\ning 600k, 340k, 50k sentence pairs, respectively.\nFor each language pair, we use the vocabulary sizes\noptimized in the low-resource alignment experi-\nment.\n3\nResults\n3.1\nIterative Subword Sampling\nAn example space of Φpq (for English-German)\nthat is explored in the Bayesian optimization to\nﬁnd the ξ∗is shown in Figure 3, a 2D representa-\ntion of the selected cells and the order of selection\nby Bayesian optimization on the English-German\ncorpus is provided in Figure 4. We observe that\nthe new segmentation in each iteration consistently\nimproves the alignment scores in the next iteration.\nFurthermore, as may be expected, the sampled vo-\ncabulary sizes are mainly chosen from the lower\nsizes, i.e., affecting more sentences (Figure 1). All\nstudied language pairs show similar behaviour in\nselecting subword vocabulary sizes (Figure 2).\n3.2\nLow-Resource Alignment Results\nTable 2 shows F1 scores of alignment across six\nlanguage pairs in the low-resource alignment (hav-\ning a maximum set of 5K aligned sentence pairs).\nThis table compares the word-level and subword-\nlevel alignments as well as the generalizability of\nthe ξ∗\nl1,l2, λ∗\nl1,l2 on the other language pairs. In-\nterestingly, across all language pairs, we observe\nimprovements of 1.4 to 8.0 percentage points in the\nalignment F1 score in comparison with the word-\n95",
      "page": 95,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "0\n10k\n20k\n30k\n40k\n50k\n0\n10k\n20k\n30k\n40k\n50k\nLanguage pair\nEnglish-French\nEnglish-German\nEnglish-Hindi\nEnglish-Romani\nEnglish-Inukti\nEnglish-Persia\nSelected subword vocabulary sizes for each language pair in Bayesian optimization\nSource #vocab\nTarget #vocab\nFigure 2: Selected subword vocabulary sizes within Bayesian optimization for each language pairs of English-\nGerman, English-French, English-Romanian, English-Persian, English-Hindi, and English-Inuktitut.\nFigure 3: The space of Φpq that is explored in the Bayesian\noptimization in the ﬁrst 3 iterations. The exploring steps are\ncolored with their alignment F1 scores.\n1\n2\n3\n4\n5\n6\n7\n8\nΦp=<de>\nΦq=<en>\nξi Selected vocabualry pairs\nmerging steps\nmerging steps\n{1000, 1098, 1245, 1490, 1980, 3452, 5904, 10809, 25524, 50000}\n{1000, 1098, 1245, 1490, 1980, 3452, 5904, 10809, 25524, 50000}\nFigure 4: An example of the space Φpq for\nEnglish and German and the selected cells\nby the Bayesian optimization.\nlevel alignment. Subword-sampling optimized on\na speciﬁc pair consistently improves the word-level\nalignment also of the other languages. Certain\nlanguage pairs, like Romanian-English and Hindi-\nEnglish, proved better generalizability when ap-\nplied to the other language pairs. This result sug-\ngests that although the gold standard is decisive for\na signiﬁcant improvement of the alignment through\noptimizing the vocabulary sizes, then optimal vo-\ncabulary sizes trained on different language pairs\n(potentially with similar morphological complex-\nity) can be efﬁciently applied to increase the align-\nment performance for a new language pair .\n3.3\nMid-Resource Alignment Results\nF1 scores of alignment across three language pairs\nin the mid-resource alignment (having less than 1M\naligned sentence pairs) is shown in Table 3. This\ntable compares the word-level and subword-level\nalignments and the generalizability of the hyper-\nparameter optimized on other language pairs in\nlow-resource for a mid-resource setting. Again,\nacross all language pairs, we observe improve-\nments of 2.7 to 7 percentage points in the align-\nment F1 score compared to the word-level align-\nment. Interestingly, the F1 we achieved, using 5K\nparallel sentences and subword sampling, is simi-\nlar to the word-level F1 score of English-Inuktitut\n96",
      "page": 96,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "English-German\nEnglish-French\nEnglish-Romanian\nEnglish-Persian\nEnglish-Hindi\nEnglish-Inuktitut\nword-level\n0.685\n0.897\n0.616\n0.527\n0.508\n0.771\nVocab-size Sampling Optimization\n0.746\n0.913\n0.662\n0.579\n0.548\n0.858*\nApply <English-German>\n0.899\n0.645\n0.560\n0.532\n0.845*\nApply <English-French>\n0.743\n0.651\n0.541\n0.524\n0.853\nApply <English-Romanian>\n0.745\n0.905\n0.579\n0.547\n0.821\nApply <English-Persian>\n0.743\n0.908\n0.663\n0.521\n0.816\nApply <English-Hindi>\n0.742\n0.904\n0.663\n0.580\n0.804\nApply <English-Inuktitut>\n0.744\n0.914\n0.655\n0.530\n0.529\nTable 2: The alignment performances (in terms of F1 score) of six language pairs in the low-resource scenario,\nwhere the subword sampling and word-level alignments are compared. In addition, the results on applying the\nhyper-parameters of language pairs on all other pairs are also provided. We experimented systematically on the\nuse of both eﬂomal and fast-align for every setting. However, for simplicity, in each cell, the best performance of\nfast-align and eﬂomal is reported. With the exception of the marked F1 ’s with *, the best results obtained using\neﬂomal method for all the alignments.\nusing 240K parallel sentences and the word-level\nF1 score of English-Persian using 600K parallel\nsentences.\n3.4\nQualitative Analysis\nWe performed a qualitative analysis for English-\nGerman and showed six examples in Figure 5. We\nobserved two sources of improvements: i) Com-\npounds, which are frequent in German, obtain bet-\nter alignments and ii) As we aggregate alignment\nedges through a λ-weight vote, we observe an “en-\nsembling” effect which mainly affects fertility. Ex-\namples 1, 2 and 3 show the compound effect: “Men-\nschenrechte” is correctly aligned to “Human rights”\nonly when using sampling optimization. Similarly,\n“Mobiltelefonlizenzen” gets successfully aligned to\n“mobile phone license” whereas pure eﬂomal only\naligns it to “licenses”. In addition, the differently\nformatted year numbers in Example 2 are easy to\nalign once subword sampling is used. Examples 4\nand 5 show the presumed ensembling effect. We hy-\npothesize that “EVP” is aligned to different words\nin the English sentence across different subword\nsamples. Once aggregated, “EVP-Fraktion” has\nhigh fertility, which is useful in this scenario. Sim-\nilarly, “des” (meaning “of the”) receives a better\nalignment through subword sampling as some mod-\nels align “des” to “of” and some others to “the”.\nSubword sampling cannot resolve all errors of eﬂo-\nmal and can also be harmful in rare cases. Example\n6 shows a case where the word “Abmessungen”\n(“dimensions” or “measurements”) obtains two in-\ncorrect alignment edges, presumably because it\nfrequently gets split into subwords like “Ab” or\n“ungen” which carry only little semantic informa-\ntion.\n4\nRelated Work\nClassical models. Statistical word alignment meth-\nods (e.g., GIZA++ (Och and Ney, 2000), fast-\nalign (Dyer et al., 2013), eﬂomal ( ¨Ostling and\nTiedemann, 2016)) are mostly based on IBM mod-\nels (Brown et al., 1993), which are generative mod-\nels describing how a source language sentence S\ngenerates a target language sentence T using align-\nment latent variables. These models use an expec-\ntation maximization (EM) algorithm to train the\nalignment and only require sentence-aligned paral-\nlel corpora.\nNeural models. In 2014, seq2seq recurrent neu-\nral network (RNN) models introduced for ma-\nchine translation providing an end-to-end transla-\ntion framework (Sutskever et al., 2014). Attention\nwas a key component to improve such models (Bah-\ndanau et al., 2014; Luong et al., 2015). Two modi-\nﬁcations to attention were proposed to improve the\nquality of underlying alignment and consequently,\nthe quality of translation. (i) Model guided align-\nment training is introduced (Chen et al., 2016; Mi\net al., 2016; Garg et al., 2019; Stengel-Eskin et al.,\n2019) where the cross-entropy between attention\nweights and the alignment coming from an IBM\nmodel (GIZA++) or a manual gold standard is used\nas an additional cost function. Garg et al. (2019)\nﬁnd that operating at the subword-level can be ben-\neﬁcial for alignment models. Note that they only\nconsider a single subword segmentation. (ii) A dis-\nadvantage of neural architectures in comparison\nwith IBM models in producing alignments is that\nin the neural model the attention weights have only\nobserved the previous target words; in contrast, the\nIBM models beneﬁt from full observation of the tar-\nget sentence in alignment generation. Target fore-\nsight (Peter et al., 2017) improves translation by\n97",
      "page": 97,
      "type": "text",
      "language": "hi"
    },
    {
      "level": "H2",
      "text": "Alignment method\nEnglish-Romanian (50K)\nEnglish-Inuktitut (340K)\nEnglish-Persian (600K)\nfast-align\n0.643\n0.794\n0.552\nword-level\neﬂomal\n0.692\n0.864\n0.58\nfast-align\n0.667\n0.915\n0.525\nApply <English-German> parameters\neﬂomal\n0.715\n0.849\n0.638\nfast-align\n0.664\n0.913\n0.534\nApply <English-French> parameters\neﬂomal\n0.709\n0.885\n0.647\nfast-align\n0.663\n0.873\n0.534\nApply <English-Romanian> parameters\neﬂomal\n0.712\n0.826\n0.587\nfast-align\n0.677\n0.897\n0.562\nApply <English-Persian> parameters\neﬂomal\n0.711\n0.812\n0.587\nfast-align\n0.659\n0.865\n0.521\nApply <English-Hindi> parameters\neﬂomal\n0.719\n0.813\n0.606\nfast-align\n0.654\n0.911\n0.519\nApply <English-Inuktitut> parameters\neﬂomal\n0.71\n0.898\n0.65\nTable 3: The alignment performances (in terms of F1 score) of three language pairs in mid-resource scenario,\nwhere the subword sampling and word-level alignments are compared. In addition, the results on applying the\nhyper-parameters of language pairs on all other pairs are also provided. For each setting, both eﬂomal and fast-\nalign results are reported.\nconsidering the target word of the current decoding\nstep as an additional input to the attention calcu-\nlation. The main purpose of the above-mentioned\nalignment structures has been to improve transla-\ntion quality. In contrast, our main motivation is\nproviding a framework to reliably relate linguis-\ntic units, words, or subwords in parallel corpora,\nwhich can be used in linguistic resource creation\n(Agi´c et al., 2016; Asgari et al., 2020) and typolog-\nical analysis ( ¨Ostling, 2015; Asgari and Sch¨utze,\n2017). The above mentioned methods work well\nfor the large parallel corpora, but they fail when\nparallel sentences are scarce. Insufﬁciency of par-\nallel sentences is usually the case for low-resource\nlanguages, which are usually the most interesting\nscenarios for linguistic resource creation and lin-\nguistic analysis (Cieri et al., 2016).\nLow-resource alignment models. The most pop-\nular dataset for low resource alignment is the\nBible Parallel Corpus containing a large number\n(1000+) of languages, but are characteristically low-\nresource, i.e., have little text per language (Mayer\nand Cysouw, 2014). Some recent work touched\nupon this problem using unsupervised cross-lingual\nembeddings and a monogamy objective (Poerner\net al., 2018). However, this method could not im-\nprove the fast-align results for the parallel corpora\ncontaining more than 250 sentences. We showed\nthat our method improves the fast-align and eﬂomal\non six language pairs consistently on the size of\n5000K parallel sentences, in the range of parallel\nsentences of 1000+ languages in BPC, the most\ninteresting parallel corpora for the low-resource\nscenario (in terms of the number of covered lan-\nguages). Our proposed method improved the mid-\nresource alignments (50K-600K parallel sentences)\nas well.\nSubword sampling. The use of multiple sub-\nword candidates has improved the machine trans-\nlation performance (Kudo, 2018). BPE-Dropout\n(Provilkov et al., 2020) followed the same idea, in-\ntroducing dropout in the merging steps of a ﬁxed\nBPE to create multiple segmentations. The prob-\nabilistic use of multiple subword candidates has\nbeen proposed to segmentation protein sequences\n(Asgari et al., 2019a). We use the inspiration from\nthe latter approach for the word-alignment of paral-\nlel sequences of language pairs, using a multitude\nof possible subword segmentations.\n5\nConclusion\nMotivated by the important NLP area of anno-\ntation projection, used to create linguistic re-\nsources/knowledge in the low-resource languages,\nwe proposed subword sampling-based alignment\nof text units. This method’s hypothesis is that\nthe aggregation of different granularities of text\nfor speciﬁc language pairs can help with word-\nlevel alignment. For individual languages where\na gold-standard alignment corpus exists, we pro-\nposed an iterative Bayesian optimization frame-\nwork to optimize selecting subwords from the\nspace of possible BPE representations of the source\nand target sentences. We showed that the sub-\nword sampling method consistently outperforms\nthe pure word-level alignment on six language\npairs of English-German, English-French, English-\nRomanian, English-Persian, English-Hindi, and\nEnglish-Inuktitut in a low-resource scenario. Al-\nthough the subword samples are selected in a super-\n98",
      "page": 98,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Figure 5: Examples from the English-German gold standard. Dark green represent sure alignment edges. Light\ngreen possible edges (only on edge in Example 5). Circles are edges predicted by word-level eﬂomal, boxes are\npredicted when applying our proposed subword sampling with eﬂomal.\nvised manner, we show that the hyperparameters\ncan fruitfully be used for other language pairs with\nno supervision and consistently improve the align-\nment results. We showed that using 5K parallel\nsentences together with our proposed subword sam-\npling approach, we obtain similar F1 scores to the\nuse of 340K and 600K parallel sentences and word-\nlevel alignment in English-Inuktitut and English-\nPersian, respectively. The proposed method can\nefﬁciently improve the creation of linguistic re-\nsources (POS tagging, sentiment lexicon, etc.) for\nlow-resource languages, where only a few thousand\nparallel sentences are available.\nAcknowledgment\nThis work has been funded by the German Federal\nMinistry of Education and Research (BMBF) under\nGrant No. 01IS18036A. The authors of this work\ntake full responsibility for its content.\nReferences\nˇZeljko Agi´c,\nAnders Johannsen,\nBarbara Plank,\nH´ector Alonso Mart´ınez, Natalie Schluter, and An-\nders Søgaard. 2016.\nMultilingual projection for\nparsing truly low-resource languages. Transactions\nof the Association for Computational Linguistics,\n4:301–312.\nEhsaneddin Asgari. 2019. Life Language Processing:\nDeep Learning-based Language-agnostic Process-\ning of Proteomics, Genomics/Metagenomics, and\nHuman Languages. Ph.D. thesis, UC Berkeley.\nEhsaneddin Asgari, Fabienne Braune, Benjamin Roth,\nChristoph Ringlstetter, and Mohammad Mofrad.\n2020. UniSent: Universal adaptable sentiment lex-\nica for 1000+ languages.\nIn Proceedings of The\n12th Language Resources and Evaluation Confer-\nence, pages 4113–4120, Marseille, France. Euro-\npean Language Resources Association.\nEhsaneddin Asgari, Alice C McHardy, and Moham-\nmad RK Mofrad. 2019a.\nProbabilistic variable-\nlength segmentation of protein sequences for dis-\ncriminative motif discovery (dimotif) and sequence\nembedding (protvecx). Scientiﬁc reports, 9(1):1–16.\nEhsaneddin Asgari, Philipp C M¨unch, Till R Lesker,\nAlice C McHardy, and Mohammad RK Mofrad.\n2019b.\nDitaxa: Nucleotide-pair encoding of 16s\nrrna for host phenotype and biomarker detection.\n99",
      "page": 99,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Bioinformatics, 35(14):2498–2500.\nEhsaneddin Asgari and Hinrich Sch¨utze. 2017. Past,\npresent, future: A computational investigation of the\ntypology of tense in 1000 languages. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 113–124,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nPeter F Brown, Vincent J Della Pietra, Stephen A Della\nPietra, and Robert L Mercer. 1993. The mathemat-\nics of statistical machine translation: Parameter esti-\nmation. Computational linguistics, 19(2):263–311.\nWenhu Chen, Evgeny Matusov, Shahram Khadivi,\nand Jan-Thorsten Peter. 2016.\nGuided alignment\ntraining for topic-aware neural machine translation.\narXiv preprint arXiv:1607.01628.\nChristopher Cieri, Mike Maxwell, Stephanie Strassel,\nand Jennifer Tracey. 2016. Selection criteria for low\nresource language programs. In LREC.\nMona Diab and Philip Resnik. 2002.\nAn unsuper-\nvised method for word sense tagging using parallel\ncorpora. In Proceedings of the 40th Annual Meet-\ning on Association for Computational Linguistics,\npages 255–262. Association for Computational Lin-\nguistics.\nChris Dyer, Victor Chahuneau, and Noah A Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of ibm model 2. Association for Computational\nLinguistics.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Lin-\nguistics.\nUlrich Germann. 2001. Aligned Hansards of the 36th\nparliament of Canada.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Machine Transla-\ntion Summit, volume 5.\nPhilipp Koehn. 2010. Statistical Machine Translation.\nCambridge University Press.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation.\narXiv preprint\narXiv:1508.04025.\nThomas Mayer and Michael Cysouw. 2014.\nCreat-\ning a massively parallel bible corpus.\nOceania,\n135(273):40.\nAnthony McEnery, Paul Baker, Rob Gaizauskas, and\nHamish Cunningham. 2000. Emille: Building a cor-\npus of South Asian languages. VIVEK-BOMBAY-,\n13(3).\nHaitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.\nSupervised attentions for neural machine translation.\narXiv preprint arXiv:1608.00112.\nFranz Josef Och and Hermann Ney. 2000. Improved\nstatistical alignment models. In Proceedings of the\n38th Annual Meeting of the Association for Com-\nputational Linguistics, pages 440–447, Hong Kong.\nAssociation for Computational Linguistics.\nRobert ¨Ostling. 2015.\nWord order typology through\nmultilingual word alignment. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 205–211.\nRobert ¨Ostling and J¨org Tiedemann. 2016. Efﬁcient\nword alignment with markov chain monte carlo.\nThe Prague Bulletin of Mathematical Linguistics,\n106(1):125–146.\nJan-Thorsten Peter, Arne Nix, and Hermann Ney.\n2017.\nGenerating alignments using target fore-\nsight in attention-based neural machine translation.\nThe Prague Bulletin of Mathematical Linguistics,\n108(1):27–36.\nMohammad Taher Pilevar, Heshaam Faili, and Ab-\ndol Hamid Pilevar. 2011.\nTEP: Tehran English-\nPersian parallel corpus. In International Conference\non Intelligent Text Processing and Computational\nLinguistics. Springer.\nNina Poerner, Masoud Jalili Sabet, Benjamin Roth,\nand Hinrich Sch¨utze. 2018.\nAligning very small\nparallel corpora using cross-lingual word embed-\ndings and a monogamy objective.\narXiv preprint\narXiv:1811.00066.\nIvan Provilkov, Dmitrii Emelianenko, and Elena Voita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1882–1892, Online. Association for\nComputational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016.\nNeural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nElias Stengel-Eskin, Tzu-ray Su, Matt Post, and Ben-\njamin Van Durme. 2019.\nA discriminative neural\nmodel for cross-lingual word alignment. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 910–920, Hong\nKong, China. Association for Computational Lin-\nguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\n100",
      "page": 100,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "Leila Tavakoli and Heshaam Faili. 2014. Phrase align-\nments in parallel corpus using bootstrapping ap-\nproach.\nInternational Journal of Information &\nCommunication Technology Research, 6(3).\nDavid Yarowsky, Grace Ngai, and Richard Wicen-\ntowski. 2001.\nInducing multilingual text analysis\ntools via robust projection across aligned corpora.\nIn Proceedings of the ﬁrst international conference\non Human language technology research, pages 1–8.\nAssociation for Computational Linguistics.\n101",
      "page": 101,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "6. subword\n\n102",
      "page": 102,
      "type": "text",
      "language": "sw"
    },
    {
      "level": "H2",
      "text": "Bibliography\nŽeljko Agi´c, Anders Johannsen, Barbara Plank, Héctor Martínez Alonso, Natalie\nSchluter, and Anders Søgaard. 2016. Multilingual projection for parsing truly\nlow-resource languages. Transactions of the Association for Computational\nLinguistics, 4:301–312.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning\nmethod for fully unsupervised cross-lingual mappings of word embeddings. In\nProceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 789–798, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normaliza-\ntion.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine\ntranslation by jointly learning to align and translate. In 3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-\ndocument transformer. CoRR, abs/2004.05150.\nDamián E. Blasi, Antonios Anastasopoulos, and Graham Neubig. 2021. Systematic\ninequalities in language technology performance across the world’s languages.\nCoRR, abs/2110.06733.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En-\nriching word vectors with subword information. Transactions of the Association\nfor Computational Linguistics, 5:135–146.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning.\n2015. A large annotated corpus for learning natural language inference. In\nProceedings of the 2015 Conference on Empirical Methods in Natural Language\n103",
      "page": 103,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nProcessing, pages 632–642, Lisbon, Portugal. Association for Computational\nLinguistics.\nP. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, R. Mercer, and\nP. Roossin. 1988a. A statistical approach to language translation. In Coling Bu-\ndapest 1988 Volume 1: International Conference on Computational Linguistics.\nP. Brown, John Cocke, Stephen Della Pietra, Vincent J. Della Pietra, Frederick\nJelinek, Robert L. Mercer, and P. Roossin. 1988b. A statistical approach to\nFrench/English translation. In Proceedings of the Second Conference on Theo-\nretical and Methodological Issues in Machine Translation of Natural Languages,\nPittsburgh, USA.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L.\nMercer. 1993. The mathematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–311.\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.\n2014. On the properties of neural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and\nStructure in Statistical Translation, pages 103–111, Doha, Qatar. Association\nfor Computational Linguistics.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2021. CANINE:\npre-training an efﬁcient tokenization-free encoder for language representation.\nCoRR, abs/2103.06874.\nRonan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu,\nand Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch.\nJournal of machine learning research, 12:2493–2537.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020a. Unsupervised cross-lingual representation learning\nat scale. In Proceedings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 8440–8451, Online. Association for Computational\nLinguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-lingual language model\npretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc,\nE. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 32, pages 7059–7069. Curran Associates, Inc.\n104",
      "page": 104,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov.\n2020b. Emerging cross-lingual structure in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 6022–6034, Online. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nZi-Yi Dou and Graham Neubig. 2021. Word alignment by ﬁne-tuning embeddings\non parallel corpora. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages\n2112–2128, Online. Association for Computational Linguistics.\nPhilipp Dufter. 2021. Distributed representations for multilingual language pro-\ncessing. Ph.D. thesis, Ludwig-Maximilians-Universität München.\nChris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and\neffective reparameterization of IBM model 2.\nIn Proceedings of the 2013\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 644–648, Atlanta, Georgia.\nAssociation for Computational Linguistics.\nDavid M. Eberhard, F. Simons Gary, and D. Fennig (eds.) Charles. 2020. Ethno-\nlogue: Languages of the World, 23rd edition. SIL International.\nManaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy,\nand Noah A. Smith. 2015. Retroﬁtting word vectors to semantic lexicons. In\nProceedings of the 2015 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies,\npages 1606–1615, Denver, Colorado. Association for Computational Linguistics.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. 2019.\nJointly learning to align and translate with transformer models. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4453–4462, Hong Kong, China. Association for Com-\nputational Linguistics.\n105",
      "page": 105,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and Anna Korhonen. 2016.\nSimVerb-3500: A large-scale evaluation set of verb similarity. In Proceedings\nof the 2016 Conference on Empirical Methods in Natural Language Processing,\npages 2173–2182, Austin, Texas. Association for Computational Linguistics.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Matsuoka. 2016. Analogy-based\ndetection of morphological and semantic relations with word embeddings: what\nworks and what doesn’t. In Proceedings of the NAACL Student Research Work-\nshop, pages 8–15, San Diego, California. Association for Computational Lin-\nguistics.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial\nnets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.\nWeinberger, editors, Advances in Neural Information Processing Systems 27,\npages 2672–2680. Curran Associates, Inc.\nStephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual\ndistributed representations without word alignments. In Proceedings of the 32nd\nInternational Conference on Machine Learning, volume 37 of Proceedings of\nMachine Learning Research, pages 748–756, Lille, France. PMLR.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015. SimLex-999: Evaluating se-\nmantic models with (genuine) similarity estimation. Computational Linguistics,\n41(4):665–695.\nGeoffrey E. Hinton, James L. McClelland, and David E. Rumelhart. 1990. Dis-\ntributed representations. In The Philosophy of Artiﬁcial Intelligence, Oxford\nreadings in philosophy, pages 248–280. Oxford University Press.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural\ncomputation, 9(8):1735–1780.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich Schütze. 2021. Superbizarre\nis not superb: Derivational morphology improves BERT’s interpretation of\ncomplex words. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 3594–3608,\nOnline. Association for Computational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, François Yvon, and Hinrich Schütze. 2020.\nSimAlign: High quality word alignments without parallel training data using\n106",
      "page": 106,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nstatic and contextualized embeddings. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 1627–1643, Online. Association\nfor Computational Linguistics.\nMasoud Jalili Sabet, Matteo Negri, Marco Turchi, and Eduard Barbu. 2016. An\nunsupervised method for automatic translation memory cleaning. In Proceedings\nof the 54th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 287–292, Berlin, Germany. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. In 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\nAlexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslin-\ngual distributed representations of words. In Proceedings of COLING 2012,\npages 1459–1474, Mumbai, India. The COLING 2012 Organizing Committee.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation.\nProceedings of the 10th Machine Translation Summit (MT Summit), 2005, pages\n79–86.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and\nHervé Jégou. 2018. Word translation without parallel data. In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and\nYee Whye Teh. 2019.\nSet transformer: A framework for attention-based\npermutation-invariant neural networks. In Proceedings of the 36th Interna-\ntional Conference on Machine Learning, volume 97 of Proceedings of Machine\nLearning Research, pages 3744–3753. PMLR.\nOmer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix\nfactorization. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems\n27, pages 2177–2185. Curran Associates, Inc.\nOmer Levy, Anders Søgaard, and Yoav Goldberg. 2017. A strong baseline for\nlearning cross-lingual word embeddings from sentence alignments. In Proceed-\nings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 1, Long Papers, pages 765–774, Valencia,\nSpain. Association for Computational Linguistics.\n107",
      "page": 107,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized BERT pretraining approach. Computing Research Repository,\narXiv:1907.11692.\nAntonis Maronikolakis, Philipp Dufter, and Hinrich Schütze. 2021. Wine is not v\ni n. on the compatibility of tokenizations across languages. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2021, pages 2382–2399,\nPunta Cana, Dominican Republic. Association for Computational Linguistics.\nThomas Mayer and Michael Cysouw. 2014. Creating a massively parallel Bible\ncorpus. In Proceedings of the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 3158–3163, Reykjavik, Iceland.\nEuropean Language Resources Association (ELRA).\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.\nLearned in translation: Contextualized word vectors. In Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-\nmation of word representations in vector space. In 1st International Conference\non Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4,\n2013, Workshop Track Proceedings.\nTomás Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities\namong languages for machine translation. CoRR, abs/1309.4168.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities\nin continuous space word representations. In Proceedings of the 2013 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 746–751, Atlanta, Georgia. Association\nfor Computational Linguistics.\nArvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum.\n2014. Efﬁcient non-parametric estimation of multiple embeddings per word in\nvector space. In Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 1059–1069, Doha, Qatar.\nAssociation for Computational Linguistics.\nFranz Josef Och and Hermann Ney. 2000. Improved statistical alignment models.\nIn Proceedings of the 38th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 440–447, Hong Kong. Association for Computational\nLinguistics.\n108",
      "page": 108,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nFranz Josef Och and Hermann Ney. 2003. A systematic comparison of various\nstatistical alignment models. Computational Linguistics, 29(1):19–51.\nRobert Östling and Jörg Tiedemann. 2016. Efﬁcient word alignment with Markov\nChain Monte Carlo. The Prague Bulletin of Mathematical Linguistics, 106(1).\nChanjun Park, Sugyeong Eo, Hyeonseok Moon, and Heuiseok Lim. 2021. Should\nwe ﬁnd another model?: Improving neural machine translation performance with\nONE-piece tokenization method without model modiﬁcation. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies: Industry Papers,\npages 97–104, Online. Association for Computational Linguistics.\nPeyman Passban, Qun Liu, and Andy Way. 2018. Improving character-based\ndecoding using target-side morphological information for neural machine trans-\nlation. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 58–68, New Orleans, Louisiana. Association for\nComputational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal vectors for word representation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 1532–\n1543, Doha, Qatar. Association for Computational Linguistics.\nMatthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power.\n2017. Semi-supervised sequence tagging with bidirectional language models. In\nProceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1756–1765, Vancouver, Canada.\nAssociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representa-\ntions. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander Miller. 2019. Language models as knowledge\nbases? In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural\n109",
      "page": 109,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nLanguage Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China.\nAssociation for Computational Linguistics.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t\nknow: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 2: Short\nPapers), pages 784–789, Melbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100,000+ questions for machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in Natural Language Processing,\npages 2383–2392, Austin, Texas. Association for Computational Linguistics.\nUma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, and Yinfei\nYang. 2020. LAReQA: Language-agnostic answer retrieval from a multilingual\npool. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5919–5930, Online. Association for\nComputational Linguistics.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder, and Iryna Gurevych. 2021.\nHow good is your tokenizer? on the monolingual performance of multilingual\nlanguage models. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 3118–3135,\nOnline. Association for Computational Linguistics.\nElizabeth Salesky, David Etter, and Matt Post. 2021. Robust open-vocabulary\ntranslation from visual text representations. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Processing, pages 7235–7252,\nOnline and Punta Cana, Dominican Republic. Association for Computational\nLinguistics.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele\nMonfardini. 2009. The graph neural network model. IEEE Transactions on\nNeural Networks, 20(1):61–80.\nTimo Schick and Hinrich Schütze. 2019. Learning semantic representations for\nnovel words: Leveraging both form and context. AAAI’19/IAAI’19/EAAI’19.\nAAAI Press.\nPeter H Schönemann. 1966. A generalized solution of the orthogonal procrustes\nproblem. Psychometrika, 31(1):1–10.\n110",
      "page": 110,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nMike Schuster and Kaisuke Nakajima. 2012. Japanese and Korean voice search.\nIn 2012 IEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing, ICASSP 2012, Kyoto, Japan, March 25-30, 2012, pages 5149–5152.\nIEEE.\nHinrich Schütze. 1992. Dimensions of meaning. In Proceedings Supercomputing\n’92, Minneapolis, MN, USA, November 16-20, 1992, pages 787–796. IEEE\nComputer Society.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine\ntranslation of rare words with subword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1715–1725, Berlin, Germany. Association for Computational\nLinguistics.\nAnders Søgaard, Željko Agi´c, Héctor Martínez Alonso, Barbara Plank, Bernd\nBohnet, and Anders Johannsen. 2015. Inverted indexing for cross-lingual NLP.\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1713–1722, Beijing, China. Association\nfor Computational Linguistics.\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: a simple way to prevent neural networks from\noverﬁtting. Journal of machine learning research, 15(1):1929–1958.\nSteinþór Steingrímsson, Hrafn Loftsson, and Andy Way. 2021. CombAlign: a tool\nfor obtaining high-quality word alignments. In Proceedings of the 23rd Nordic\nConference on Computational Linguistics (NoDaLiDa), pages 64–73, Reykjavik,\nIceland (Online). Linköping University Electronic Press, Sweden.\nYulia Tsvetkov, Manaal Faruqui, Wang Ling, Guillaume Lample, and Chris Dyer.\n2015. Evaluation of word vector representations by subspace alignment. In\nProceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, pages 2049–2054, Lisbon, Portugal. Association for Computational\nLinguistics.\nAlan Turing. 1950. Computing machinery and intelligence. Mind, LIX(236):433–\n460.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\n111",
      "page": 111,
      "type": "text",
      "language": "en"
    },
    {
      "level": "H2",
      "text": "BIBLIOGRAPHY\nneed. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 30, pages 5998–6008. Curran Associates, Inc.\nIvan Vuli´c and Marie-Francine Moens. 2015. Bilingual word embeddings from\nnon-parallel document-aligned data applied to bilingual lexicon induction. In\nProceedings of the 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers), pages 719–725, Beijing, China. Association\nfor Computational Linguistics.\nIvan Vuli´c, Sebastian Ruder, and Anders Søgaard. 2020. Are all good word vector\nspaces isomorphic?\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 3178–3192, Online.\nAssociation for Computational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language understanding systems. In Advances in\nNeural Information Processing Systems, volume 32, pages 3266–3280. Curran\nAssociates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. 2018. GLUE: A multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Computational Linguistics.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015. Normalized word embed-\nding and orthogonal transform for bilingual word translation. In Proceedings\nof the 2015 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 1006–1011,\nDenver, Colorado. Association for Computational Linguistics.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, and Colin Raffel. 2021. Byt5: Towards a token-free future\nwith pre-trained byte-to-byte models. CoRR, abs/2105.13626.\nThomas Zenkel, Joern Wuebker, and John DeNero. 2020. End-to-end neural word\nalignment outperforms GIZA++. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pages 1605–1617, Online.\nAssociation for Computational Linguistics.\n112",
      "page": 112,
      "type": "text",
      "language": "en"
    }
  ],
  "pdf_type": "Mixed PDF",
  "languages": [],
  "processing_time": 156.82164311408997,
  "error": null
}